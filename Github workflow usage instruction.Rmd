---
title: "Arson Wildfire data analysis workflow"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data")
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation

This repo is accompanying the publication: "The Use of Computational Fingerprinting Techniques to Distinguish Sources of Accelerants Used in Wildfire Arson".

Users need to first install R with this [link](https://cran.r-project.org/mirrors.html) and Rstudio with this [link](https://posit.co/download/rstudio-desktop/).

This workflow ran on Windows 11 OS 11th Gen Intel(R) Core(TM) i7-11800H \@ 2.30GHz, 16 GB RAM;

THe RStudio version used in this demo is 2023.06.0+421 "Mountain Hydrangea" Release for Windows;

The R version used in this demo is 4.3.1

## Data processing

First, the following R packages are installed and loaded in the global environment along with in-house built functions to minimize repetitiveness in the code.

Details about these functions can be found in Data processing.R file in this repo.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(Hmisc)
library(bestNormalize)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Functions -------------------------------------------------------------------------------------------------------
filtering <- function(df, filter_list) {
  clean_data <-  copy(df)
  for (ele in filter_list) {
    clean_data <- clean_data %>%
      filter(!grepl(ele, Compound))
  }
  return(clean_data)
}

# Filtering limit of observations
limit_obser <- function(df_list, file_list, cap) {
  df_list_filter_area <- list()
  df_list_removed_area <- list()
  for (i in 1:length(df_list)) {
    df_list_filter_area[[i]] <- df_list[[i]] %>%
      filter(., Area > cap) %>%
      mutate(sample_name = file_list[[i]]) %>%
      # Grouping samples into fuel types
      mutate(fuel_type = ifelse(str_detect(sample_name, "DieselComp"), "DieselComp", 
                                ifelse(str_detect(sample_name, "GasComp"), "GasComp",
                                       ifelse(str_detect(sample_name, "D"), "Diesel", "Gas")))) %>%
      # Grouping samples into respective Gas stations
      mutate(gas_station = ifelse(str_detect(sample_name, "F009"), "Station_6", 
                                  ifelse(str_detect(sample_name, "F001"), "Station_1",
                                         ifelse(str_detect(sample_name, "F007"), "Station_4", 
                                                ifelse(str_detect(sample_name, "F005"), "Station_3", 
                                                       ifelse(str_detect(sample_name, "F003"), "Station_2", 
                                                              ifelse(str_detect(sample_name, "F008"), "Station_5", "Composite")))))))
    
    df_list_removed_area[[i]] <- df_list[[i]] %>%
      filter(., Area <= cap) %>%
      mutate(sample_name = file_list[[i]]) %>%
      # Grouping samples into fuel types
      mutate(fuel_type = ifelse(str_detect(sample_name, "DieselComp"), "DieselComp", 
                                ifelse(str_detect(sample_name, "GasComp"), "GasComp",
                                       ifelse(str_detect(sample_name, "D"), "Diesel", "Gas")))) %>%
      # Grouping samples into respective Gas stations
      mutate(gas_station = ifelse(str_detect(sample_name, "F009"), "Station_6", 
                                  ifelse(str_detect(sample_name, "F001"), "Station_1",
                                         ifelse(str_detect(sample_name, "F007"), "Station_4", 
                                                ifelse(str_detect(sample_name, "F005"), "Station_3", 
                                                       ifelse(str_detect(sample_name, "F003"), "Station_2", 
                                                              ifelse(str_detect(sample_name, "F008"), "Station_5", "Composite")))))))
  }
  return(list(df_list_filter_area, df_list_removed_area))
}

# Notin function
`%notin%` <- Negate(`%in%`)

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp_ver1 <- function(data, rt1thres, rt2thres, ion1thres, ion2thres) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rt1thres: RT1 threshold window
  # rt2thres: RT2 threshold window
  # ion1thres: Ion1 threshold window
  # ion2thres: Ion2 threshold window
  # region_applied: list of x-y coordinate for different regions to applied different threshold window, x-axis is RT1, y-axis is RT2
  dat <- copy(data) %>% 
    arrange(RT1, RT2)
  
  # Initialize the compound column filled with NA values
  dat$collapsed_compound <- NA
  i <- 1
  
  for (row in 1:nrow(dat)) {
    # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    rt1 <- dat[row,]$RT1
    rt2 <- dat[row,]$RT2
    ion1 <- dat[row,]$Ion1
    ion2 <- dat[row,]$Ion2
    
    idx_thres <- which(dat$RT1 <= (rt1 + rt1thres) & dat$RT1 >= (rt1 - rt1thres) & 
                         dat$RT2 <= (rt2 + rt2thres) & dat$RT2 >= (rt2 - rt2thres) & 
                         dat$Ion1 <= (ion1 + ion1thres) & dat$Ion1 >= (ion1 - ion1thres) & 
                         dat$Ion2 <= (ion2 + ion2thres) & dat$Ion2 >= (ion2 - ion2thres) &
                         is.na(dat$collapsed_compound))
    
    if (identical(idx_thres, integer(0))) {
      next
    }
    else {
      dat[idx_thres, "collapsed_compound"] <- paste0("Compound_", i, ".")
      i <- i + 1
    }  
  }
  
  return(dat)
}

# Filtering similar and unique compound
comp_filter_ver1 <- function(data) {
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()

  for (comp_grp in unique(data$collapsed_compound)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$collapsed_compound, fixed = TRUE))
    
    if (length(idx) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    } else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}

add_data_normalization <- function(data) {
  temp_list <- list()
  i <- 1
  for (sample in unique(data$sample_name)) {
    df <- data[which(data$sample_name == sample),] %>%
      # TSN - Percent-based normalization
      mutate(Percent_Area = Area/sum(.$Area)) %>%
      mutate(Log_Area = log10(Area)) %>%
      mutate(boxcox_area = bestNormalize::boxcox(Area)$x.t)
    temp_list[[i]] <- df
    i <- i + 1
  }
  # Then combine data again to 1 grand data frame
  newdata <- dplyr::bind_rows(temp_list)
  return(newdata)
}
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# STEP 1.1: Data import --------------------------------------------
setwd( "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data")

file_list <- list.files(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data",
                        pattern = '*.xlsx')

# Pipe operator for isolating IL types
ILR_file_list <- file_list %>%
  .[!str_ends(., "check.xlsx")] %>%
  .[!str_detect(., "grouping_compounds")] %>%
  .[!str_detect(., "Station")] %>%
  .[!str_detect(., "Workflow")] %>%
  .[!str_detect(., "ASTM")] %>%
  .[!str_detect(., "Comp")] %>%
  .[!str_detect(., "data_table")]

ASTM_list <- read_xlsx(paste0("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data", "/ASTM Compound List.xlsx"))

# Import IL samples to list
df_list_step1.1 <- purrr::map(ILR_file_list, read_xlsx,
                              sheet = "Results")

# remove spaces in column names in df_list_step1.1
for (i in 1:length(df_list_step1.1)) {
  colnames(df_list_step1.1[[i]]) <- gsub(" ", "", colnames(df_list_step1.1[[i]]))
}

# STEP 1.2A: Filtering out column bleed, solvent and BTEX and minimum area observations--------------------------------------
  
df_step1.2 <- purrr::map(df_list_step1.1, filtering, filter_list = c("^Carbon disulfide$", 
                                                                     "Cyclotrisiloxane..hexamethyl",
                                                                     "Cyclotetrasiloxane..octamethyl",
                                                                     "^Benzene$",
                                                                     "^Toluene$",
                                                                     "^Ethylbenzene$",
                                                                     "Xylene"))

# STEP 1.2B Filtering out limit of observations----------------------------
list_remaining_area <- limit_obser(df_step1.2, ILR_file_list, cap = 50000)[[1]]
list_removed_area <- limit_obser(df_step1.2, ILR_file_list, cap = 50000)[[2]]

# STEP 1.3: Grouping compounds based on RT1, RT2, Ion1 -----------------------------------------------------------------------
df_step1.3 <- bind_rows(list_remaining_area)

rt10.1 <- grouping_comp_ver1(df_step1.3,
                             rt1thres = 0.1,
                             rt2thres = 0.15,
                             ion1thres = 0.05, # Ion 1 and 2 indicates molecular structure (2 most prevalent mass-to-charge)
                             ion2thres = 0.05)

# STEP 2: Identify shared and unique compound groups across samples ================================================================================
filter_rt10.1 <- comp_filter_ver1(comp_normalized_rt10.1)

shared_comp_rt10.1 <- comp_normalized_rt10.1[filter_rt10.1[[1]],]

# STEP 3: Data Normalization ------------------------------------------------
comp_normalized_rt10.1 <- add_data_normalization(rt10.1)
```

### Confirming ASTM at each step of Data processing
```{r, echo = FALSE, message = FALSE, warning = FALSE}
count_ASTM <- function(data, ASTM_list, which_step) {
  count <- 0
  for (r in 1:nrow(ASTM_list)) {
    rt1 <- ASTM_list[r,]$RT1
    rt2 <- ASTM_list[r,]$RT2
    ion1 <- ASTM_list[r,]$Ion1
    ion2 <- ASTM_list[r,]$Ion2
    idx <- which(data$RT1 <= (rt1 + 0.1) & data$RT1 >= (rt1 - 0.1) & 
                   data$RT2 <= (rt2 + 0.15) & data$RT2 >= (rt2 - 0.15) & 
                   # Tolerance ion window of 0.5 for ASTM compounds
                   data$Ion1 <= (ion1 + 0.5) & data$Ion1 >= (ion1 - 0.5) & 
                   data$Ion2 <= (ion2 + 0.5) & data$Ion2 >= (ion2 - 0.5))
    
    if (length(idx) > 0) {
      count <- count + 1
    } else {
      next
    }
  }
  print(paste0("The number of ASTM compounds found at ", which_step ," is: ", count))
}

count_ASTM(bind_rows(df_list_step1.1), ASTM_list, which_step = "STEP 1.1")
count_ASTM(bind_rows(list_remaining_area), ASTM_list, which_step = "STEP 1.2")
count_ASTM(rt10.1, ASTM_list, which_step = "STEP 1.3")
count_ASTM(comp_normalized_rt10.1, ASTM_list, which_step = "STEP 2")
count_ASTM(shared_comp_rt10.1, ASTM_list, which_step = "STEP 3")
```

## PCA

### Gas versus Diesel

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(PCAtools)
library(FactoMineR)

# Research Question 1 PCA -----------------
# category 5 >=2 Gas and /or >=2 Diesel data pts 
df_pca_rq1 <- function(data) {
  cat_5 <- data %>%
    filter(., fuel_type %in% c("Gas", "Diesel")) %>%
    select(sample_name, collapsed_compound, Percent_Area, fuel_type) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(sample_name, collapsed_compound, fuel_type) %>%
    summarise(across(Percent_Area, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Percent_Area)
  
  # Filter any compound that has > 90% NAs
  colid_90_na <- c()
  for (i in 3:ncol(cat_5)) {
    if (length(which(!is.na(cat_5[, i])))/nrow(cat_5[, i]) < 0.1) {
      colid_90_na <- c(colid_90_na, i)
    }
  }
  
  cat_5_new <- cat_5[, -colid_90_na]
  
  for (r in 1:nrow(cat_5_new)) { 
    cat_5_new[r, which(base::is.na(cat_5_new[r,]))] <- 0 
  }

  
  return(cat_5_new)
}

df_pca_rq1_rt10.1 <- df_pca_rq1(shared_comp_rt10.1)

res.pca <- FactoMineR::PCA(
  df_pca_rq1_rt10.1[, 3:ncol(df_pca_rq1_rt10.1)],
  scale.unit = FALSE,
  graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            habillage = factor(df_pca_rq1_rt10.1$fuel_type),
                            addEllipses = TRUE,
                            ellipse.level=0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Plastic Sample"
                            ) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'bottom') 
  # + coord_equal()
```

### Between groups of gas stations

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Research Question 2 PCA ##########################
# Category 2: Compounds that have >=2 data points for each gas stations
df_pca_rq2 <- function(data) {
  mydata2 <- data %>%
    filter(., fuel_type %in% "Gas") %>%
    select(sample_name, collapsed_compound, Percent_Area, gas_station) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(sample_name, collapsed_compound, gas_station) %>%
    summarise(across(Percent_Area, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Percent_Area)

  # Filter any compound that has > 90% NAs
  colid_90_na <- c()
  for (i in 3:ncol(mydata2)) {
    if (length(which(!is.na(mydata2[, i])))/nrow(mydata2[, i]) < 0.1) {
      colid_90_na <- c(colid_90_na, i)
    }
  }
  
  mydata2_new <- mydata2[, -colid_90_na]
  
  for (r in 1:nrow(mydata2_new)) { 
    mydata2_new[r, which(base::is.na(mydata2_new[r,]))] <- 0 
  }

  return(mydata2_new)
}  

df_pca_rq2_rt10.1 <- df_pca_rq2(shared_comp_rt10.1)

# Conduct principal component analysis (PCA):
res.pca <- FactoMineR::PCA(
  df_pca_rq2_rt10.1[, 3:ncol(df_pca_rq2_rt10.1)],
  scale.unit = FALSE,
  graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            habillage = factor(df_pca_rq2_rt10.1$gas_station),
                            addEllipses = TRUE,
                            ellipse.level=0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Plastic Sample"
                            ) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'bottom') 
  # + coord_equal()
```

## Multiple Wilcoxon tests with p-value correction for multiple testing

### Research Question 1: Gas versus Diesel

```{r, echo = FALSE, message = FALSE, warning = FALSE}
stats_rq1 <- shared_comp_rt10.1  %>%
  filter(., fuel_type %in% c("Gas", "Diesel")) %>%
  select(sample_name, collapsed_compound, Percent_Area) %>%
  mutate(sample_name = factor(sample_name, levels = c(unique(sample_name)))) %>%
  mutate(collapsed_compound = factor(collapsed_compound, levels = c(unique(collapsed_compound)))) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(sample_name, collapsed_compound) %>%
  summarise(across(Percent_Area, mean)) %>%
  pivot_wider(names_from = sample_name, values_from = Percent_Area) %>%
  relocate(`0220F001D.xlsx`, `0220F009D.xlsx`, `0220F009-2D.xlsx`, `0220F005D.xlsx`, .after = last_col()) %>%
  column_to_rownames(., var = "collapsed_compound")

# category 5: >=2 gas and >=2 diesel (multiple testing) --------------------------------------------------------
cat_5 <- stats_rq1[(rowSums(!is.na(stats_rq1[, 1:21])) >= 2) & 
                       (rowSums(!is.na(stats_rq1[, 22:25])) >= 2),] # -> 374 out of 4769 compounds

for (r in 1:nrow(cat_5)) { 
  cat_5[r, which(base::is.na(cat_5[r,]))] <- runif(length(which(base::is.na(cat_5[r,]))),
                                                       min = sort(shared_comp_rt10.1$Percent_Area)[1]/100000,
                                                       max = sort(shared_comp_rt10.1$Percent_Area)[2]/100000)
}
```

Before running multiple Wilcoxon tests, it is recommended to examine whether criteria for univariate parametric test, such as t-test, are violated. If yes, then it is safe to proceed using non-parametric univariate test, such as Wilcoxon test.

First, equal variance and normally distributed between two populations, here are Gas and Diesel using histogram, Q-Q plots. Here, both Gas and Diesel populations are NOT normally distributed.

```{r, echo = TRUE, message = FALSE, warning = FALSE}
GasData <- as.vector(t(cat_5[,c(1:21)])) # 100149 data points
DieselData <- as.vector(t(cat_5[,c(22:25)])) # 19076 data point

# Histogram
hist(GasData, col='steelblue', main='Gas')
hist(DieselData, col='steelblue', main='Diesel')

# Q-Q plots aka. Normal Probability plots
stats::qqnorm(GasData, main='Gas')
stats::qqline(GasData)

stats::qqnorm(DieselData, main='Diesel')
stats::qqline(DieselData)
```

Then, equality of variance between Gas and Diesel populations can be examined using Levene's test and Fligner-Killeen test for non-normally distributed data. Here, for both tests, p values are \< 0.05, and thus, there is significant difference in variances between Gas and Diesel populations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Levene’s test-non-normally distributed data
library(car)
data <- c(GasData, DieselData)
group <- as.factor(c(rep("Gas", times = length(GasData)), rep("Diesel", times = length(DieselData))))
non_norm_dist_data <- data.frame(data, group)
car::leveneTest(data ~ group, data = non_norm_dist_data)

# Fligner-Killeen test
stats::fligner.test(data ~ group, data = non_norm_dist_data)
```
Here, multiple Wilcoxon tests followed by p-value correction for multiple testing was done. Different method for p-value correction from function *p.adjust* from package **stats** were used. After p-value correction, the threshold for p-value can be set (for example,p.adjust \< 0.05 or \< 0.1), to see how it affects the number of significant compounds that can be found. Importantly, the threshold should be set so that it can include as many ASTM reference compounds in the list of significant compounds as possible. For example here, no significant compounds can be found with adjusted p-value \< 0.05. But when adjusted p-value is \< 0.1, 127 significant compounds are found.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Wilcoxon Test ========================================================================================
pvalue.w <- c()

for (i in 1:nrow(cat_5)) { 
  pvalue.w[i] <- wilcox.test(as.numeric(cat_5[i, c(1:21)]), as.numeric(cat_5[i, c(22:25)]))$p.value
}

summary_table <- cbind(rownames(cat_5), as.data.frame(pvalue.w))
colnames(summary_table) <- c("collapsed_compound", "pvalue")

summary_table$adjusted_pvalue_holm <- stats::p.adjust(summary_table$pvalue, method = "holm")
summary_table$adjusted_pvalue_hochberg <- stats::p.adjust(summary_table$pvalue, method = "hochberg")
summary_table$adjusted_pvalue_hommel <- stats::p.adjust(summary_table$pvalue, method = "hommel")
summary_table$adjusted_pvalue_bonferroni <- stats::p.adjust(summary_table$pvalue, method = "bonferroni")
summary_table$adjusted_pvalue_BH <- stats::p.adjust(summary_table$pvalue, method = "BH")
summary_table$adjusted_pvalue_BY <- stats::p.adjust(summary_table$pvalue, method = "BY")
summary_table$adjusted_pvalue_fdr <- stats::p.adjust(summary_table$pvalue, method = "fdr")

rq1_p0.05 <- summary_table %>%
  filter(., adjusted_pvalue_holm < 0.05)

paste0("Number of significant compounds with adjusted p-value < 0.05 = ", dim(rq1_p0.05)[1])

rq1_p0.1 <- summary_table %>%
  filter(., adjusted_pvalue_holm < 0.1) %>%
  arrange(adjusted_pvalue_holm)

paste0("Number of significant compounds with adjusted p-value < 0.1 = ", dim(rq1_p0.1)[1])


View(shared_comp_rt10.1  %>%
  filter(., fuel_type %in% c("Gas", "Diesel")) %>%
  filter(., collapsed_compound %in% unique(rq1_p0.1$collapsed_compound)))

export_rq1 <- shared_comp_rt10.1  %>%
  filter(., fuel_type %in% c("Gas", "Diesel")) %>%
  filter(., collapsed_compound %in% unique(rq1_p0.1$collapsed_compound)) %>%
  filter(., collapsed_compound %notin% c("Compound_7036.", "Compound_2600.", "Compound_1300.", "Compound_6023.",
                                         "Compound_5849.", "Compound_5728.", "Compound_3702.", "Compound_3841.", 
                                         "Compound_2437.", "Compound_2285.", "Compound_1980.", "Compound_1804.", 
                                         "Compound_1721.", "Compound_1671.", "Compound_1650.", "Compound_1887.")) %>%
  group_by(collapsed_compound) %>%
  summarise(., mean_rt1 = round(mean(RT1), 3),
            mean_rt2 = round(mean(RT2), 3),
            mean_ion1 = round(mean(Ion1), 2), 
            mean_ion2 = round(mean(Ion2), 2))

writexl::write_xlsx(x = export_rq1, path = paste0(getwd(), "/significant beyond ASTM compound_RQ1.xlsx"))
```

### Research Question 2: Distinguishing samples from groups of gas stations

```{r, echo = FALSE, message = FALSE, warning = FALSE}
df_rq2 <- function(data) {
  mydata2 <- data %>%
    filter(., fuel_type %in% "Gas") %>%
    select(sample_name, collapsed_compound, Percent_Area) %>%
    mutate(sample_name = factor(sample_name, levels = c(unique(sample_name)))) %>%
    mutate(collapsed_compound = factor(collapsed_compound, levels = c(unique(collapsed_compound)))) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(sample_name, collapsed_compound) %>%
    summarise(across(Percent_Area, mean)) %>%
    pivot_wider(names_from = sample_name, values_from = Percent_Area) %>%
    column_to_rownames(., var = "collapsed_compound")
  
  # transpose the rows and columns
  transpose_mydata2 <- data.table::transpose(mydata2)
  rownames(transpose_mydata2) <- colnames(mydata2)
  colnames(transpose_mydata2) <- rownames(mydata2)
  transpose_mydata2 <- transpose_mydata2 %>%
    rownames_to_column(., var = "sample_name")
  
  transpose_mydata2_new <- transpose_mydata2 %>%
    # # Grouping samples into respective Gas stations
    mutate(gas_station = ifelse(str_detect(sample_name, "F009"), "Station_6", 
                                  ifelse(str_detect(sample_name, "F001"), "Station_1",
                                         ifelse(str_detect(sample_name, "F007"), "Station_4", 
                                                ifelse(str_detect(sample_name, "F005"), "Station_3", 
                                                       ifelse(str_detect(sample_name, "F003"), "Station_2", 
                                                              ifelse(str_detect(sample_name, "F008"), "Station_5", "Composite"))))))) %>%
    relocate(gas_station, .after = sample_name)
  
  # Category 1 (rq2_cat1) : Compound found in only 1 gas station and not in any other
  rq2_cat1 <- data.frame(matrix(nrow = nrow(transpose_mydata2_new)))
  # Category 2 (rq2_cat2): Compound found in >=2 gas stations
  rq2_cat2 <- data.frame(matrix(nrow = nrow(transpose_mydata2_new)))
  
  # if compounds has only 1 record
  rq2_cat1 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) == 1]
  # if compounds has 2 record
  temp_1 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) == 2]
  rq2_cat2_col_id <- c()
  rq2_cat1_col_id <- c()
  for (col in 1:ncol(temp_1)) {
    idx1 <- which(!is.na(temp_1[,col]))
    # if 2 records from 2 different gas stations
    if (length(unique(transpose_mydata2_new[idx1]$gas_station)) > 1) {
      rq2_cat2_col_id <- c(rq2_cat2_col_id, col)
    } else { # if 2 records from same gas stations
      rq2_cat1_col_id <- c(rq2_cat1_col_id, col)
    }
  }
  rq2_cat1 <- base::cbind(rq2_cat1, temp_1[, rq2_cat1_col_id])
  rq2_cat2 <- temp_1[, rq2_cat2_col_id]
  # if compounds have 3 records
  temp_2 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) == 3]
  rq2_cat2_col_id <- c()
  rq2_cat1_col_id <- c()
  for (col in 1:ncol(temp_2)) {
    idx2 <- which(!is.na(temp_2[,col]))
    # if 3 records from different gas stations
    if (length(unique(transpose_mydata2_new[idx2,]$gas_station)) > 1) {
      rq2_cat2_col_id <- c(rq2_cat2_col_id, col)
    } else {  # if 3 records from same gas stations
      rq2_cat1_col_id <- c(rq2_cat1_col_id, col)
    }
  }
  rq2_cat1 <- base::cbind(rq2_cat1, temp_2[, rq2_cat1_col_id])
  rq2_cat2 <- base::cbind(rq2_cat2, temp_2[, rq2_cat2_col_id])
  
  # if compounds have >=4 records -> it definitely appear in >=2 Gas stations (712 compounds)
  temp_3 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) > 3]
  rq2_cat2 <- base::cbind(rq2_cat2, temp_3)
  
  # Insert sample name and gas station codes to Cat1 and Cat2 data frames ====================================
  rq2_cat1 <- base::cbind(rq2_cat1, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  rq2_cat2 <- base::cbind(rq2_cat2, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  # Subset compounds that have >= 2 obs per gas station for Wilcoxon test ------------------------------------------------
  temp_4 <- base::cbind(temp_3, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  col_id <- c()
  
  for (col in 3:ncol(temp_4)) {
    record_count <- c()
    idx <- which(!is.na(temp_4[,col]))
    
    for (station in unique(temp_4[idx, "gas_station"])) {
      count <- sum(temp_4[idx,]$gas_station == station)
      record_count <- c(record_count, count) 
    }
    
    if (sum(record_count >= 2) >= 2) {
      col_id <- c(col_id, col)
    } else {
      next
    }
  }
  
  # Data frame of compounds that have >= 2 obs per gas station
  rq2_cat2_stats <- temp_4[, col_id] # 506 compounds
  
  # Impute NA with LOD  for RQ1 Category 2: compounds in >=2 Gas stations
  for (col in 1:ncol(rq2_cat2_stats)) {
    rq2_cat2_stats[which(is.na(rq2_cat2_stats[,col])), col] <- runif(length(which(is.na(rq2_cat2_stats[,col]))),
                                                                     min = sort(data$Percent_Area)[1],
                                                                     max = sort(data$Percent_Area)[2])
  }


  rq2_cat2_stats_imputed <- base::cbind(rq2_cat2_stats, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  return(rq2_cat2_stats_imputed)
}  

df_stats_rq2_rt10.1 <- df_rq2(shared_comp_rt10.1)
```

Here again, before running multiple Wilcoxon tests, it is recommended to examine whether criteria for univariate parametric test, such as t-test, are violated. If yes, then it is safe to proceed using non-parametric univariate test, such as Wilcoxon test.

First, equal variance and normally distributed between two populations, here are Gas and Diesel using histogram, Q-Q plots. Here, both Gas station group 1 (1, 5, 2) and Gas station group 2 (3, 4, 6) populations are NOT normally distributed.

Equality of variance between Gas station group 1 (1, 5, 2) and Gas station group 2 (3, 4, 6) populations can be examined using Fligner-Killeen test for non-normally distributed data. Here, p values are \< 0.05, and thus, there is significant difference in variances between Gas station group 1 (1, 3, 8) and Gas station group 2 (5, 7, 9) populations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Examine whether 2 gas station groups have equal variance and normally distributed.-----------------------------------------------
gasgr1_Data <- as.vector(t(df_stats_rq2_rt10.1 %>% 
                             filter(., gas_station %in% c("Station_1", "Station_5", "Station_2")) %>% 
                             select(., 3:ncol(.))))
gasgr2_Data <- as.vector(t(df_stats_rq2_rt10.1 %>%
                             filter(., gas_station %in% c("Station_3", "Station_4", "Station_6")) %>% 
                             select(., 3:ncol(.))))

# Histogram
hist(gasgr1_Data, col='steelblue', main='Gas stations 1, 2, 5')
hist(gasgr2_Data, col='steelblue', main='Gas stations 3, 4, 6')

# Q-Q plots aka. Normal Probability plots
stats::qqnorm(gasgr1_Data, main='Gas stations 1, 2, 5')
stats::qqline(gasgr1_Data)

stats::qqnorm(gasgr2_Data, main='Gas stations 3, 4, 6')
stats::qqline(gasgr2_Data)

# Fligner-Killeen test
stats::fligner.test(data ~ group, data = non_norm_dist_data)
```

Here, multiple Wilcoxon tests followed by p-value correction for multiple testing was done. Different method for p-value correction from function *p.adjust* from package **stats** were used. After p-value correction, the threshold for p-value can be set (for example,p.adjust \< 0.05 or \< 0.1), to see how it affects the number of significant compounds that can be found. Importantly, the threshold should be set so that it can include as many ASTM reference compounds in the list of significant compounds as possible. For example here, no significant compounds can be found with adjusted p-value \< 0.05. But when adjusted p-value is \< 0.1, 127 significant compounds are found.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Wilcoxon exact test -----------------------

pvalue.w_rq2 <- c()
i <- 1
for (col in 3:ncol(df_stats_rq2_rt10.1)){
  pvalue.w_rq2[i] <- stats::wilcox.test(as.numeric(df_stats_rq2_rt10.1[which(df_stats_rq2_rt10.1$gas_station %in% c("Station_1", "Station_2", "Station_5")), col]),
                                        as.numeric(df_stats_rq2_rt10.1[which(df_stats_rq2_rt10.1$gas_station %in% c("Station_3", "Station_4", "Station_6")), col]))$p.value
  i <- i + 1
}

rq2_cat2_summary_table <- cbind(colnames(df_stats_rq2_rt10.1[,3:ncol(df_stats_rq2_rt10.1)]), 
                                as.data.frame(pvalue.w_rq2))
colnames(rq2_cat2_summary_table) <- c("collapsed_compound", "pvalue")

rq2_cat2_summary_table$adjusted_pvalue_holm <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "holm")
rq2_cat2_summary_table$adjusted_pvalue_hochberg <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "hochberg")
rq2_cat2_summary_table$adjusted_pvalue_hommel <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "hommel")
rq2_cat2_summary_table$adjusted_pvalue_bonferroni <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "bonferroni")
rq2_cat2_summary_table$adjusted_pvalue_BH <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "BH")
rq2_cat2_summary_table$adjusted_pvalue_BY <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "BY")
rq2_cat2_summary_table$adjusted_pvalue_fdr <- stats::p.adjust(rq2_cat2_summary_table$pvalue, method = "fdr")

rq2_cat2_alpha0.05 <- rq2_cat2_summary_table %>%
  filter(., adjusted_pvalue_holm < 0.05)

paste0("Number of significant compounds with adjusted p-value < 0.05 = ", dim(rq2_cat2_alpha0.05)[1])

rq2_cat2_alpha0.1 <- rq2_cat2_summary_table %>%
  filter(., adjusted_pvalue_holm < 0.1) %>%
  arrange(adjusted_pvalue_holm)

paste0("Number of significant compounds adjusted p-value < 0.1 = ", dim(rq2_cat2_alpha0.1)[1])

# View(shared_comp_rt10.1  %>%
#   filter(., fuel_type %in% "Gas") %>%
#   filter(., collapsed_compound %in% unique(rq2_cat2_alpha0.05$collapsed_compound)))

export_rq2 <- shared_comp_rt10.1  %>%
  filter(., fuel_type %in% c("Gas")) %>%
  filter(., collapsed_compound %in% unique(rq2_cat2_alpha0.05$collapsed_compound)) %>%
  filter(., collapsed_compound %notin% c("Compound_1300.", "Compound_3742.", "Compound_3118.", "Compound_3076.",
                                         "Compound_2285.", "Compound_1980.", "Compound_1804.", "Compound_1721.",
                                         "Compound_1671.", "Compound_1650.", "Compound_1887.")) %>%
  group_by(collapsed_compound) %>%
  summarise(., mean_rt1 = round(mean(RT1), 3),
            mean_rt2 = round(mean(RT2), 3),
            mean_ion1 = round(mean(Ion1), 2), 
            mean_ion2 = round(mean(Ion2), 2))

writexl::write_xlsx(x = export_rq2, path = paste0(getwd(), "/significant beyond ASTM compound_RQ2.xlsx"))
```
## Reserve code
```{r, echo = FALSE, message = FALSE, warning = FALSE}
 transpose_mydata2 <- data.table::transpose(mydata2)
  rownames(transpose_mydata2) <- colnames(mydata2)
  colnames(transpose_mydata2) <- rownames(mydata2)
  transpose_mydata2 <- transpose_mydata2 %>%
    rownames_to_column(., var = "sample_name")
  
  transpose_mydata2_new <- transpose_mydata2 %>%
    # # Grouping samples into respective Gas stations
    mutate(gas_station = ifelse(str_detect(sample_name, "F009"), "Station_9",
                                ifelse(str_detect(sample_name, "F001"), "Station_1",
                                       ifelse(str_detect(sample_name, "F007"), "Station_7",
                                              ifelse(str_detect(sample_name, "F005"), "Station_5",
                                                     ifelse(str_detect(sample_name, "F003"), "Station_3",
                                                            ifelse(str_detect(sample_name, "F008"), "Station_8", "Composite"))))))) %>%
    relocate(gas_station, .after = sample_name)
  
  # Category 1 (rq2_cat1) : Compound found in only 1 gas station and not in any other
  rq2_cat1 <- data.frame(matrix(nrow = nrow(transpose_mydata2_new)))
  # Category 2 (rq2_cat2): Compound found in >=2 gas stations
  rq2_cat2 <- data.frame(matrix(nrow = nrow(transpose_mydata2_new)))
  
  # if compounds has only 1 record
  rq2_cat1 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) == 1]
  # if compounds has 2 record
  temp_1 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) == 2]
  rq2_cat2_col_id <- c()
  rq2_cat1_col_id <- c()
  for (col in 1:ncol(temp_1)) {
    idx1 <- which(!is.na(temp_1[,col]))
    # if 2 records from 2 different gas stations
    if (length(unique(transpose_mydata2_new[idx1]$gas_station)) > 1) {
      rq2_cat2_col_id <- c(rq2_cat2_col_id, col)
    } else { # if 2 records from same gas stations
      rq2_cat1_col_id <- c(rq2_cat1_col_id, col)
    }
  }
  rq2_cat1 <- base::cbind(rq2_cat1, temp_1[, rq2_cat1_col_id])
  rq2_cat2 <- temp_1[, rq2_cat2_col_id]
  # if compounds have 3 records
  temp_2 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) == 3]
  rq2_cat2_col_id <- c()
  rq2_cat1_col_id <- c()
  for (col in 1:ncol(temp_2)) {
    idx2 <- which(!is.na(temp_2[,col]))
    # if 3 records from different gas stations
    if (length(unique(transpose_mydata2_new[idx2,]$gas_station)) > 1) {
      rq2_cat2_col_id <- c(rq2_cat2_col_id, col)
    } else {  # if 3 records from same gas stations
      rq2_cat1_col_id <- c(rq2_cat1_col_id, col)
    }
  }
  rq2_cat1 <- base::cbind(rq2_cat1, temp_2[, rq2_cat1_col_id])
  rq2_cat2 <- base::cbind(rq2_cat2, temp_2[, rq2_cat2_col_id])
  
  # if compounds have >=4 records -> it definitely appear in >=2 Gas stations (712 compounds)
  temp_3 <- transpose_mydata2_new[,3:ncol(transpose_mydata2_new)][,colSums(!is.na(transpose_mydata2_new[,3:ncol(transpose_mydata2_new)])) > 3]
  rq2_cat2 <- base::cbind(rq2_cat2, temp_3)
  
  # Insert sample name and gas station codes to Cat1 and Cat2 data frames ====================================
  rq2_cat1 <- base::cbind(rq2_cat1, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  rq2_cat2 <- base::cbind(rq2_cat2, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  # Subset compounds that have >= 2 obs per gas station for Wilcoxon test ------------------------------------------------
  temp_4 <- base::cbind(temp_3, transpose_mydata2_new[, 1:2]) %>%
    relocate(sample_name, gas_station, .before = everything())
  
  col_id <- c()
  
  for (col in 3:ncol(temp_4)) {
    record_count <- c()
    idx <- which(!is.na(temp_4[,col]))
    
    for (station in unique(temp_4[idx, "gas_station"])) {
      count <- sum(temp_4[idx,]$gas_station == station)
      record_count <- c(record_count, count) 
    }
    
    if (sum(record_count >= 2) >= 2) {
      col_id <- c(col_id, col)
    } else {
      next
    }
  }
  
  # Data frame of compounds that have >= 2 obs per gas station
  rq2_cat2_stats <- temp_4[, col_id] # 506 compounds
```