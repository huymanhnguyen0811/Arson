---
title: "Arson Wildfire data analysis workflow"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data")
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation

This repo is accompanying the publication: "The Use of Computational Fingerprinting Techniques to Distinguish Sources of Accelerants Used in Wildfire Arson".

Users need to first install R with this [link](https://cran.r-project.org/mirrors.html) and Rstudio with this [link](https://posit.co/download/rstudio-desktop/).

This workflow ran on Windows 11 OS 11th Gen Intel(R) Core(TM) i7-11800H \@ 2.30GHz, 16 GB RAM;

THe RStudio version used in this demo is 2023.06.0+421 "Mountain Hydrangea" Release for Windows;

The R version used in this demo is 4.3.1

## Data processing

First, the following R packages are installed and loaded in the global environment along with in-house built functions to minimize repetitiveness in the code.

Details about these functions can be found in Data processing.R file in this repo.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(purrr)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)
library(stats)
library(FactoMineR)
library(factoextra)
library(compositions)
library(ggforce)
library(latticeExtra)
library(cluster)

# Functions -------------------------------------------------------------------------------------------------------
# Notin function
`%notin%` <- Negate(`%in%`)

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp_ver1 <- function(data, rt1thres, rt2thres, ion1thres, ion2thres) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rt1thres: RT1 threshold window
  # rt2thres: RT2 threshold window
  # ion1thres: Ion1 threshold window
  # ion2thres: Ion2 threshold window
  # region_applied: list of x-y coordinate for different regions to applied different threshold window, x-axis is RT1, y-axis is RT2
  dat <- copy(data) %>% 
    arrange(RT1, RT2)
  
  # Initialize the compound column filled with NA values
  dat$Feature <- NA
  i <- 1
  
  for (row in 1:nrow(dat)) {
    # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    rt1 <- dat[row,]$RT1
    rt2 <- dat[row,]$RT2
    ion1 <- dat[row,]$Ion1
    ion2 <- dat[row,]$Ion2
    
    idx_thres <- which(dat$RT1 <= (rt1 + rt1thres) & dat$RT1 >= (rt1 - rt1thres) & 
                         dat$RT2 <= (rt2 + rt2thres) & dat$RT2 >= (rt2 - rt2thres) & 
                         dat$Ion1 <= (ion1 + ion1thres) & dat$Ion1 >= (ion1 - ion1thres) & 
                         dat$Ion2 <= (ion2 + ion2thres) & dat$Ion2 >= (ion2 - ion2thres) &
                         is.na(dat$Feature))
    
    if (identical(idx_thres, integer(0))) {
      next
    }
    else {
      dat[idx_thres, "Feature"] <- paste0("Compound_", i, ".")
      i <- i + 1
    }  
  }
  
  return(dat)
}

# Filtering similar and unique compound 

comp_filter <- function(data) {
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()

  for (comp_grp in unique(data$Feature)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$Feature, fixed = TRUE))
    
    if (length(idx) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    } else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}
```


# STEP 1.1: Data import
```{r, echo = FALSE, message = FALSE, warning = FALSE}
ASTM_list <- read_xlsx("ASTM Compound List.xlsx")
file_path <- "Gasolines_BOP_110424.xlsx"

dfs <- excel_sheets(file_path) %>%
  set_names() %>%
  map(~ read_excel(file_path, sheet = .x) %>% mutate(Sample_name = .x))

df_step1.1 <- bind_rows(dfs) %>%
  select(-c("MF", "RMF", "Area %")) %>%
  mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas_87", 
                              ifelse(str_detect(Sample_name, "B"), "Gas_89",
                                     ifelse(str_detect(Sample_name, "C"), "Gas_91", "Gas_94")))) %>%
  mutate(samp_event = ifelse(str_detect(Sample_name, "b"), "blue",
                             ifelse(str_detect(Sample_name, "p"), "purple", "orange")))

# modify columns names
colnames(df_step1.1)[2] <- "RT1"
colnames(df_step1.1)[3] <- "RT2"
colnames(df_step1.1)[6] <- "Ion3"
colnames(df_step1.1)[8] <- "Ion1"
colnames(df_step1.1)[9] <- "Ion2"
```

# STEP 1.2: Filtering out column bleed, solvent and BTEX and minimum area observations
```{r, echo = FALSE, message = FALSE, warning = FALSE}
filter_list <- c("^Carbon disulfide$", 
                "Cyclotrisiloxane..hexamethyl",
                "^Benzene$",
                "^Toluene$",
                "^Ethylbenzene$",
                "Xylene")

df_step1.2 <- copy(df_step1.1)
for (filter_comp in filter_list) {
  df_step1.2 <- df_step1.2 %>%
      filter(!grepl(filter_comp, Compound))
}


bm_df <- df_step1.1 %>% 
  filter(grepl("Toluene-D8", Compound)) %>%
  filter(RT2 >= 3 & RT2 <= 5)

# Examining Retention time fluctuation of Toluene benchmark 
# RT1 hist plot 
hist(as.numeric(bm_df$RT1), breaks = 10, 
     xlab = "RT1", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")

# RT2 hist plot 
hist(as.numeric(bm_df$RT2), 
     xlab = "RT2", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")
```

# STEP 2: Grouping compounds based on RT1, RT2, Ion1, Ion2
```{r, echo = FALSE, message = FALSE, warning = FALSE}

rt10.1 <- grouping_comp_ver1(df_step1.2 %>% arrange(RT1, RT2),
                             rt1thres = 0.2,
                             rt2thres = 0.15,
                             ion1thres = 0.05, # Ion 1 and 2 indicates molecular structure (2 most prevalent mass-to-charge)
                             ion2thres = 0.05)

# STEP 2: Identify shared and unique compound groups across samples ================================================================================
filter_rt10.1 <- comp_filter(rt10.1)

shared_comp_rt10.1 <- rt10.1[filter_rt10.1[[1]],]
```

### Confirming ASTM at each step of Data processing
```{r, echo = FALSE, message = FALSE, warning = FALSE}
count_ASTM <- function(data, ASTM_list, which_step) {
  count <- 0
  for (r in 1:nrow(ASTM_list)) {
    rt1 <- ASTM_list[r,]$RT1
    rt2 <- ASTM_list[r,]$RT2
    ion1 <- ASTM_list[r,]$Ion1
    ion2 <- ASTM_list[r,]$Ion2
    idx <- which(data$RT1 <= (rt1 + 0.1) & data$RT1 >= (rt1 - 0.1) & 
                   data$RT2 <= (rt2 + 0.15) & data$RT2 >= (rt2 - 0.15) & 
                   # Tolerance ion window of 0.5 for ASTM compounds
                   data$Ion1 <= (ion1 + 0.5) & data$Ion1 >= (ion1 - 0.5) & 
                   data$Ion2 <= (ion2 + 0.5) & data$Ion2 >= (ion2 - 0.5))
    
    if (length(idx) > 0) {
      count <- count + 1
    } else {
      next
    }
  }
  print(paste0("The number of ASTM compounds found at ", which_step ," is: ", count))
}

count_ASTM(bind_rows(df_list_step1.1), ASTM_list, which_step = "STEP 1.1")
count_ASTM(bind_rows(list_remaining_area), ASTM_list, which_step = "STEP 1.2")
count_ASTM(rt10.1, ASTM_list, which_step = "STEP 1.3")
count_ASTM(comp_normalized_rt10.1, ASTM_list, which_step = "STEP 2")
count_ASTM(shared_comp_rt10.1, ASTM_list, which_step = "STEP 3")
```

# STEP 3: Missing value imputation
```{r, echo = FALSE, message = FALSE, warning = FALSE}
normalized_df <- shared_comp_rt10.1 %>%
    dplyr::select(Sample_name,
                  Octane_rating,
                  samp_event,
                  Feature,
                  Area) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    dplyr::group_by(
      Sample_name,
      Octane_rating,
      samp_event,
      Feature) %>%
    dplyr::summarise(across(Area, base::mean)) %>%
    tidyr::pivot_wider(names_from =  Feature,
                       values_from = Area) %>%
    mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station_1",
                                ifelse(str_detect(Sample_name, "F002"), "Station_2",
                                       ifelse(str_detect(Sample_name, "F003"), "Station_3",
                                              ifelse(str_detect(Sample_name, "F004"), "Station_4",
                                                     ifelse(str_detect(Sample_name, "F005"), "Station_5",
                                                            ifelse(str_detect(Sample_name, "F006"), "Station_6",
                                                                   ifelse(str_detect(Sample_name, "F007"), "Station_7",
                                                                          ifelse(str_detect(Sample_name, "F008"), "Station_8",
                                                                                 ifelse(str_detect(Sample_name, "F009"), "Station_9", "Station_10")))))))))) %>%
    tibble::column_to_rownames(., var="Sample_name") %>% 
    relocate(Octane_rating, samp_event, gas_station, .after = 1)

## REMOVING COMPOUND THAT APPEAR IN ONLY 1 SAMPLE ------------
unique_col <- c()
for (col in 3:ncol(normalized_df)) {
  if (sum(!is.na(normalized_df[,col])) == 1) {
    unique_col <- c(unique_col, col)
  }
}

normalized_df <- normalized_df[, -unique_col]

## REMOVE COMPOUNDS THAT HAVE >90% MISSING VALUES ----------------
col_90 <- c()
for (col in 3:ncol(normalized_df)) {
  if (sum(!is.na(normalized_df[,col]))/length(normalized_df[,col]) < 0.1) {
    col_90 <- c(col_90, col)
  }
}

normalized_df <- normalized_df[, -col_90]

## SELECT TOP 150 COMPOUNDS THAT HAVE HIGHEST CUMULATIVE PEAK AREA ACROSS ALL SAMPLES ---------------
# Compute the sum of values of each column
column_sums <- colSums(normalized_df[,3:ncol(normalized_df)])

# Order the columns based on their sum values
ordered_columns <- order(column_sums, decreasing = TRUE)

## Select the top 100 columns with the highest sum value ----------------
top_100_columns <- names(column_sums)[ordered_columns[1:100]]

# Create a new dataframe with only the top 100 columns
normalized_df <- normalized_df[, c("Octane_rating", "samp_event",top_100_columns)]

## SELECT TOP 150 COMPOUNDS THAT HAVE HIGHEST Variance of ACROSS ALL SAMPLES ------------
# Compute variance of values for each column
variances <- apply(normalized_df[,3:ncol(normalized_df)], 2, var)

# Sort variances in descending order and get the indices of the top 100 columns
top_100_indices <- order(variances, decreasing = TRUE)[1:100]

# Select the top 100 columns with highest variance values
top_100_variance <- normalized_df[, c(1, 2, top_100_indices)]

```

## Option 1: Zero
```{r, echo = FALSE, message = FALSE, warning = FALSE}
for (r in 1:nrow(normalized_df)) {
  normalized_df[r, which(base::is.na(normalized_df[r,]))] <- 0
}
```

## Option 2: Minimum 
```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(normalized_df)) { 
  normalized_df[r, which(base::is.na(normalized_df[r,]))] <- min(shared_comp_rt10.1$Area)
} 

```

## Option 3: LOD 
LOD = Multiply the lowest values of each sample by 3

```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(normalized_df[, 3:ncol(normalized_df)])) { 
  normalized_df[, 3:ncol(normalized_df)][r, which(base::is.na(normalized_df[, 3:ncol(normalized_df)][r,]))] <- 3*min(normalized_df[, 3:ncol(normalized_df)][r, which(!base::is.na(normalized_df[, 3:ncol(normalized_df)][r,]))])
} 

```

# STEP 4: Data Normalization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
for (i in 1:length(normalized_df)) {
  comp_normalized_rt10.1 <- as.data.frame(t(apply(normalized_df[, 4:ncol(normalized_df)],
                                                  MARGIN = 1, 
                                                  function(row) {row/sum(row, na.rm = TRUE)}))) 
}

comp_normalized_rt10.1 <- comp_normalized_rt10.1 %>% 
  mutate(Octane_rating = normalized_df$Octane_rating) %>%
  mutate(sampling_season = normalized_df$samp_event) %>%
  mutate(gas_station = normalized_df$gas_station)
```

# Export file for ML
```{r, echo = FALSE, message = FALSE, warning = FALSE}
writexl::write_xlsx(comp_normalized_rt10.1, path = paste0(getwd(), "/arson.xlsx"))
```


# Clustering analyses
## ANOVA 
(Update 15 April 2024:) 
*** With Zero imputation 
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 2.370e-31 1.184e-31   0.603   0.55
Residuals       68 1.335e-29 1.963e-31   

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 1.800e-32 6.160e-33    0.03  0.993
Residuals     67 1.356e-29 2.025e-31 

*** With global minimum imputation
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 6.710e-27 3.354e-27   1.222  0.301
Residuals       68 1.866e-25 2.744e-27  

2. There is no difference between Octane rating
              Df   Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 2.28e-27 7.603e-28   0.267  0.849
Residuals     67 1.91e-25 2.851e-27

==> WHAT's NEXT?: Try to remove compound that appear in only 1 sample and run ANOVA//PCA again

!!! After removing unique cols
*** With Zero imputation
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 8.280e-31 4.141e-31   1.354  0.265
Residuals       68 2.079e-29 3.058e-31 

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 4.420e-31 1.473e-31   0.466  0.707
Residuals     67 2.118e-29 3.161e-31 

*** With global minimum imputation 
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value  Pr(>F)   
sampling_season  2 2.539e-26 1.269e-26    6.71 0.00219 **
Residuals       68 1.286e-25 1.892e-27                   
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 3.250e-27 1.085e-27   0.482  0.696
Residuals     67 1.508e-25 2.250e-27

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(aov(as.formula(paste(paste(setdiff(names(comp_normalized_rt10.1 %>%
                                             select(-"Octane_rating")), "sampling_season"), collapse = "+"), " ~ sampling_season")), data = comp_normalized_rt10.1 %>%
                                             select(-"Octane_rating")))
```

## PCA

(Update 15 April 2024:) When using either Octane_rating OR sampling_season, there is no clustering whatsoever between seasons OR Octane rate

!!! After removing unique cols
*** With global minimum imputation:

!! Tried top 150 compounds with highest cumulative peak area => still no clustering on both sampling seasons and octane rating.

!! Tried top 150 compounds with highst variance 

```{r, echo = FALSE, message = FALSE, warning = FALSE}

res.pca <- FactoMineR::PCA(
  comp_normalized_rt10.1 %>% select(-c("sampling_season", "Octane_rating", "gas_station")),
  # clr_transformed_pca,
  scale.unit = FALSE,
  graph = FALSE)

# Scree plot
fviz_screeplot(res.pca, ncp=10)

# Biplot
factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 10, 
                            habillage = factor(comp_normalized_rt10.1$gas_station),
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
                            ) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'bottom') + coord_equal()

```

## t-SNE
```{r, echo=FALSE, warning = FALSE, message=FALSE}
# REFERENCES VISUALIZATION: 
# https://plotly.com/r/t-sne-and-umap-projections/
# https://distill.pub/2016/misread-tsne/

library(tsne)
library(plotly)

dat <- comp_normalized_rt10.1

features <- subset(dat, select = -c(sampling_season, Octane_rating))

tsne <- tsne(features,
             initial_dims = 3, 
             k = 3, 
             perplexity = 20, # Hyperparameter: perplexity (optimal number of neighbors) < number of samples
             max_iter = 5000
             )

pdb <- cbind(data.frame(tsne), comp_normalized_rt10.1$Octane_rating)
options(warn = -1)
tsne_plot <- plot_ly(data = pdb ,x =  ~X1, y = ~X2, z = ~X3, 
               color = ~comp_normalized_rt10.1$Octane_rating) %>% 
  add_markers(size = 8) %>%
  layout( 
    xaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'), 
    yaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'),
    scene =list(bgcolor = "#e5ecf6"))

tsne_plot
```

## UMAP clustering 

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(umap)

dat <- comp_normalized_rt10.1 

features <- subset(dat, select = -c(sampling_season, Octane_rating
                                    ))

umap <- umap(features, n_components = 3,
             method = 'naive', metric= "manhattan",
             alpha = 0.0001, gamma = 0.0001)

layout <- cbind(data.frame(umap[["layout"]]), comp_normalized_rt10.1$sampling_season)
umap_plot <- plot_ly(layout, x = ~X1, y = ~X2, z = ~X3, 
                color = ~comp_normalized_rt10.1$sampling_season) %>% 
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x-axis'), 
                                   yaxis = list(title = 'y-axis'), 
                                   zaxis = list(title = 'z-axis'))) 
umap_plot
```


## Multiple Wilcoxon tests with p-value correction for multiple testing

Before running multiple Wilcoxon tests, it is recommended to examine whether criteria for univariate parametric test, such as t-test, are violated. If yes, then it is safe to proceed using non-parametric univariate test, such as Wilcoxon test.

First, equal variance and normally distributed between two populations, here are Gas and Diesel using histogram, Q-Q plots. Here, both Gas and Diesel populations are NOT normally distributed.

```{r, echo = TRUE, message = FALSE, warning = FALSE}
GasData <- as.vector(t(cat_5[,c(1:21)])) # 100149 data points
DieselData <- as.vector(t(cat_5[,c(22:25)])) # 19076 data point

# Histogram
hist(GasData, col='steelblue', main='Gas')
hist(DieselData, col='steelblue', main='Diesel')

# Q-Q plots aka. Normal Probability plots
stats::qqnorm(GasData, main='Gas')
stats::qqline(GasData)

stats::qqnorm(DieselData, main='Diesel')
stats::qqline(DieselData)
```

Then, equality of variance between Gas and Diesel populations can be examined using Levene's test and Fligner-Killeen test for non-normally distributed data. Here, for both tests, p values are \< 0.05, and thus, there is significant difference in variances between Gas and Diesel populations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Levene’s test-non-normally distributed data
library(car)
data <- c(GasData, DieselData)
group <- as.factor(c(rep("Gas", times = length(GasData)), rep("Diesel", times = length(DieselData))))
non_norm_dist_data <- data.frame(data, group)
car::leveneTest(data ~ group, data = non_norm_dist_data)

# Fligner-Killeen test
stats::fligner.test(data ~ group, data = non_norm_dist_data)
```
Here, multiple Wilcoxon tests followed by p-value correction for multiple testing was done. Different method for p-value correction from function *p.adjust* from package **stats** were used. After p-value correction, the threshold for p-value can be set (for example,p.adjust \< 0.05 or \< 0.1), to see how it affects the number of significant compounds that can be found. Importantly, the threshold should be set so that it can include as many ASTM reference compounds in the list of significant compounds as possible. For example here, no significant compounds can be found with adjusted p-value \< 0.05. But when adjusted p-value is \< 0.1, 127 significant compounds are found.


### Between gas station
```{r, echo = FALSE, message = FALSE, warning = FALSE}

df <- data.frame(Feature=character(), comparison_pair=character(), pval_wilcox_test=integer())

comp_normalized_rt10.1_filtered <- comp_normalized_rt10.1[!(comp_normalized_rt10.1$gas_station %in% names(table(comp_normalized_rt10.1$gas_station)[table(comp_normalized_rt10.1$gas_station) < 3])),] %>% 
  select(-c("Octane_rating", "sampling_season")) %>%
  relocate(gas_station, .before = 1)

for (c in 2:ncol(comp_normalized_rt10.1_filtered)) {      
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(comp_normalized_rt10.1_filtered$gas_station), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(comp_normalized_rt10.1_filtered$gas_station), 2)[1,col]
    p_2 <- utils::combn(unique(comp_normalized_rt10.1_filtered$gas_station), 2)[2,col]
    
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(as.numeric(unlist(tempdf_filtered[which(comp_normalized_rt10.1_filtered$gas_station == p_1), c])), 
                                    as.numeric(unlist(tempdf_filtered[which(comp_normalized_rt10.1_filtered$gas_station == p_2), c])))$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
  }
}
```

### Between sampling seasons

### Between octane rating