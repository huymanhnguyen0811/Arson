---
title: "Arson Wildfire data analysis workflow"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data")
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation

This repo is accompanying the publication: "The Use of Computational Fingerprinting Techniques to Distinguish Sources of Accelerants Used in Wildfire Arson".

Users need to first install R with this [link](https://cran.r-project.org/mirrors.html) and Rstudio with this [link](https://posit.co/download/rstudio-desktop/).

This workflow ran on Windows 11 OS 11th Gen Intel(R) Core(TM) i7-11800H \@ 2.30GHz, 16 GB RAM;

THe RStudio version used in this demo is 2023.06.0+421 "Mountain Hydrangea" Release for Windows;

The R version used in this demo is 4.3.1

## Data processing

First, the following R packages are installed and loaded in the global environment along with in-house built functions to minimize repetitiveness in the code.

Details about these functions can be found in Data processing.R file in this repo.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(purrr)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)
library(stats)
library(FactoMineR)
library(factoextra)
library(compositions)
library(ggforce)
library(latticeExtra)
library(cluster)
library(umap)
library(plotly)

# Functions -------------------------------------------------------------------------------------------------------
# Notin function
`%notin%` <- Negate(`%in%`)

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp_ver1 <- function(data, rt1thres, rt2thres, ion1thres, ion2thres) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rt1thres: RT1 threshold window
  # rt2thres: RT2 threshold window
  # ion1thres: Ion1 threshold window
  # ion2thres: Ion2 threshold window
  # region_applied: list of x-y coordinate for different regions to applied different threshold window, x-axis is RT1, y-axis is RT2
  dat <- copy(data) %>% 
    arrange(RT1, RT2)
  
  # Initialize the compound column filled with NA values
  dat$Feature <- NA
  i <- 1
  
  for (row in 1:nrow(dat)) {
    # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    rt1 <- dat[row,]$RT1
    rt2 <- dat[row,]$RT2
    ion1 <- dat[row,]$Ion1
    ion2 <- dat[row,]$Ion2
    
    idx_thres <- which(dat$RT1 <= (rt1 + rt1thres) & dat$RT1 >= (rt1 - rt1thres) & 
                         dat$RT2 <= (rt2 + rt2thres) & dat$RT2 >= (rt2 - rt2thres) & 
                         dat$Ion1 <= (ion1 + ion1thres) & dat$Ion1 >= (ion1 - ion1thres) & 
                         dat$Ion2 <= (ion2 + ion2thres) & dat$Ion2 >= (ion2 - ion2thres) &
                         is.na(dat$Feature))
    
    if (identical(idx_thres, integer(0))) {
      next
    }
    else {
      dat[idx_thres, "Feature"] <- paste0("Compound_", i, ".")
      i <- i + 1
    }  
  }
  
  return(dat)
}

# Filtering similar and unique compound 

comp_filter <- function(data) {
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()

  for (comp_grp in unique(data$Feature)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$Feature, fixed = TRUE))
    
    if (length(unique(data[idx,]$Sample_name)) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    } else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}
```


# STEP 1.1: Data import
```{r, echo = FALSE, message = FALSE, warning = FALSE}
ASTM_list <- read_xlsx("ILR Compound List 05-15-2024.xlsx")
file_path <- "Gasolines_BOP_110424.xlsx"

dfs <- excel_sheets(file_path) %>%
  set_names() %>%
  map(~ read_excel(file_path, sheet = .x) %>% mutate(Sample_name = .x))

df_step1.1 <- bind_rows(dfs) %>%
  select(-c("RMF", "Area %")) %>%
  mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas_87", 
                                ifelse(str_detect(Sample_name, "B"), "Gas_89",
                                       ifelse(str_detect(Sample_name, "C"), "Gas_91", "Gas_94")))) %>%
  mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                  ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station_1",
                              ifelse(str_detect(Sample_name, "F002"), "Station_2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station_3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station_4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station_5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station_6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station_7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station_8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station_9", "Station_10")))))))))) 

# modify columns names
colnames(df_step1.1)[colnames(df_step1.1) == '<sup>1</sup>t<sub>R</sub>'] <- 'RT1'
colnames(df_step1.1)[colnames(df_step1.1) == '<sup>2</sup>t<sub>R</sub>'] <- 'RT2'
colnames(df_step1.1)[colnames(df_step1.1) == 'Major'] <- 'Ion1'
colnames(df_step1.1)[colnames(df_step1.1) == 'Qual'] <- 'Ion2'
```

# STEP 1.2: Filtering out column bleed, solvent and BTEX and MF = 0
```{r, echo = FALSE, message = FALSE, warning = FALSE}
filter_list <- c("^Carbon disulfide$", 
                "Cyclotrisiloxane..hexamethyl"
                # "^Benzene$",
                # "^Toluene$",
                # "^Ethylbenzene$",
                # "Xylene"
                )

df_step1.2 <- copy(df_step1.1) %>%
  filter(MF > 0)

for (filter_comp in filter_list) {
  df_step1.2 <- df_step1.2 %>%
      filter(!grepl(filter_comp, Compound))
}
```

### Plot benchmark distribution for aligning compounds later
```{r, echo = FALSE, message = FALSE, warning = FALSE}
bm_df <- df_step1.1 %>% 
  filter(grepl("Toluene-D8", Compound)) %>%
  filter(RT2 >= 3 & RT2 <= 5)

# Examining Retention time fluctuation of Toluene benchmark 
# RT1 hist plot 
hist(as.numeric(bm_df$RT1), breaks = 10, 
     xlab = "RT1", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")

# RT2 hist plot 
hist(as.numeric(bm_df$RT2), 
     xlab = "RT2", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")
```

# QUALITY CONTROL A OF STEP 1.2B: Plot Percentage coverage after removal of limit observation

```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot_a <- list()
i <- 1
for (name in unique(df_step1.2$Sample_name)[50:70]) {
  coverage <- c()
  for (threshold in c(seq(from = 0, to = 250000, by = 50000))) {
    temp <- df_step1.2 %>%
      filter(Sample_name %in% name)
    df_filter_area <- temp %>%
      filter(Area > threshold)
    coverage <- c(coverage, sum(df_filter_area$Area)*100/sum(temp$Area))
  }
  df <- data.frame(thres = seq(from = 0, to = 250000, by = 50000), cover = coverage)
  plot_a[[i]] <- ggplot(data = df,
                      aes(x = thres, y = cover)) +
    geom_col() +
    theme(text = element_text(size = 10)) +
    geom_text(aes(label = round(cover, digits = 3)), color = "green", angle = 90, hjust = 1, size = 5) +
    scale_x_continuous(breaks = seq(from = 0, to = 250000, by = 50000),
                       # remove space between plotted data and xy-axes
                       expand = c(0,0)) +
    scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
                       # remove space between plotted data and xy-axes
                       expand = c(0,0)) +
    ggtitle(name) +
    labs(x = NULL, y = NULL) 
  i <- i + 1
}

y <- textGrob("Percentage coverage of remaining peaks after removal", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Threshold of removal for limit observations", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_a, ncol = 5, 
             left = y,
             bottom = x)
```


# QUALITY CONTROL B OF STEP 1.2B: Plot number of peak remains after removal of limit observation

```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot_b <- list()
i <- 1
for (name in unique(df_step1.2$Sample_name)[50:70]) {
  peak_remain <- c()
  for (threshold in c(seq(from = 0, to = 250000, by = 50000))) {
    temp <- df_step1.2 %>%
      filter(Sample_name %in% name)
    df_filter_area <- temp %>%
      filter(Area > threshold)
    peak_remain <- c(peak_remain, dim(df_filter_area)[1])
  }
  df <- data.frame(thres = seq(from = 0, to = 250000, by = 50000), remain = peak_remain)
  plot_b[[i]] <- ggplot(data = df,
                      aes(x = thres, y = remain)) +
    geom_col() +
    geom_text(aes(label = remain), color = "green", vjust = 1.2, size = 5) +
    scale_x_continuous(breaks = seq(from = 0, to = 250000, by = 50000), 
                       # remove space between plotted data and xy-axes
                       expand = c(0,0)) +
    ggtitle(name) +
    theme(axis.text.x = element_text(size = 20),
          axis.text.y = element_text(size = 20)) +
    labs(x = NULL, y = NULL) +
    theme_classic()
  i <- i + 1
}

y <- textGrob("Number of peak remains after removal of limit observation", rot = 90, gp = gpar(fontsize = 15))
x <- textGrob("Threshold of removal for limit observations", gp = gpar(fontsize = 15))

grid.arrange(grobs = plot_b, ncol = 5, 
             left = y,
             bottom = x)
```



# STEP 2: Grouping compounds based on RT1, RT2, Ion1, Ion2 and Filt

```{r, echo = FALSE, message = FALSE, warning = FALSE}
alignment <- grouping_comp_ver1(df_step1.2 %>%
                               filter(Area > 300000) %>% 
                               arrange(RT1, RT2),
                             rt1thres = 0.3,
                             rt2thres = 0.15,
                             ion1thres = 0.05, # Ion 1 and 2 indicates molecular structure (2 most prevalent mass-to-charge)
                             ion2thres = 0.05)
```

## QUALITY CONTROL: Confirming ASTM at each step of Data processing

(Update 16th May 2024:) 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
count_ASTM <- function(data, ASTM_list, which_step) {
  count <- 0
  for (r in 1:nrow(ASTM_list)) {
    rt1 <- ASTM_list[r,]$RT1
    rt2 <- ASTM_list[r,]$RT2
    ion1 <- ASTM_list[r,]$Ion1
    ion2 <- ASTM_list[r,]$Ion2
    idx <- which(data$RT1 <= (rt1 + 0.3) & data$RT1 >= (rt1 - 0.3) & 
                   data$RT2 <= (rt2 + 0.15) & data$RT2 >= (rt2 - 0.15) & 
                   # Tolerance ion window of 0.5 for ASTM compounds
                   data$Ion1 <= (ion1 + 0.05) & data$Ion1 >= (ion1 - 0.05) & 
                   data$Ion2 <= (ion2 + 0.05) & data$Ion2 >= (ion2 - 0.05))
    
    if (length(idx) > 0) {
      count <- count + 1
    } else {
      next
    }
  }
  print(paste0("The number of ASTM compounds found at ", which_step ," is: ", count))
}

count_ASTM(df_step1.1, ASTM_list, which_step = "STEP 1.1")
count_ASTM(bind_rows(bm_df), ASTM_list, which_step = "STEP 1.2")
count_ASTM(shared_comp_rt10.1, ASTM_list, which_step = "STEP 2")


### Overlaying Scatter plot to confirm ASTM of RT windows
ASTM_subset <- ASTM_list %>% 
  mutate(type = "ASTM") %>% 
  filter(!is.na(Group))

data <- subset.data.frame(alignment, 
                  select = c("Feature", "RT1", "RT2", "Ion1", "Ion2")) %>% 
  mutate(Group = "aligned compounds",
         type = "Sample")

colnames(data)[1] <- "Compound"

# Visual confirmation of appropriate RT windows by checking overlapping of RT1 and RT2 of aligned compounds and ASTMs 
plotly::ggplotly(ggplot(data = data, aes(x = RT1, y = RT2, label = Compound)) +
                   geom_point(pch = 1, size = 0.5) +
                   geom_point(data = ASTM_subset, pch = 4, size = 1.5,
                              aes(x=RT1, y= RT2, 
                                  colour = Group)))

```


# STEP 3: Identify shared and unique compound groups across samples
```{r, echo = FALSE, message = FALSE, warning = FALSE}

filter_alignment <- comp_filter(alignment)

shared_comp_alignment <- alignment[filter_alignment[[1]],]
```

### Catch by compound name
```{r, echo = FALSE, message = FALSE, warning = FALSE}
step3 <- df_step1.2 %>%
  filter(Area > 300000) %>%
  filter(!is.na(Compound))

unique_comp <- c()
# all_unique_compounds_idx <- c()

for (comp_grp in unique(step3$Compound)) {
  idx <- which(grepl(comp_grp, step3$Compound, fixed = TRUE))
  
  if (length(unique(step3[idx,]$Sample_name)) < 2) {
    print(comp_grp)
    unique_comp <- c(unique_comp, comp_grp)
    next
  }
}

shared_comp_alignment <- step3 %>% filter(Compound %notin% unique_comp)
```





# STEP 4: Missing value imputation
(Update 21 April 2024:) After remove compounds appear in only one sample and compounds that have >90% Missing values -> plot histogram of distribution -> most of compounds are < 2e+06 => try filter all peaks that is >= 2e+06

## Investigate data distribution for eremoval of compounds with high mean and low variance

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Investigate data distribution for removal of compounds with high mean and low variance
hist((shared_comp_alignment %>%
  filter(Area < 10000000))$Area)


summary_table <- shared_comp_alignment %>%
  group_by(Compound) %>%
  summarise(
    mean_RT1 = mean(RT1),
    mean_RT2 = mean(RT2),
    mean_Area = mean(Area),
    sd_RT1 = sd(RT1),
    sd_RT2 = sd(RT2),
    sd_Area = sd(Area),
    Ion1 = mean(Ion1),
    Ion2 = mean(Ion2)
  ) %>%
  ungroup() %>%
  mutate(
    compound_name = Compound  # Create a new column for compound_name
  ) %>%
  select(compound_name, mean_Area, sd_Area, mean_RT1, sd_RT1, mean_RT2, sd_RT2, Ion1, Ion2)  # Select columns for the summary table

# Rename columns
colnames(summary_table) <- c("compound_name", "mean", "standard_deviation", "meanRT1", "sdRT1", "meanRT2", "sdRT2", "Ion1", "Ion2")

# Print the summary table
View(summary_table %>% arrange(standard_deviation, desc(mean)))

plotdf <- (summary_table %>% arrange(standard_deviation, desc(mean)))[1:100,]
ggplot(data = plotdf, aes(x=compound_name, y = mean)) +
  geom_bar(stat = 'identity', width = 0.5) +
  geom_errorbar(aes(ymin = mean - standard_deviation,
                    ymax = mean + standard_deviation, colour = "red")) +
  ggtitle("Rank 1:100") +
  theme_classic(base_size = 15) +
  theme(axis.text.x = element_text(angle = 90))
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
df_step3 <- shared_comp_alignment %>%
  # remove top x compounds with high Area and low variance across samples
  filter(Feature %notin% (summary_table %>% arrange(standard_deviation, desc(mean)))[1:100,]$compound_name) %>%
  dplyr::select(
    Sample_name,
    Octane_rating,
    sampling_season,
    gas_station,
    Feature,
    Area) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    Sample_name,
    Octane_rating,
    sampling_season,
    gas_station,
    Feature) %>%
  dplyr::summarise(across(Area, base::mean)) %>%
  tidyr::pivot_wider(names_from = Feature,
                     values_from = Area) %>%
  relocate(
    Octane_rating,
    sampling_season,
    gas_station,
    Sample_name,
    .after = 1)
```

### REMOVE COMPOUNDS THAT HAVE >90% MISSING VALUES ----------------

```{r, echo = FALSE, message = FALSE, warning = FALSE}
col_90na <- c()
for (col in 5:ncol(df_step3)) {
  if (sum(!is.na(df_step3[,col]))/dim(df_step3)[1] < 0.1) {
    col_90na <- c(col_90na, col)
  }
}

if (!is.null(col_90na)) {
  df_step3 <- df_step3[, -col_90na]
}
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
## SELECT TOP 100 COMPOUNDS THAT HAVE HIGHEST CUMULATIVE PEAK AREA ACROSS ALL SAMPLES ---------------
# Compute the sum of values of each column
column_sums <- colMeans(df_step3[,4:ncol(df_step3)])

# Order the columns based on their sum values
# Select the top 100 columns with the highest sum value
top_100_cum_sum <- names(column_sums)[order(column_sums, decreasing = TRUE)[1:100]]

# Create a new dataframe with only the top 100 columns
df_step3 <- df_step3[, c("Octane_rating", "samp_event", "gas_station", top_100_cum_sum)]

## SELECT TOP 150 COMPOUNDS THAT HAVE HIGHEST Variance of ACROSS ALL SAMPLES ------------
# Compute variance of values for each column
variances <- apply(df_step3[,3:ncol(df_step3)], 2, var)

# Sort variances in descending order and get the indices of the top 100 columns
top_100_var <- order(variances, decreasing = TRUE)[1:100]

# Select the top 100 columns with highest variance values
df_step3 <- df_step3[, c(1, 2, top_100_var)]

```

## Option 1: Zero
```{r, echo = FALSE, message = FALSE, warning = FALSE}
for (r in 1:nrow(df_step3)) {
  df_step3[r, which(base::is.na(df_step3[r,]))] <- 0
}
```

## Option 2: Minimum 
```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(df_step3)) { 
  df_step3[r, which(base::is.na(df_step3[r,]))] <- min(shared_comp_alignment$Area)
} 
```

## Option 3: LOD 
LOD = go through each row of the df and replacing missing values multiply the lowest values of each sample by 3

```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(df_step3[, 5:ncol(df_step3)])) { 
  df_step3[r, 5:ncol(df_step3)][,which(base::is.na(df_step3[r, 5:ncol(df_step3)]))] <- 3*min( df_step3[r, 5:ncol(df_step3)][,which(!base::is.na(df_step3[r, 5:ncol(df_step3)]))])
} 

```

## Option 4: Randomized values drawn from uniform distribution of the two global minimum values
```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(df_step3)) { 
  df_step3[r, which(base::is.na(df_step3[r,]))] <- as.list(runif(length(which(is.na(df_step3[r,]))),
                                                                 min = sort(shared_comp_alignment$Area)[1],
                                                                 max = sort(shared_comp_alignment$Area)[2]))
} 
```

# STEP 4: Data Normalization

### Percentage-based normalization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
comp_normalized_rt10.1 <- as.data.frame(t(apply(df_step3[, 5:ncol(df_step3)],
                                                MARGIN = 1, 
                                                function(row) {row/sum(row, na.rm = TRUE)}))) 


comp_normalized_rt10.1 <- comp_normalized_rt10.1 %>% 
  mutate(Octane_rating = df_step3$Octane_rating) %>%
  mutate(sampling_season = df_step3$sampling_season) %>%
  mutate(gas_station = df_step3$gas_station) %>%
  mutate(Sample_name = df_step3$Sample_name) %>%
  relocate(Octane_rating, sampling_season, gas_station, Sample_name, .before = 1)
```


### Log normalization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
log_normalized_df <- as.data.frame(t(apply(df_step3[, 5:ncol(df_step3)], 1, function(x) log(x))))

comp_normalized_rt10.1 <- log_normalized_df %>% 
  mutate(Octane_rating = df_step3$Octane_rating) %>%
  mutate(sampling_season = df_step3$sampling_season) %>%
  mutate(gas_station = df_step3$gas_station) %>%
  mutate(Sample_name = df_step3$Sample_name) %>%
  relocate(Octane_rating, sampling_season, gas_station, Sample_name, .before = 1)
```

# Export file for ML
```{r, echo = FALSE, message = FALSE, warning = FALSE}
writexl::write_xlsx(comp_normalized_rt10.1, path = paste0(getwd(), "/python/arson_min.xlsx"))
```


# Clustering analyses
## ANOVA 
(Update 15 April 2024:) 
*** With Zero imputation 
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 2.370e-31 1.184e-31   0.603   0.55
Residuals       68 1.335e-29 1.963e-31   

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 1.800e-32 6.160e-33    0.03  0.993
Residuals     67 1.356e-29 2.025e-31 

*** With global minimum imputation
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 6.710e-27 3.354e-27   1.222  0.301
Residuals       68 1.866e-25 2.744e-27  

2. There is no difference between Octane rating
              Df   Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 2.28e-27 7.603e-28   0.267  0.849
Residuals     67 1.91e-25 2.851e-27

==> WHAT's NEXT?: Try to remove compound that appear in only 1 sample and run ANOVA//PCA again

!!! After removing unique cols
*** With Zero imputation
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 8.280e-31 4.141e-31   1.354  0.265
Residuals       68 2.079e-29 3.058e-31 

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 4.420e-31 1.473e-31   0.466  0.707
Residuals     67 2.118e-29 3.161e-31 

*** With global minimum imputation 
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value  Pr(>F)   
sampling_season  2 2.539e-26 1.269e-26    6.71 0.00219 **
Residuals       68 1.286e-25 1.892e-27                   
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 3.250e-27 1.085e-27   0.482  0.696
Residuals     67 1.508e-25 2.250e-27

(Update 22nd April 2024:) There is no difference between gas stations
            Df    Sum Sq   Mean Sq F value Pr(>F)
gas_station  9 4.311e-30 4.790e-31   0.926  0.509
Residuals   61 3.155e-29 5.171e-31 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(aov(as.formula(paste(paste(setdiff(names(comp_normalized_rt10.1 %>%
                                             select(-c("Octane_rating", "sampling_season"))), "gas_station"), collapse = "+"), " ~ gas_station")), data = comp_normalized_rt10.1 %>%
                                             select(-c("Octane_rating", "sampling_season"))))
```

## PCA

(Update 15 April 2024:) When using either Octane_rating OR sampling_season, there is no clustering whatsoever between seasons OR Octane rate

!!! After removing unique cols
*** With global minimum imputation:

!! Tried top 150 compounds with highest cumulative peak area => still no clustering on both sampling seasons and octane rating.

!! Tried top 150 compounds with highest variance 

(Update 29th April:) 
**DATA COMPRESSION OR NOT??**
If I **compress all samples into a grouping variables**, such as Octane_rating or sampling_season, I can clearly see the differentiation between these grouping variables. 

**For gas station, ** all 3 imputation techniques all resulted in low Dim1 and Dim2. However, with Zero and Global Min, there seems to be a grouping of gas station no. 2,8,3 (group 1); no. 1, 5 (group 2), no. 10, 4, 9, 7, 6 (group 3).

***If I keep the samples separated*** , then I tried log normalization with 3 imputation techniques ==> Zero imputation will resulted in -Inf after Log-normalization so we can only do Global min and LOD. However, both global min and LOD does nto result in any clear grouping/clustering between "sampling_season", "Octane_rating", "gas_station".

**LOG NORMALIZATION VERSUS. PERCENTAGE-BASED NORMALIZATION**
I also tried remove the top 100 compounds that have really high mean and low variance across sample and perform zero//global min// LOD imputation on them and then tried Log-normalization ==> still no clustering between gas station!!

Also, I tried to run PCA with un-normalized data (after imputation) with 3 imputation methods but still no clustering of gas stations across all 3 imputation methods.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
res.pca <- FactoMineR::PCA(
  df_step3[, 5:ncol(df_step3)],
  scale.unit = TRUE,
  graph = FALSE)

# Scree plot
fviz_screeplot(res.pca, ncp=10)

# Biplot
factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 10, 
                            habillage = factor(df_step3$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name"
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
                            ) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'bottom') + coord_equal()
```

## HCA
```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- comp_normalized_rt10.1[, -c(1:4)]

## Dissimilarity Indices Calculated by vegan::vegdist()
hca_samp <- stats::hclust(vegan::vegdist(hc_df,
                                         method = "robust.aitchison")) # Since our data is continous -> canberra // manhattan // robust.aitchison

plot(hca_samp,
     labels = comp_normalized_rt10.1$Sample_name,
     hang = -1,
     main = "", cex = 1.25)
```    

## t-SNE

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# REFERENCES VISUALIZATION: 
# https://plotly.com/r/t-sne-and-umap-projections/
# https://distill.pub/2016/misread-tsne/

library(tsne)
library(plotly)

features <- subset(comp_normalized_rt10.1, select = 5:ncol(df_step3))

tsne <- tsne(features,
             initial_dims = 3, 
             k = 3, 
             perplexity = 30, # Hyperparameter: perplexity (optimal number of neighbors) < number of samples
             max_iter = 5000
             )

pdb <- cbind(data.frame(tsne), comp_normalized_rt10.1$Octane_rating)
options(warn = -1)
tsne_plot <- plot_ly(data = pdb ,x =  ~X1, y = ~X2, z = ~X3, 
               color = ~comp_normalized_rt10.1$Octane_rating) %>% 
  add_markers(size = 8) %>%
  layout( 
    xaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'), 
    yaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'),
    scene =list(bgcolor = "#e5ecf6"))

tsne_plot
```

## UMAP clustering 

```{r, echo=FALSE, warning = FALSE, message=FALSE}
features <- comp_normalized_rt10.1[, -c(1:4)]

umap <- umap(features, n_components = 3,
             alpha = 0.0001, gamma = 0.0001,
             metric = "pearson2") # euclidean, manhattan, cosine, pearson, pearson2

layout <- cbind(data.frame(umap[["layout"]]), comp_normalized_rt10.1$Octane_rating)
umap_plot <- plot_ly(layout, x = ~X1, y = ~X2, z = ~X3, 
                color = ~comp_normalized_rt10.1$Octane_rating) %>% 
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x-axis'),
                                   yaxis = list(title = 'y-axis'),
                                   zaxis = list(title = 'z-axis')))
umap_plot
```


## Multiple Wilcoxon tests with p-value correction for multiple testing

Before running multiple Wilcoxon tests, it is recommended to examine whether criteria for univariate parametric test, such as t-test, are violated. If yes, then it is safe to proceed using non-parametric univariate test, such as Wilcoxon test.

First, equal variance and normally distributed between two populations, here are Gas and Diesel using histogram, Q-Q plots. Here, both Gas and Diesel populations are NOT normally distributed.

```{r, echo = TRUE, message = FALSE, warning = FALSE}
# Histogram
hist(, col='steelblue', main='')
hist(, col='steelblue', main='')

# Q-Q plots aka. Normal Probability plots
stats::qqnorm(, main='')
stats::qqline()

stats::qqnorm(, main='')
stats::qqline()
```

Then, equality of variance between Gas and Diesel populations can be examined using Levene's test and Fligner-Killeen test for non-normally distributed data. Here, for both tests, p values are \< 0.05, and thus, there is significant difference in variances between Gas and Diesel populations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Levene’s test-non-normally distributed data
library(car)
data <- c(GasData, DieselData)
group <- as.factor(c(rep("Gas", times = length(GasData)), rep("Diesel", times = length(DieselData))))
non_norm_dist_data <- data.frame(data, group)
car::leveneTest(data ~ group, data = non_norm_dist_data)

# Fligner-Killeen test
stats::fligner.test(data ~ group, data = non_norm_dist_data)
```


### Between gas station

(Update 14th May 2024:) 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
sig_comp <- list()
j <- 1

df <- data.frame(Feature=character(), comparison_pair=character(), pval_wilcox_test=integer())

df_filtered <- comp_normalized_rt10.1[!(comp_normalized_rt10.1$Octane_rating %in% names(table(comp_normalized_rt10.1$Octane_rating)[table(comp_normalized_rt10.1$Octane_rating) < 3])),] %>% 
  select(-c("sampling_season", "gas_station")) %>%
  relocate(Octane_rating, .before = 1)

for (c in 2:ncol(df_filtered)) {      
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(df_filtered$Octane_rating), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(df_filtered$Octane_rating), 2)[1,col]
    p_2 <- utils::combn(unique(df_filtered$Octane_rating), 2)[2,col]
    
    # calculating the p-value between each gas station pair 
    pval_wilcox_test <- wilcox.test(as.numeric(unlist(df_filtered[which(df_filtered$Octane_rating == p_1), 2])), # number "2" here is because we slice in the 2nd column of the subset dataframe
                                    as.numeric(unlist(df_filtered[which(df_filtered$Octane_rating == p_2), 2])))$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(df_filtered)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
  }
}

pvaluecorrect <- c("holm", "hochberg", "hommel", "BH", "BY")
for (m in pvaluecorrect) {
  df$adjusted_pvalue <- stats::p.adjust(df$pval_wilcox_test, method = m)
  print(df %>%
          filter(., adjusted_pvalue < 0.05) %>%
          arrange(adjusted_pvalue))
  
  sig_comp[[j]] <- df
  j <- j + 1
}
```
