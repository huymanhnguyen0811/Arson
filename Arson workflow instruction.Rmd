---
title: "Arson Wildfire data analysis workflow"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data")
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation

This repo is accompanying the publication: "The Use of Computational Fingerprinting Techniques to Distinguish Sources of Accelerants Used in Wildfire Arson".

Users need to first install R with this [link](https://cran.r-project.org/mirrors.html) and Rstudio with this [link](https://posit.co/download/rstudio-desktop/).

This workflow ran on Windows 11 OS 11th Gen Intel(R) Core(TM) i7-11800H \@ 2.30GHz, 16 GB RAM;

THe RStudio version used in this demo is 2023.06.0+421 "Mountain Hydrangea" Release for Windows;

The R version used in this demo is 4.3.1

First, the following R packages are installed and loaded in the global environment along with in-house built functions to minimize repetitiveness in the code.

Details about these functions can be found in Data processing.R file in this repo.

## Loading packages
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(purrr)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)
library(stats)
library(FactoMineR)
library(factoextra)
library(ggforce)
library(caret)
library(xgboost)
library(lightgbm)
library(pROC)
library(dplyr)
library(missForest)      # for random forest imputation
library(VIM)             # for kNN imputation (kNN())
library(randomForest)
library(caret)           # for model training & cross-validation
library(stats)           # for basic stats tests, prcomp, etc.
library(pheatmap)
library(viridis)
library(ggsignif)
library(ggpubr)
# Notin function
`%notin%` <- Negate(`%in%`)
```


# STEP 1.1: Data import
```{r, echo = FALSE, message = FALSE, warning = FALSE}
target_comp <- read_xlsx(path = "Shortened ILR Compound List PF001A 07-06-2024.xlsx")
# ASTM <- read_xlsx(path = "ILR Compound List 05-15-2024_Without DieselASTM.xlsx")
file_path <- "Gasolines_BOP_110424.xlsx"

dfs <- excel_sheets(file_path) %>%
  set_names() %>%
  map(~ read_excel(file_path, sheet = .x) %>% mutate(Sample_name = .x))

df_step1.1 <- bind_rows(dfs) %>%
  dplyr::select(-c("RMF", "Area %")) %>%
  mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas_87", 
                                ifelse(str_detect(Sample_name, "B"), "Gas_89",
                                       ifelse(str_detect(Sample_name, "C"), "Gas_91", "Gas_94")))) %>%
  mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                  ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station_1",
                              ifelse(str_detect(Sample_name, "F002"), "Station_2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station_3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station_4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station_5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station_6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station_7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station_8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station_9", "Station_10")))))))))) 

# modify columns names
colnames(df_step1.1)[colnames(df_step1.1) == '<sup>1</sup>t<sub>R</sub>'] <- 'RT1'
colnames(df_step1.1)[colnames(df_step1.1) == '<sup>2</sup>t<sub>R</sub>'] <- 'RT2'
colnames(df_step1.1)[colnames(df_step1.1) == 'Major'] <- 'Ion1'
colnames(df_step1.1)[colnames(df_step1.1) == 'Qual'] <- 'Ion2'
```

# STEP 1.2: Filtering out column bleed, solvent and BTEX and MF = 0

```{r, echo = FALSE, message = FALSE, warning = FALSE}
filter_list <- c("^Carbon disulfide$", 
                "Cyclotrisiloxane..hexamethyl",
                "Cyclotetrasiloxane..octamethyl"
                # "^Benzene$",
                # "^Toluene$",
                # "^Ethylbenzene$",
                # "Xylene"
                )

# ^Carbon disulfide$ 75.890 - 75.959, 77.881 - 77.948

df_step1.2 <- copy(df_step1.1) # %>%
  # filter(MF > 0)

for (filter_comp in filter_list) {
  df_step1.2 <- df_step1.2 %>%
      filter(!grepl(filter_comp, Compound))
}

df_step2 <- df_step1.2 %>%
  filter(Area > 300000) %>%
  arrange(RT1, RT2)
```

#### Quality assurance: Histogram distribution of Peak values before data normalization
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
ggplot(data = df_step2) +
  geom_histogram(aes(x= Area)) +
  facet_wrap(~Sample_name) +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5) 
        # strip.text = element_blank()
        )
```

### Plot benchmark distribution for aligning compounds later
```{r, echo = FALSE, message = FALSE, warning = FALSE}
bm_df <- df_step1.1 %>% 
  filter(grepl("Toluene-D8", Compound)) %>%
  filter(RT2 >= 3 & RT2 <= 5)

# Examining Retention time fluctuation of Toluene benchmark 
# RT1 hist plot 
hist(as.numeric(bm_df$RT1), breaks = 10, 
     xlab = "RT1", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")

# RT2 hist plot 
hist(as.numeric(bm_df$RT2), 
     xlab = "RT2", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")
```

# QUALITY CONTROL A OF STEP 1.2B: Plot Percentage coverage after removal of limit observation

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# df <- data.frame(list(thres = c(seq(from = 0, to = 300000, by = 50000)), cover = coverage_list))
# plot <- ggplot(data = df,
#        aes(x = thres, y = cover)) +
#   geom_col(fill = "skyblue") +
#   geom_text(aes(label = round(cover, digits = 3)), color = "black", angle = 90, hjust = 2, size = 10) +
#   scale_x_continuous(breaks = seq(from = 0, to = 300000, by = 50000),
#                      # remove space between plotted data and xy-axes
#                      expand = c(0,0)) +
#   scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
#                      # remove space between plotted data and xy-axes
#                      expand = c(0,0)) +
#   theme_classic(base_size = 20) +
#   theme(axis.text.x = element_text(size = 20),
#         axis.text.y = element_text(size = 20),
#         strip.text = element_text(size = 30, face = "bold"),   # Make facet label text bold
#         strip.background = element_blank()) +
#   labs(x = "Threshold of removal for limit observations", 
#        y = "Percentage coverage of remaining peaks after removal (%)") 
# 
# ggsave(filename = paste0("Percentage coverage_", format(Sys.time(), "%d-%m-%Y"),".png"), 
#        plot = plot, 
#        device = "png",
#        dpi = 600,        # Set DPI to 600
#        width = 13,       # Width in inches (adjust as needed)
#        height = 8,       # Height in inches (adjust as needed)
#        units = "in") 

# plot_a <- list()
# for (threshold in c(seq(from = 0, to = 300000, by = 50000))) {
# for (name in unique(df_step1.2$Sample_name)[50:70]) {
#   coverage <- c()
#   for (threshold in c(seq(from = 0, to = 300000, by = 50000))) {
#     temp <- df_step1.2 %>%
#       filter(Sample_name %in% name)
#     df_filter_area <- temp %>%
#       filter(Area > threshold)
#     coverage <- c(coverage, sum(df_filter_area$Area)*100/sum(temp$Area))
#   }
#   df <- data.frame(thres = seq(from = 0, to = 300000, by = 50000), cover = coverage)
#   plot_a[[i]] <- ggplot(data = df,
#                       aes(x = thres, y = cover)) +
#     geom_col() +
#     theme(text = element_text(size = 10)) +
#     geom_text(aes(label = round(cover, digits = 3)), color = "green", angle = 90, hjust = 1, size = 5) +
#     scale_x_continuous(breaks = seq(from = 0, to = 300000, by = 50000),
#                        # remove space between plotted data and xy-axes
#                        expand = c(0,0)) +
#     scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
#                        # remove space between plotted data and xy-axes
#                        expand = c(0,0)) +
#     ggtitle(name) +
#     labs(x = NULL, y = NULL) 
#   i <- i + 1
# }

# y <- textGrob("Percentage coverage of remaining peaks after removal", rot = 90, gp = gpar(fontsize = 20))
# x <- textGrob("Threshold of removal for limit observations", gp = gpar(fontsize = 20))
# 
# grid.arrange(grobs = plot_a, ncol = 5, 
#              left = y,
#              bottom = x)
```


# Figure S1 - QUALITY CONTROL OF STEP 1.2B: Plot number of peak remains after removal of limit observation with Percentage coverage after removal of limit observation (on top of each bar)

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Calculate the mean/median percentage of all sample in df_step1.2
coverage_list <- c()
for (threshold in c(seq(from = 0, to = 1000000, by = 50000))) {
  df_filter_area <- df_step1.2 %>%
    dplyr::filter(Area > threshold) %>%
    dplyr::group_by(Sample_name) %>%
    dplyr::summarise(across(Area, base::sum))
  
  coverage <- c()
  for (sample in df_filter_area$Sample_name) {
    coverage <- c(coverage, df_filter_area[which(df_filter_area$Sample_name == sample),]$Area*100/sum(df_step1.2[which(df_step1.2$Sample_name == sample),]$Area))
  }
  coverage_list <- c(coverage_list, mean(coverage))
}

num_comp_list <- c()

# Precompute sample counts for all thresholds using vectorized operations
for (threshold in seq(from = 0, to = 1000000, by = 50000)) {
  
  # Filter the data based on threshold
  df_filter_area <- df_step1.2 %>%
    dplyr::filter(Area > threshold) %>%
    dplyr::group_by(Sample_name) %>%
    dplyr::summarize(num_comp = n())   # Count occurrences per Sample_name
  
  # Compute the mean of num_comp and store it in num_comp_list
  num_comp_list <- c(num_comp_list, mean(df_filter_area$num_comp))
}

df <- data.frame(list(thres = c(seq(from = 0, to = 1000000, by = 50000)), remain = num_comp_list))
plot <- ggplot(data = df,
               aes(x = thres, y = remain)) +
  geom_col(fill = "skyblue") +
  geom_text(aes(label = round(remain, digits = 0)), color = "black", 
            angle = 90, hjust = 1.5, size = 9) +
  geom_text(aes(label = paste0(round(coverage_list, digits =1),"%"), 
                y = remain + 5),  # Adjust 'y = remain + 5' for positioning above bars
            color = "black", size = 9, vjust = 0) +     # Adjust vjust for fine-tuning
  scale_x_continuous(breaks = seq(from = 0, to = 1000000, by = 50000),
                     # remove space between plotted data and xy-axes
                     expand = c(0,0)) +
  # scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
  #                    # remove space between plotted data and xy-axes
  #                    expand = c(0,0)) +
  theme_classic(base_size = 25) +
  theme(axis.text.x = element_text(size = 25, angle = 90, vjust = 0.5),
        axis.text.y = element_text(size = 25),
        strip.text = element_text(size = 30, face = "bold"),   # Make facet label text bold
        strip.background = element_blank()) +
  labs(x = "Stepwise threshold of peak area for removal of compounds with low signal to noise", 
       y = "Number of peaks remains after applying thresholds of peak area") 
plot

ggsave(filename = paste0("Figure 1_Number of compound remain & Percentage coverage_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 25,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in")


# plot_b <- list()
# i <- 1
# for (name in unique(df_step1.2$Sample_name)[50:70]) {
#   peak_remain <- c()
#   for (threshold in c(seq(from = 0, to = 300000, by = 50000))) {
#     temp <- df_step1.2 %>%
#       filter(Sample_name %in% name)
#     df_filter_area <- temp %>%
#       filter(Area > threshold)
#     peak_remain <- c(peak_remain, dim(df_filter_area)[1])
#   }
#   df <- data.frame(thres = seq(from = 0, to = 300000, by = 50000), remain = peak_remain)
#   plot_b[[i]] <- ggplot(data = df,
#                       aes(x = thres, y = remain)) +
#     geom_col() +
#     geom_text(aes(label = remain), color = "green", vjust = 1.2, size = 5) +
#     scale_x_continuous(breaks = seq(from = 0, to = 300000, by = 50000), 
#                        # remove space between plotted data and xy-axes
#                        expand = c(0,0)) +
#     ggtitle(name) +
#     theme(axis.text.x = element_text(size = 20),
#           axis.text.y = element_text(size = 20)) +
#     labs(x = NULL, y = NULL) +
#     theme_classic()
#   i <- i + 1
# }

# y <- textGrob("Number of peak remains after removal of limit observation", rot = 90, gp = gpar(fontsize = 15))
# x <- textGrob("Threshold of removal for limit observations", gp = gpar(fontsize = 15))
# 
# grid.arrange(grobs = plot_b, ncol = 5, 
#              left = y,
#              bottom = x)
```



## QUALITY CONTROL TO SELECT ALIGNMENT WINDOW: Confirming targeted compounds at each step of Data processing

(Update 22nd May 2024:) Using the target compound list, we will review 18 samples (6 samples across 3 seasons (p,b,o)) to determine how many samples have the target compounds. This data will help determine the effectiveness of the compression - similar to your microplastic table. 

We want to make sure that it works for all potential types of fuels. 

For the reporting table, there will be cases where the target compounds might not occur in some samples. The best alignment window will be the one that have highest  **"Proportion"** = samples have the target compounds/ 71 (total number of samples in the dataset).

Then, I pick the combination of Rt1 and Rt2 that results in the highest cumulative sum of the **"Proportion"** from all target compound by using **rowSums**.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# For each target compounds,  Apply different alignment windows and record number of samples that have the =target compounds
combi <- tidyr::crossing(
  # RT1
  c(0.1 ,0.15, 0.2, 0.25, 0.3, 0.35, 0.4), 
  # RT2
  c(0.12,0.13,0.14,0.15, 0.16, 0.17, 0.18, 0.19, 0.2))

df <- data.frame(RT1_window=integer(), RT2_window=integer(), target=character(), proportion=integer())
  
for (i in 1:nrow(combi)) {
  for (j in 1:nrow(target_comp)) {
    # Catch all peaks in dataframe that falling into the window with target compounds as center of the window
    idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                    
                   (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                   df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
    
    temp <- df_step2[idx1,]
    minrt1 <- max(temp$RT1)
    maxrt1 <- min(temp$RT1)
    minrt2 <- max(temp$RT2)
    maxrt2 <- max(temp$RT2)
    
    
    idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + as.numeric(combi[i,][1])) & 
                    temp$RT1 >= (target_comp[j,]$RT1 - as.numeric(combi[i,][1])) &
                    temp$RT2 <= (target_comp[j,]$RT2 + as.numeric(combi[i,][2])) & 
                    temp$RT2 >= (target_comp[j,]$RT2 - as.numeric(combi[i,][2])))
    
    df[nrow(df) + 1,] <- c(as.numeric(combi[i,][1]),
                           as.numeric(combi[i,][2]),
                           target_comp[j,]$Compound,
                           # paste0(length(unique(temp[idx2,]$Sample_name)), "/", 71))
                           100*(length(unique(temp[idx2,]$Sample_name)) / length(unique(temp$Sample_name))))
  }
}

summary_df <- df %>% pivot_wider(names_from = target, values_from = proportion)

# Which window have the highest proportion of samples that have the target compounds
summary_df[, 3:ncol(summary_df)] <- lapply(summary_df[, 3:ncol(summary_df)], as.numeric)
max_row <- summary_df[which.max(rowSums(summary_df[, 3:ncol(summary_df)])), ]
print(max_row)

# Adding the number of samples where the compounds were found with matching Ion1 and Ion2 to colnames of each compound
i <- 3
for (j in 1:nrow(target_comp)) {
  idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                  
                  (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                     df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
  
  temp <- df_step2[idx1,]
  idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + as.numeric(max_row$RT1_window)) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - as.numeric(max_row$RT1_window)) &
                  temp$RT2 <= (target_comp[j,]$RT2 + as.numeric(max_row$RT2_window)) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - as.numeric(max_row$RT2_window)))
  
  colnames(summary_df)[i] <- paste0(target_comp[j,]$Compound, " (n = ", length(unique(temp$Sample_name)), ")")
  i <- i + 1 
}

writexl::write_xlsx(summary_df, path = paste0(getwd(), "/Testing combinations of Retention time window for compound alignment.xlsx"))
```


# QUALITY CONTROL: Examine the distribution of RT of target compounds after alignment with pF001A

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Get the column names from the existing data frame
column_names <- c( "Target compound", "RT", "Retention time of pF001A", "Min.", "1st Qu.", "Median",  "Mean", "3rd Qu.", "Max.", "Max. RT - Min. RT")
# Create an empty data frame with the same column names
summary_df <- data.frame(matrix(ncol = length(column_names), nrow = 0))
colnames(summary_df) <- column_names

summary_target_compounds <- list()
for (j in 1:nrow(target_comp)) {
    # First for each target compound, matching Major and minor ion 
    idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.2) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.2) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.2) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.2)) |
                    
                   (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.2) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.2) &
                   df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.2) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.2)))
    
    temp <- df_step2[idx1,]
    
    # Then, for each target compound, matching within RT +- 0.1 wrt the pF001A retention time. 
    idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + 0.1) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - 0.1) &
                  temp$RT2 <= (target_comp[j,]$RT2 + 0.1) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - 0.1))
    
    
    
    if (nrow(temp[idx2, ]) == 0) {
      summary_df[nrow(summary_df) + 1,] <- c(paste0(target_comp[j,]$Compound, " was not found with matching ions and within RT1/RT2 windows of 0.1 of pF001A"), NA, NA, NA, NA, NA, NA, NA, NA, NA)
    } else {
      
      # Make descriptive stats summary of RT1 of all peaks that was aligned to target compound in pF001A
      summary_df[nrow(summary_df) + 1,] <- c(target_comp[j,]$Compound,
                                             "RT1",
                                             as.numeric(target_comp[j,]$RT1),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[1]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[2]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[3]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[4]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[5]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[6]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[6]] - summary(as.numeric(temp[idx2,]$RT1))[[1]]))
      
      # Make descriptive stats summary of RT2 of all peaks that was aligned to target compound in pF001A
      summary_df[nrow(summary_df) + 1,] <- c(target_comp[j,]$Compound,
                                             "RT2",
                                             as.numeric(target_comp[j,]$RT2),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[1]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[2]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[3]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[4]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[5]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[6]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[6]] - summary(as.numeric(temp[idx2,]$RT2))[[1]]))
      # Make histogram of distribution of RT of these target compound
      # par(mfrow = c(1, 2))
      # hist(as.numeric(temp[idx2,]$RT1),
      #      xlab = "RT1", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 1,
      #      main = paste0("RT1 of ", target_comp[j,]$Compound, "\n with pF001A's RT1 = ", target_comp[j,]$RT1, " and RT1 window = 0.1"))
      # 
      # hist(as.numeric(temp[idx2,]$RT2), 
      #      xlab = "RT2", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 1,
      #      main = paste0("RT2 of ", target_comp[j,]$Compound, "\n in pF001A's RT2 = ", target_comp[j,]$RT2, " and RT2 window = 0.1"))
      
      # Reset the layout to default
      # par(mfrow = c(1, 1))
    }
}

View(summary_df)

writexl::write_xlsx(summary_df, path = paste0(getwd(), "/summary of target compounds matching by major and minor ions and rt1_rt2 of (0.1)_", format(Sys.Date(), "%d-%m-%y"), ".xlsx"))
```


# Make a table for the compounds that was not found in all samples, give the names of the samples where these target compounds are missing
***If the compound can be found in all samples then plot it with x-axis as 71 samples sorted from gas_station 1 to 10, the legend is the sampling season***

(Update 31 May 2024): For the bar plot of target compounds, if the compounds were not presented in a station -> it will have zeros values instead of NA -> it is show in the plot that the compound was not there in certain gas stations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary_df2 <- data.frame(target=character(), samples_that_missing_target_compound=character())

df_step2 <- df_step2 %>% 
  mutate(gas_station = factor(gas_station, levels = unique(df_step2$gas_station)))

for (j in 1:nrow(target_comp)) {
  idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                  
                  (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                     df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
  
  temp <- df_step2[idx1,]
  idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + as.numeric(max_row$RT1_window)) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - as.numeric(max_row$RT1_window)) &
                  temp$RT2 <= (target_comp[j,]$RT2 + as.numeric(max_row$RT2_window)) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - as.numeric(max_row$RT2_window)))
  
  if (length(unique(temp[idx2,]$Sample_name)) < 71) {
    summary_df2[nrow(summary_df2) + 1,] <- c(target_comp[j,]$Compound,
                                             paste(setdiff(unique(df_step2$Sample_name), 
                                                           unique(temp[idx2,]$Sample_name)), 
                                                   collapse = ", "))
    
    # Artificial add in the missing samples and put the Area of those sample to be 0
    empty_df <- temp[idx2,] %>% select(Area, Sample_name, Octane_rating, sampling_season, gas_station) %>% slice(0)
    new_rows <- data.frame(
      Area = rep(0, length(setdiff(unique(df_step2$Sample_name), 
                                   unique(temp[idx2,]$Sample_name)))),
      Sample_name = setdiff(unique(df_step2$Sample_name), 
                     unique(temp[idx2,]$Sample_name)),
      Octane_rating = rep(NA, length(setdiff(unique(df_step2$Sample_name), 
                                            unique(temp[idx2,]$Sample_name)))),
      sampling_season = rep(NA, length(setdiff(unique(df_step2$Sample_name), 
                                              unique(temp[idx2,]$Sample_name)))),
      gas_station = rep(NA, length(setdiff(unique(df_step2$Sample_name), 
                                          unique(temp[idx2,]$Sample_name))))
    )
    temp2 <- bind_rows(empty_df, new_rows) %>%
      mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas_87", 
                                    ifelse(str_detect(Sample_name, "B"), "Gas_89",
                                           ifelse(str_detect(Sample_name, "C"), "Gas_91", "Gas_94")))) %>%
      mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                      ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
      mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station_1",
                                  ifelse(str_detect(Sample_name, "F002"), "Station_2",
                                         ifelse(str_detect(Sample_name, "F003"), "Station_3",
                                                ifelse(str_detect(Sample_name, "F004"), "Station_4",
                                                       ifelse(str_detect(Sample_name, "F005"), "Station_5",
                                                              ifelse(str_detect(Sample_name, "F006"), "Station_6",
                                                                     ifelse(str_detect(Sample_name, "F007"), "Station_7",
                                                                            ifelse(str_detect(Sample_name, "F008"), "Station_8",
                                                                                   ifelse(str_detect(Sample_name, "F009"), "Station_9", "Station_10")))))))))) %>% 
      mutate(new_sample_name = paste0(Sample_name, "_", gas_station, "_", Octane_rating))
    
    plotdata <- bind_rows(temp2, temp[idx2,] %>% 
                            select(Area, Sample_name, Octane_rating, sampling_season, gas_station) %>%
                            mutate(new_sample_name = paste0(Sample_name, "_", gas_station, "_", Octane_rating)))
    
    print(ggplot(data = plotdata, aes(x = new_sample_name , y = Area, fill = sampling_season)) + 
            geom_bar(stat = "identity") +
            labs(title = target_comp[j,]$Compound, 
                 x = "Samples", y = "Peak Area",
                 fill = "") +
            theme_minimal(base_size = 15) +
            theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
                  plot.title = element_text(size = 25),
                  axis.ticks.length.x = unit(0, "cm"),
                  panel.grid.minor.x = element_blank(),
                  panel.grid.minor.y = element_blank()) + 
            scale_y_continuous(expand = c(0,0)))
  } else {
    temp2 <- temp[idx2,] %>%
      mutate(new_sample_name = paste0(Sample_name, "_", gas_station, "_", Octane_rating))
    print(ggplot(data = temp2, aes(x = new_sample_name , y = Area, fill = sampling_season)) + 
            geom_bar(stat = "identity") +
            labs(title = target_comp[j,]$Compound, 
                 x = "Samples", y = "Peak Area",
                 fill = "") +
            theme_minimal(base_size = 15) +
            theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
                  plot.title = element_text(size = 25),
                  axis.ticks.length.x = unit(0, "cm"),
                  panel.grid.minor.x = element_blank(),
                  panel.grid.minor.y = element_blank()) + 
            scale_y_continuous(expand = c(0,0)))
  }
}

writexl::write_xlsx(summary_df2, path = paste0(getwd(), "/Samples where target compounds are missing.xlsx"))
```

# STEP 2A: Grouping compounds based on RT1, RT2, Ion1, Ion2

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Define tolerances
tolerances <- list(RT1 = 0.1, RT2 = 0.1, Ion1 = 0.5, Ion2 = 0.5)

# Use pF001A as a base
df_all <- df_step2 %>% 
  filter(Sample_name %in% "pF001A") %>% 
  filter(`Signal to Noise` > 10)

df_all$Feature <- 1:nrow(df_all)

# Loop through the samples apart from pF001A
for (sample in setdiff(unique(df_step2$Sample_name), c("pF001A", "bF001A", "bF007B"))) {
  # print(sample)
  df <- df_step2 %>% 
    filter(Sample_name %in% sample) %>% 
    filter(`Signal to Noise` > 10)
  df$Feature <- NA
  
  # Go through each row
  for (i in 1:nrow(df)) {
    row <- df[i, ]
    mask <- (
      abs(df_all$RT1 - row$RT1) <= tolerances$RT1 &
      abs(df_all$RT2 - row$RT2) <= tolerances$RT2 & 
      abs(df_all$Ion1 - row$Ion1) <= tolerances$Ion1 &
      abs(df_all$Ion2 - row$Ion2) <= tolerances$Ion2
    )
    
    idx <- which(mask)
    # If there is a match between a peak and the existing peak list, then assign the same Feature number to that peak 
    if (any(mask)) {
      row$Feature <- unique(df_all[idx, ]$Feature)[1]
    } else { # If not a match, then create new identity for the new Feature 
      row$Feature <- max(df_all$Feature) + 1
    }
    
    # adding the peak that have matchs in df_all
    df_all <- bind_rows(df_all, row)
  }
}

df_all <- df_all %>%
  # remove all peak with RT1 > 36 (which does not belong Gasoline)
  filter(RT1 < 30)

# Create metadata for next data analysis
metadata <- df_all %>%
  dplyr::select(
    Feature,
    Sample_name,
    Area) %>%
  group_by(Feature,
           Sample_name) %>%
  dplyr::summarise(across(Area, base::mean)) %>% # If feature appear in 
  tidyr::pivot_wider(names_from = Sample_name,
                     values_from = Area) 

#### Adding avg RT and Ions to the master df for Gwen
mean_rt1 <- c()
mean_rt2 <- c()
mean_ion1 <- c()
mean_ion2 <- c()

for (i in 1:nrow(metadata)) {
  mean_rt1 <- c(mean_rt1, mean(df_all[which(df_all$Feature %in% metadata[i, ]$Feature),]$RT1))
  mean_rt2 <- c(mean_rt2, mean(df_all[which(df_all$Feature  %in% metadata[i, ]$Feature),]$RT2))
  mean_ion1 <- c(mean_ion1, max(df_all[which(df_all$Feature  %in% metadata[i, ]$Feature),]$Ion1)) 
  mean_ion2 <- c(mean_ion2, max(df_all[which(df_all$Feature  %in% metadata[i, ]$Feature),]$Ion2))
}

metadata$RT1 <- mean_rt1
metadata$RT2 <- mean_rt2
metadata$Ion1 <- mean_ion1
metadata$Ion2 <- mean_ion2

metadata <- metadata %>% relocate(RT1, RT2, Ion1, Ion2, .after = 1) # Ion2

#### Adding Chemical groups of  to the master df for Gwen
# chemical_group <- c()
# 
# for (i in 1:nrow(metadata)) {
#   idx <- which(abs(metadata[i,]$Ion1 -target_comp$Ion1) <=  0.5 & 
#                  # abs(metadata[i,]$Ion2 -target_comp$Ion2) <=  0.5 &
#                  abs(metadata[i,]$RT1 -target_comp$RT1) <=  0.1   &
#                  abs(metadata[i,]$RT2 -target_comp$RT2) <=  0.1
#   )
#   if (identical(idx, integer(0))) {
#     chemical_group <- c(chemical_group, "unknown")
#     next
#   } else {
#     chemical_group <- c(chemical_group, unique(target_comp[idx,]$Group))
#   }
# }
# 
# metadata$Chemical_group <- chemical_group

core_metadata <- metadata %>% 
  # relocate(Chemical_group, .after = 1) %>% 
  arrange(RT1, RT2)

# Export masterlist and metadata for Gwen
# writexl::write_xlsx(df_all, path = paste0("MasterList_", format(Sys.Date(), format = "%d-%b-%Y"), ".xlsx"))
# writexl::write_xlsx(metadata_export, path = paste0("PeakTable_", format(Sys.Date(), format = "%d-%b-%Y"), ".xlsx"))
```

# STEP 2B: Data compression of compounds that share Ion1 (m/z ranges) and RT1/RT2
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Ion signature of groups
mz_alkane <- c(43,57,71,85,99)
mz_cycloalkane <- c(41, 55, 69, 83, 97)
mz_aromatics <- c(91, 105, 106, 119, 120, 134)
mz_indane <- c(117,118, 131, 132, 145, 146)
mz_threemusketeer <- c(91,106)
mz_castlegroup <- c(91, 105, 117, 120)
mz_gangoffour <- c(91, 105, 115, 119, 134)
mz_tetris <- c(119,134)
mz_2_1 <- c(119,134)
mz_alkylnaphthalene <- c(128, 142, 156, 170)

new_meta_data <- copy(metadata)
new_meta_data$Ion1 <- round(new_meta_data$Ion1)
new_meta_data$Ion2 <- round(new_meta_data$Ion2)
new_meta_data$new_feature_name_rt <- NA
new_meta_data$new_feature_name_mz_layer1 <- NA
new_meta_data$new_feature_name_mz_layer2 <- NA
new_meta_data$Chemical_group <-  NA
new_meta_data <- new_meta_data %>% relocate(new_feature_name_rt, new_feature_name_mz_layer1,
                                            new_feature_name_mz_layer2, Chemical_group, .before = 1)
```

### Layer 1: Check if the next row has is within +- 0.05 of RT1 and RT2 and has m/z fall in either one of the Ion signature of groups => name them as same compound 
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
i <- 1
for (row in 1:nrow(new_meta_data)) {
  current_row_rt1 <- new_meta_data[row,]$RT1
  current_row_rt2 <- new_meta_data[row,]$RT2
  # Layer 1: Find all RT1, RT2 that is in the same range +-0.05
  idx1 <- which(abs(current_row_rt1 - new_meta_data$RT1) < 0.05 & 
                  abs(current_row_rt2 - new_meta_data$RT2) < 0.05 & 
                  is.na(new_meta_data$new_feature_name_rt))
  if (length(idx1) > 1) {
    new_meta_data[idx1, ]$new_feature_name_rt <- paste0("Feature_rt_", i)
    i <- i + 1
  } else {
    next
  }
}
```

### Layer 2: Check for similar ion1 and ion2, and switching of Ion1 and Ion2
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
i <- 1
for (feature in unique(new_meta_data[which(!is.na(new_meta_data$new_feature_name_rt)),]$new_feature_name_rt)) {
  mz_major <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion1
  mz_minor <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion2
  idx2 <- which(new_meta_data$new_feature_name_rt == feature)
  temp <- new_meta_data[idx2,]
  # Initialize a logical vector to store selection conditions
  selected <- rep(FALSE, nrow(temp))
  # Iterate through the rows and check for switching or perfect matching
  for (row in 1:(nrow(temp) - 1)) {
    if ((temp$Ion1[row] == temp$Ion2[row + 1] && temp$Ion2[row] == temp$Ion1[row + 1]) || 
        (temp$Ion1[row] == temp$Ion1[row + 1] && temp$Ion2[row] == temp$Ion2[row + 1])) {
      selected[row] <- TRUE
      selected[row + 1] <- TRUE
    }
  }
  new_idx2 <- idx2[selected]
  new_meta_data[new_idx2, ]$new_feature_name_mz_layer1 <- paste0("Feature_mz_layer1_", i)
  i <- i + 1
}
```

### Layer 3: Check if the ions fall into ions of signature groups
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
i <- 1
for (feature in unique(new_meta_data[which(!is.na(new_meta_data$new_feature_name_rt)),]$new_feature_name_rt)) {
  mz_major <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion1
  mz_minor <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion2
  idx2 <- which(new_meta_data$new_feature_name_rt == feature)
  temp <- new_meta_data[idx2,]
  if (any(temp$Ion1 %in% mz_alkane & temp$Ion2 %in% mz_alkane)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_alkane & temp$Ion2 %in% mz_alkane]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Alkane"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_cycloalkane & temp$Ion2 %in% mz_cycloalkane)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_cycloalkane & temp$Ion2 %in% mz_cycloalkane]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Cycloalkane"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_aromatics & temp$Ion2 %in% mz_aromatics)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_aromatics & temp$Ion2 %in% mz_aromatics]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Aromatics"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_indane & temp$Ion2 %in% mz_indane)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_indane & temp$Ion2 %in% mz_indane]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Indane"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_threemusketeer & temp$Ion2 %in% mz_threemusketeer)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_threemusketeer & temp$Ion2 %in% mz_threemusketeer]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Three Musketeers"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_castlegroup & temp$Ion2 %in% mz_castlegroup)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_castlegroup & temp$Ion2 %in% mz_castlegroup]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Castle Group"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_tetris & temp$Ion2 %in% mz_tetris)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_tetris & temp$Ion2 %in% mz_tetris]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Tetris"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_2_1 & temp$Ion2 %in% mz_2_1)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_2_1 & temp$Ion2 %in% mz_2_1]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "2-1"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_alkylnaphthalene & temp$Ion2 %in% mz_alkylnaphthalene)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_alkylnaphthalene & temp$Ion2 %in% mz_alkylnaphthalene]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Alkylnaphthalene"
    i <- i + 1
  } else {
    next
  }
}
```

### Data compression: Sum up values for each coeluting compounds --------------------------------------------------
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Case 1: have NA of both layer1 and layer2
case1 <- copy(new_meta_data %>% 
                dplyr::filter(!is.na(new_feature_name_rt) &
                                is.na(new_feature_name_mz_layer1) & 
                                is.na(new_feature_name_mz_layer2)))

# Case 2: Does Not have NA of layer1 but has NA of layer2
case2 <- copy(new_meta_data %>% 
                filter(!is.na(new_feature_name_mz_layer1)) %>% 
                filter(is.na(new_feature_name_mz_layer2))) 
case2_list <- list()
i <- 1
for (feature in unique(case2$new_feature_name_mz_layer1)) {
  idx2 <- which(case2$new_feature_name_mz_layer1 == feature) # Get idx coeluting features
  tempdf <- case2[idx2, ] # extract temp df of coeluting features
  # Take the sum for each column of samples
  case2_list[[i]] <- data.frame(
    new_feature_name_rt = paste0(unique(tempdf$new_feature_name_rt), collapse = "_"),
    new_feature_name_mz_layer1 = paste0(unique(tempdf$new_feature_name_mz_layer1), collapse = "_"),
    new_feature_name_mz_layer2 = paste0(unique(tempdf$new_feature_name_mz_layer2), collapse = "_"),
    Feature = t(max(tempdf$Feature[sapply(tempdf$Feature, is.numeric)])),
    Chemical_group = paste0(unique(tempdf$Chemical_group), collapse = "_"),
    t(colMeans(tempdf[, c(6,7)][sapply(tempdf[, c(6,7)], is.numeric)])), # Average of RT1, RT2
    Ion1 = paste0(unique(tempdf$Ion1), collapse = "_"), # create new Ion1 values that combine all unique Ion1 values of the coeluting compounds
    Ion2 = paste0(unique(tempdf$Ion2), collapse = "_"), # create new Ion2 values that combine all unique Ion2 values of the coeluting compounds
    t(sapply(tempdf[, 10:ncol(tempdf)], function(column) { 
      if (all(is.na(column))) {
        return(NA)
      } else {
        return(sum(column, na.rm = TRUE))
      }
    }))
  )
  i <- i + 1
}

# Case 3: Have NA of layer1 but does not have NA of layers
case3 <- copy(new_meta_data %>% 
                filter(is.na(new_feature_name_mz_layer1)) %>% 
                filter(!is.na(new_feature_name_mz_layer2))) 

case3_list <- list()
i <- 1
for (feature in unique(case3$new_feature_name_mz_layer2)) {
  idx2 <- which(case3$new_feature_name_mz_layer2 == feature) # Get idx coeluting features
  tempdf <- case3[idx2,] # extract temp df of coeluting features
  # Take the sum for each column of samples
  case3_list[[i]] <- data.frame(
    new_feature_name_rt = paste0(unique(tempdf$new_feature_name_rt), collapse = "_"),
    new_feature_name_mz_layer1 = paste0(unique(tempdf$new_feature_name_mz_layer1), collapse = "_"),
    new_feature_name_mz_layer2 = paste0(unique(tempdf$new_feature_name_mz_layer2), collapse = "_"),
    Feature = t(max(tempdf$Feature[sapply(tempdf$Feature, is.numeric)])),
    Chemical_group = paste0(unique(tempdf$Chemical_group), collapse = "_"),
    t(colMeans(tempdf[, c(6,7)][sapply(tempdf[, c(6,7)], is.numeric)])), # Average of RT1, RT2
    Ion1 = paste0(unique(tempdf$Ion1), collapse = "_"), # create new Ion1 values that combine all unique Ion1 values of the coeluting compounds
    Ion2 = paste0(unique(tempdf$Ion2), collapse = "_"), # create new Ion2 values that combine all unique Ion2 values of the coeluting compounds
    t(sapply(tempdf[, 10:ncol(tempdf)], function(column) { 
      if (all(is.na(column))) {
        return(NA)
      } else {
        return(sum(column, na.rm = TRUE))
      }
    }))
  )
  i <- i + 1
}

# Case 4:  Appear in both layer1 and layer2
not_case4_features <- c(unique(case1$Feature), 
                    unique(case2$Feature),
                    unique(case3$Feature))
case4 <- new_meta_data %>% dplyr::filter(Feature %notin% c(not_case4_features) & 
                                           !is.na(new_feature_name_mz_layer2))
case4_list <- list()
i <- 1
for (feature in unique(case4$new_feature_name_mz_layer2)) { # Can either iterate through layer 1 or 2, doesn't matter which
  idx2 <- which(case4$new_feature_name_mz_layer2 == feature) # Get idx coeluting features
  tempdf <- case4[idx2,] # extract temp df of coeluting features
  # Take the sum for each column of samples
  case4_list[[i]] <- data.frame(
    new_feature_name_rt = paste0(unique(tempdf$new_feature_name_rt), collapse = "_"),
    new_feature_name_mz_layer1 = paste0(unique(tempdf$new_feature_name_mz_layer1), collapse = "_"),
    new_feature_name_mz_layer2 = paste0(unique(tempdf$new_feature_name_mz_layer2), collapse = "_"),
    Feature = t(max(tempdf$Feature[sapply(tempdf$Feature, is.numeric)])),
    Chemical_group = paste0(unique(tempdf$Chemical_group), collapse = "_"),
    t(colMeans(tempdf[, c(6,7)][sapply(tempdf[, c(6,7)], is.numeric)])), # Average of RT1, RT2
    Ion1 = paste0(unique(tempdf$Ion1), collapse = "_"), # create new Ion1 values that combine all unique Ion1 values of the coeluting compounds
    Ion2 = paste0(unique(tempdf$Ion2), collapse = "_"), # create new Ion2 values that combine all unique Ion2 values of the coeluting compounds
    t(sapply(tempdf[, 10:ncol(tempdf)], function(column) { 
      if (all(is.na(column))) {
        return(NA)
      } else {
        return(sum(column, na.rm = TRUE))
      }
    }))
  )
  i <- i + 1
}


compressed_data <- rbind(new_meta_data %>% 
                         dplyr::filter(is.na(new_feature_name_rt) &
                                         is.na(new_feature_name_mz_layer1) & 
                                         is.na(new_feature_name_mz_layer2)) %>%
                         mutate(Ion1 = as.character(Ion1)) %>%
                         mutate(Ion2 = as.character(Ion2)), 
                       case1 %>%
                         mutate(Ion1 = as.character(Ion1)) %>%
                         mutate(Ion2 = as.character(Ion2)),
                       dplyr::bind_rows(case2_list), 
                       dplyr::bind_rows(case3_list), 
                       dplyr::bind_rows(case4_list)
                       ) # %>%
  # dplyr::mutate(Feature = paste0("RT1_", RT1, "_",
  #                                "RT2_", RT2, "_",
  #                                "Ion1_", Ion1, "_",
  #                                "Ion2_", Ion2))
# View(final_cleandf)

coredf <- as.data.frame(t(compressed_data[, -c(1,2,3,4,6,7,8,9)] %>% # 1,2,3,5,6,7,8,9
                            column_to_rownames(., var = "Feature"))) %>% 
    rownames_to_column(., var="Sample_name") %>% 
    mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas 87", 
                                  ifelse(str_detect(Sample_name, "B"), "Gas 89",
                                         ifelse(str_detect(Sample_name, "C"), "Gas 91", "Gas 94")))) %>%
    mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                    ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
    mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
                                ifelse(str_detect(Sample_name, "F002"), "Station 2",
                                       ifelse(str_detect(Sample_name, "F003"), "Station 3",
                                              ifelse(str_detect(Sample_name, "F004"), "Station 4",
                                                     ifelse(str_detect(Sample_name, "F005"), "Station 5",
                                                            ifelse(str_detect(Sample_name, "F006"), "Station 6",
                                                                   ifelse(str_detect(Sample_name, "F007"), "Station 7",
                                                                          ifelse(str_detect(Sample_name, "F008"), "Station 8",
                                                                                 ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>% 
    # Add info on Suppliers
    mutate(supplier = ifelse(str_detect(Sample_name, "F001"), "Miscellanous",
                             ifelse(str_detect(Sample_name, "F002"), "Miscellanous",
                                    ifelse(str_detect(Sample_name, "F003"), "Shell",
                                           ifelse(str_detect(Sample_name, "F004"), "Cenovus",
                                                  ifelse(str_detect(Sample_name, "F005"), "Miscellanous",
                                                         ifelse(str_detect(Sample_name, "F006"), "Suncor",
                                                                ifelse(str_detect(Sample_name, "F007"), "Imperial",
                                                                       ifelse(str_detect(Sample_name, "F008"), "Burnaby",
                                                                              ifelse(str_detect(Sample_name, "F009"), "Imperial", "unknown")))))))))) %>% 
    relocate(Octane_rating, sampling_season, gas_station, supplier, .after = 1) %>% 
  mutate(supplier = ifelse(supplier %in% c("Miscellanous", "unknown"), "Unbranded", supplier))
```

# Violin plot before and after data compression
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Function to overlay violin plots
overlay_violin_plots <- function(df1, df2) {
  # Check if the dataframes have the same columns
  if (!identical(names(df1), names(df2))) {
    stop("The two dataframes do not have the same columns.")
  }
  
  # Loop through each column
  for (col_name in names(df1)) {
    # Ensure the column is numeric
    if (!is.numeric(df1[[col_name]]) || !is.numeric(df2[[col_name]])) next
    
    # Combine the two columns into a single dataframe for plotting
    plot_data <- data.frame(
      value = c(df1[[col_name]], df2[[col_name]]),
      group = rep(c("df1", "df2"), each = nrow(df1))
    )
    
    # Create the violin plot
    p <- ggplot(plot_data, aes(x = group, y = value, fill = group)) +
      geom_violin(alpha = 0.4, na.rm = TRUE) + # Use alpha for light hue
      scale_fill_manual(values = c("skyblue", "red")) +
      labs(title = col_name, x = "Group", y = "Value") +
      theme_minimal() +
      theme(plot.title = element_text(hjust = 0.5))
    
    # Print the plot
    print(p)
  }
}

# Before data compression
grid.arrange(grobs = create_violin_plots(metadata[, -c(1,2,3,4,5)]), ncol = 10, nrow = 7)
grid.arrange(grobs = create_violin_plots(final_cleandf[, -c(1,2,3,4,5)]), ncol = 10, nrow = 7)
```


# STEP 3: Feature elimination/ Feature prioritization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- coredf

# Step 1: Remove features with values in only one sample (row) ---------
df_filtered_step1 <- df[, colSums(!is.na(df) & df != 0) > 1]

# Step 2: Remove features with values in only one gas_station label --------

# Identify feature columns (excluding the "gas_station" label)
feature_columns <- setdiff(names(df_filtered_step1), "gas_station")

# Logical vector indicating which features to keep
keep_features <- sapply(df_filtered_step1[, feature_columns], function(feature) {
  unique_labels_with_values <- unique(df_filtered_step1$gas_station[!is.na(feature) & feature != 0])
  length(unique_labels_with_values) > 1
})

# Subset the dataframe to retain the desired features and "gas_station"
df_filtered_step2 <- df_filtered_step1[, c(names(df_filtered_step1)[keep_features])]

# Step 3: Remove features with less than 3 data points per gas_station label  ---------
selected_col <- integer()
gas_station_combinations <- t(utils::combn(unique(df_filtered_step2$gas_station), 2))
# Loop through the feature columns
for (feature_col in 6:ncol(df_filtered_step2)) {      
  feature_data <- df_filtered_step2[[feature_col]]  # Extract column once for efficiency

  # Loop through the precomputed combinations of gas_station
  for (combo in seq_len(nrow(gas_station_combinations))) {
    p_1 <- gas_station_combinations[combo, 1]
    p_2 <- gas_station_combinations[combo, 2]

    vec1 <- as.numeric(feature_data[df_filtered_step2$gas_station == p_1])
    vec2 <- as.numeric(feature_data[df_filtered_step2$gas_station == p_2])
    
    if (sum(!is.na(vec1)) >= 3 & sum(!is.na(vec2)) >= 3) {
      selected_col <- union(selected_col, feature_col)  # Use union for deduplication
      break  # Break inner loop early since column is already selected
    }
  }
}

df_filtered_step3 <- cbind(df_filtered_step2[, c(1:5)], df_filtered_step2[, selected_col])

#-----------------------------#
# STEP 4: Identify & remove features that are >90% missing overall
#-----------------------------#
colid_90_missingvalues <- c()

for (i in 6:ncol(df_filtered_step3)) {
  if (length(df_filtered_step3[which(is.na(df_filtered_step3[, i])), i]) / nrow(df_filtered_step3) > 0.7) {
    colid_90_missingvalues <- c(colid_90_missingvalues, i)
  }
}

if (!is.null(colid_90_missingvalues)) {
  df_filtered_step4 <- df_filtered_step3[, -colid_90_missingvalues]
}
```


# STEP 4: Assessing Imputation and Normalization methods
```{r, echo = FALSE, message = FALSE, warning = FALSE}
##############################
## 1. New Imputation Methods ##
##############################

## Option 1: Replace missing values with 0.001 
impute_0.001 <- function(df) {
  # Work on a copy of the data so that metadata (columns 1:5) remain intact.
  df_0.001 <- copy(df)
  for (r in 1:nrow(df_0.001)) {
  df_0.001[r, which(base::is.na(df_0.001[r,]))] <- 0.001
  }
  return(df_0.001)
}

## 1.3 Mean Imputation
mean_imputation <- function(df){
  df_numeric <- df
  for(i in seq_along(df_numeric)){
    col_data <- df_numeric[[i]]
    # Replace NA with mean
    col_data[is.na(col_data)] <- mean(col_data, na.rm = TRUE)
    df_numeric[[i]] <- col_data
  }
  return(df_numeric)
}

## 1.4 kNN Imputation
knn_imputation <- function(df, k = 5){
  # VIM::kNN modifies the data in place; to avoid that, use a copy
  df_copy <- df
  # kNN returns a data.frame with the imputed columns appended with ".imp"
  # The original columns remain but with no missing values replaced in place
  # So we can do something like:
  imputed_data <- kNN(df_copy, k = k, imp_var = FALSE)  # no new columns
  return(imputed_data)
}

## 1.7 Fill with half-min of non-missing values
half_min_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    non_zero <- col_data[col_data > 0]
    if(length(non_zero) > 0){
      half_min_val <- min(non_zero, na.rm = TRUE) / 2
      col_data[is.na(col_data)] <- half_min_val
    } else {
      # if entire column is zero or NA, just set to 0 or small value
      col_data[is.na(col_data)] <- 0
    }
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.8 Fill with median
median_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    col_data[is.na(col_data)] <- median(col_data, na.rm = TRUE)
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## Option 3: Replace missing values with randomized values drawn from a uniform distribution 
## The random numbers are drawn with min = 0 and max = the global minimum among all numeric values.
impute_random_uniform <- function(df) {
  df_out <- df
  num_cols <- ncol(df_out)
  # Compute the global minimum across all numeric features (ignoring NAs)
  global_min <- min(as.matrix(df_out[, 1:num_cols]), na.rm = TRUE)
  for (r in 1:nrow(df_out)) {
    row_vals <- as.numeric(df_out[r, 1:num_cols])
    missing_idx <- which(is.na(row_vals))
    if (length(missing_idx) > 0) {
      # Draw a random number for each missing value in this row
      random_vals <- runif(length(missing_idx), min = 0, max = global_min)
      row_vals[missing_idx] <- random_vals
      df_out[r, 1:num_cols] <- row_vals
    }
  }
  return(df_out)
}

## Option 4: Imputation with missForest 
impute_missforest <- function(df){
  # missForest returns a list: $ximp is the completed data
  imputed <- missForest::missForest(as.data.frame(df), verbose = FALSE)$ximp
  return(imputed)
}

##############################
## 2. New Normalization Methods ##
##############################

## Normalization Method 1: Percentage-based normalization 
## For each row, the numeric features (columns 6 onward) are divided by the row sum.
normalize_percentage <- function(df) {
  percentage_normalized <- as.data.frame(t(apply(df[, 1:ncol(df)],
                                               MARGIN = 1, 
                                               function(row) {row/sum(row, na.rm = TRUE)}))) 
  return(percentage_normalized)
}

## Normalization Method 2: Log normalization 
## For each row, take the logarithm of each numeric feature.
normalize_log <- function(df) {
  df_out <- df
  log_normalized <- as.data.frame(t(apply(df_out[, 1:ncol(df_out)], 
                                        MARGIN = 1, function(x) log(x))))
  return(log_normalized)
}

no_normalization <- function(x){
  return(x)
}

## 2.2 Min-Max scaler 
scale_minmax <- function(df){
  # We can do it manually or use caret::preProcess
  # Here is a manual approach for numeric columns
  df_mat <- as.matrix(df)
  mins <- apply(df_mat, 2, min, na.rm = TRUE)
  maxs <- apply(df_mat, 2, max, na.rm = TRUE)
  
  scaled_mat <- sweep(df_mat, 2, mins, FUN = "-")
  ranges <- maxs - mins
  scaled_mat <- sweep(scaled_mat, 2, ranges, FUN = "/")
  
  # convert back to data frame
  df_scaled <- as.data.frame(scaled_mat)
  colnames(df_scaled) <- colnames(df)
  rownames(df_scaled) <- rownames(df)
  
  return(df_scaled)
}

## 2.3 Z-score normalization
z_score_normalization <- function(df){
  # scale() in R does mean-center and unit variance by column
  df_scaled <- scale(df, center = TRUE, scale = TRUE)
  return(as.data.frame(df_scaled))
}


###########################################
## 3. Downstream Evaluation Functions  ##
###########################################

## 3.4 Pairwise Significance Testing
# Modified pairwise significance testing function
pairwise_significance_tests_modified <- function(input_df,
                                                 group_col,
                                                 start_col_index,
                                                 metadata,
                                                 target_comp) {
  # Initialize the uncorrected results data frame
  df_results <- data.frame(Feature = character(),
                           comparison_pair = character(),
                           pval = numeric(),
                           stringsAsFactors = FALSE)
  
  # Get the unique groups and compute all pairwise combinations
  groups <- unique(input_df[[group_col]])
  group_pairs <- utils::combn(groups, 2)
  
  # Loop over each feature (assumed to be in columns start_col_index to ncol(input_df))
  for(feature_col in start_col_index:ncol(input_df)) {
    # Loop over each pair of groups
    for(col_idx in 1:ncol(group_pairs)) {
      p_1 <- group_pairs[1, col_idx]
      p_2 <- group_pairs[2, col_idx]
      
      # Extract the vectors corresponding to the two groups
      vec1 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_1), feature_col]))
      vec2 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_2), feature_col]))
      
      # Require at least 3 non-missing observations in each vector
      if(sum(!is.na(vec1)) >= 3 && sum(!is.na(vec2)) >= 3) {
        # Skip if all values in one vector are identical
        if(all(vec1 == vec1[1]) || all(vec2 == vec2[1])) {
          next
        } else {
          # Test for normality using the Shapiro–Wilk test
          shapiro1 <- tryCatch(shapiro.test(vec1)$p.value, error = function(e) NA)
          shapiro2 <- tryCatch(shapiro.test(vec2)$p.value, error = function(e) NA)
          
          if(is.na(shapiro1) || is.na(shapiro2)) next
          
          # Use wilcox.test if one or both groups are not normally distributed;
          # otherwise, use t.test.
          # here we used paired=TRUE, because each feature in one category also appears in the other category (we compare one feature at a time(.
          if(shapiro1 < 0.05 || shapiro2 < 0.05) {
            pval <- tryCatch(stats::wilcox.test(vec1, vec2, paired = TRUE)$p.value, error = function(e) NA)
          } else {
            pval <- tryCatch(stats::t.test(vec1, vec2, paired=TRUE)$p.value, error = function(e) NA)
          }
        }
      } else {
        next
      }
      
      # If an error occurred (or pval is NA), skip this pair
      if (is.na(pval)) next
      
      # Append the results: record the feature name and the comparison pair
      df_results[nrow(df_results) + 1, ] <- list(Feature = colnames(input_df)[feature_col],
                                                 comparison_pair = paste0(p_1, " & ", p_2),
                                                 pval = pval)
    }
  }
  
  # Now perform multiple testing corrections using several methods.
  pvalue_methods <- c("bonferroni", "holm", "hochberg", "hommel", "BH", "BY")
  corrected_list <- list()
  uncorrected_list <- list()
  j <- 1
  
  for(method in pvalue_methods) {
    temp <- df_results
    temp$adjusted_pvalue <- stats::p.adjust(as.numeric(temp$pval), method = method)
    
    # Filter for significant features (adjusted p-value < 0.05)
    significant <- temp %>% dplyr::filter(adjusted_pvalue < 0.05) %>% dplyr::arrange(adjusted_pvalue)
    corrected_list[[j]] <- significant
    uncorrected_list[[j]] <- temp
    
    if(nrow(significant) == 0) {
      print(paste0(method, " does not result in any significant compound"))
      j <- j + 1
      next
    } else {
      # print(method)
      
      # Retrieve additional metadata (RT1, RT2, Ion1, Ion2) for each significant feature.
      rt1 <- c()
      rt2 <- c()
      Ion1 <- c()
      Ion2 <- c()
      
      for(r in 1:nrow(significant)) {
        feat <- as.numeric(significant[r, "Feature"])
        meta_row <- metadata[which(metadata$Feature == feat), ]
        if(nrow(meta_row) == 0) {
          rt1 <- c(rt1, NA)
          rt2 <- c(rt2, NA)
          Ion1 <- c(Ion1, NA)
          Ion2 <- c(Ion2, NA)
        } else {
          rt1 <- c(rt1, meta_row$RT1[1])
          rt2 <- c(rt2, meta_row$RT2[1])
          Ion1 <- c(Ion1, meta_row$Ion1[1])
          Ion2 <- c(Ion2, meta_row$Ion2[1])
        }
      }
      
      significant$RT1  <- rt1
      significant$RT2  <- rt2
      significant$Ion1 <- Ion1
      significant$Ion2 <- Ion2
      
      # Next, use the target_comp data frame to match compounds based on RT and ion values.
      compound_name <- c()
      chem_grp <- c()
      
      for(r in 1:nrow(significant)) {
        idx <- which(abs(target_comp$RT1 - significant[r, "RT1"]) <= 0.1 &
                     abs(target_comp$RT2 - significant[r, "RT2"]) <= 0.1 &
                     abs(target_comp$Ion1 - significant[r, "Ion1"]) <= 0.5 &
                     abs(target_comp$Ion2 - significant[r, "Ion2"]) <= 0.5)
        if(length(idx) == 0) {
          chem_grp <- c(chem_grp, "unknown")
          compound_name <- c(compound_name, "unknown")
        } else {
          chem_grp <- c(chem_grp, target_comp$Group[idx[1]])
          compound_name <- c(compound_name, target_comp$Compound[idx[1]])
        }
      }
      
      significant$compound_name  <- compound_name
      significant$Chemical_group <- chem_grp
      
      # print(significant %>% dplyr::arrange(adjusted_pvalue))
      # print(paste0(method, " has ", length(unique(significant$Feature)), " significant features"))
      corrected_list[[j]] <- significant
      
      j <- j + 1
    }
  }
  
  return(list(
    uncorrected = df_results,
    corrected   = corrected_list,
    uncorrected_all = uncorrected_list
  ))
}

###############################
##  Cluster resolution  ##
###############################

## Calculate cluster resolution using the first two PCA components
calculate_cluster_resolution <- function(data1, data2) {
  centroid1 <- colMeans(data1)
  centroid2 <- colMeans(data2)
  
  # Computes the Euclidean distance With Dim.1 as x-axis and Dim.2 as y-axis
  centroid_distance <-  sqrt((centroid1[1][[1]] - centroid2[1][[1]])^2 + (centroid1[2][[1]] - centroid2[2][[1]])^2)
  
  avg_distance1 <- mean(sqrt(rowSums((as.matrix(data1) - 
                       matrix(centroid1, nrow = nrow(data1), ncol = ncol(data1), byrow = TRUE))^2)))
  avg_distance2 <- mean(sqrt(rowSums((as.matrix(data2) - 
                       matrix(centroid2, nrow = nrow(data2), ncol = ncol(data2), byrow = TRUE))^2)))
  
  resolution <- centroid_distance / (avg_distance1 + avg_distance2)
  return(resolution)
}

## Compute average cluster resolution over all pairs of groups
calculate_average_cluster_resolution <- function(X, group_vector) {
  pca_model <- res.pca <- FactoMineR::PCA(
    X,
    scale.unit = FALSE,
    graph = FALSE)
  scores <- as.data.frame(pca_model$ind$coord[, 1:2])
  
  group <- as.factor(group_vector)
  group_levels <- levels(group)
  
  pair_resolutions <- c()
  
  for (i in 1:(length(group_levels) - 1)) {
    for (j in (i + 1):length(group_levels)) {
      data1 <- scores[group == group_levels[i], , drop = FALSE]
      data2 <- scores[group == group_levels[j], , drop = FALSE]
      
      if (nrow(data1) > 1 && nrow(data2) > 1) {
        res <- calculate_cluster_resolution(data1, data2)
        pair_resolutions <- c(pair_resolutions, res)
      }
    }
  }
  
  avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
  return(avg_resolution)
}

evaluate_rf_performance <- function(data, 
                                    type_col, 
                                    remove_cols, 
                                    train_proportion,
                                    ntree_candidates = c(100, 500, 1000, 2500), 
                                    seed  = NULL, 
                                    metric = "Accuracy") {
  
  set.seed(123)
  train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
  train_data <- data[train_index, ]
  test_data  <- data[-train_index, ]
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test, ntree_candidates, metric
  )
  
  eval_metrics <- all_feats$eval_metrics
  
  return(eval_metrics)
}

evaluate_model_stability <- function(
  X, y,
  ntree_candidates = c(100, 500, 1000, 2500),
  mtry_candidates  = c(floor(sqrt(ncol(X))), floor(log2(ncol(X))))
) {
  # how many folds?
  folds <- min(table(y))
  if (folds < 2) {
    stop("Not enough samples per class to do CV.")
  }

  # combine predictors + response
  data_in <- data.frame(X, Class = factor(y))

  # stratified CV control
  ctrl <- trainControl(
    method = "cv",
    number = folds,
    classProbs = TRUE
  )

  # collect *all* per‑fold accuracies here
  all_accs <- c()

  # loop over each hyperparameter combo
  for (nt in ntree_candidates) {
    for (m in mtry_candidates) {
      set.seed(sample.int(1e6, 1))  # optional: vary seed per fit
      rf_fit <- train(
        Class ~ .,
        data      = data_in,
        method    = "rf",
        metric    = "Accuracy",
        trControl = ctrl,
        tuneGrid  = data.frame(mtry = m),
        ntree     = nt
      )

      # append this combo's fold‑wise accuracies
      all_accs <- c(all_accs, rf_fit$resample$Accuracy)
    }
  }

  # compute overall mean and sd across *every* individual accuracy
  return(list(
    cv_mean = mean(all_accs, na.rm = TRUE),
    cv_std  = sd(all_accs,   na.rm = TRUE)
  ))
}

##############################################
## 4. Function to Find the “Best” Combo    ##
##############################################
# Define lists of new imputation and normalization functions
imputation_methods <- list(
  impute_missforest    = impute_missforest,
  impute_random_uniform = impute_random_uniform,
  mean_imputation            = mean_imputation,
  knn_imputation             = knn_imputation,
  impute_0.001         = impute_0.001,
  half_min_imputation        = half_min_imputation,
  median_imputation          = median_imputation
)

normalization_methods <- list(
  Percentage_Normalization = normalize_percentage,
  Min_Max_Scaler       = scale_minmax,
  Z_Score_Normalization= z_score_normalization,
  Log_Normalization        = normalize_log,
  No_normalization     = no_normalization
)

find_best_impute_normalize_new <- function(df,
                                           type_col,
                                           group_for_significance,
                                           remove_cols,
                                           train_proportion,
                                           split_with_miscunknown_supplier,
                                           start_col_index,
                                           metadata,
                                           target_comp) {
  # 1) Extract numeric features X and target y
  X       <- df %>% dplyr::select(-all_of(c(remove_cols, type_col))) %>% as.data.frame()
  y       <- df[[type_col]]
  
  # We'll store the results
  results_list <- list()
  
  # 2) Loop over imputation + normalization
  for(imp_name in names(imputation_methods)){
    impute_fun <- imputation_methods[[imp_name]]
    
    for(norm_name in names(normalization_methods)){
      norm_fun <- normalization_methods[[norm_name]]
      
      # Impute
      if(imp_name == "No_imputation"){
        X_imputed <- X
      } else {
        X_imputed <- tryCatch(
          impute_fun(X),
          error = function(e) {
            message("Error in ", imp_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_imputed)) next
      }
      
      # Normalize
      if(norm_name == "No_normalization"){
        X_norm <- X_imputed
      } else {
        X_norm <- tryCatch(
          norm_fun(X_imputed),
          error = function(e){
            message("Error in ", norm_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_norm)) next
      }
      
      if(any(is.na(X_norm))){
        stop("Combo", " Imputation method ", imp_name, " and Normalization method ", norm_name, " produced NAs!")
      }
      if(any(!is.finite(as.matrix(X_norm)))){
        stop("Combo", " Imputation method", imp_name, " and Normalization method ", norm_name, " produced Inf/NaN!")
      }
      
      # Evaluate cluster resolution (using PCA on numeric features)
      cluster_res <- calculate_average_cluster_resolution(X_norm, y)
      
       ###############################
      ##  New: Correlation Score:Computes the Pearson correlation between the upper‐triangle elements of the correlation matrix of your original data (computed with pairwise complete observations) and that of your imputed/normalized data. A higher score means that the imputation has preserved the pairwise relationships better.      The correlation score is computed as a Pearson correlation coefficient, which can range from -1 to 1. In this context, a score close to 1 indicates that the imputed/normalized data have preserved the original pairwise relationships very well. A score of 0 would indicate no linear relationship between the two sets of correlation values, and a negative value would suggest that the dependency structure is inversely related.##
      ###############################
      # Compute correlation matrix on original data using pairwise complete observations
      orig_corr <- cor(X, use = "pairwise.complete.obs")
      # Compute correlation matrix on imputed/normalized data
      imp_corr <- cor(X_norm, use = "pairwise.complete.obs")
      # Extract upper-triangle values (excluding diagonal) for both matrices
      orig_upper <- orig_corr[upper.tri(orig_corr)]
      imp_upper  <- imp_corr[upper.tri(imp_corr)]
      # Compute correlation between the two sets of values
      corr_score <- cor(orig_upper, imp_upper, use = "complete.obs")
      
      ###############################
      ##  New: KS Test p-value: Compare the distribution of orignal data with missing values against data with imputed values ##
      ###############################
      # Combine observed values and imputed values across all numeric columns
      observed_all <- c()
      imputed_all  <- c()
      
      for(col in names(X)) {
        missing_idx <- which(is.na(X[[col]]))
        # Only consider columns that originally had missing values and at least one observed value
        if(length(missing_idx) >= 1 && length(X[[col]][!is.na(X[[col]])]) >= 1){
          # 1) original non‐missing values in this column
          obs_values <- X[[col]][!is.na(X[[col]])]
          
          # 2) entire column from the imputed dataset (includes both originally observed
          #    and newly imputed values)
          imp_values <- X_norm[[col]]
          
          # Append to the pooled vectors
          observed_all <- c(observed_all, obs_values)
          imputed_all  <- c(imputed_all, imp_values)
        }
      }
      
      if(length(observed_all) < 2 || length(imputed_all) < 2) {
        ks_p <- NA
      } else {
        ks_result <- tryCatch({
          ks.test(observed_all, imputed_all)
        }, error = function(e) NULL)
        if(!is.null(ks_result)){
          ks_p <- ks_result$p.value
        } else {
          ks_p <- NA
        }
      }
      
      ###############################
      ##  New: RF Test Accuracy    ##
      ###############################
      # Use the imputed/normalized data (X_norm) and class vector y to compute test accuracy.
      df_input_test_acc <- cbind(X_norm, df[,c(type_col, remove_cols)])

      performance_output <- evaluate_rf_performance(data = df_input_test_acc,
                                               type_col=type_col,
                                               remove_cols=remove_cols,
                                               train_proportion = train_proportion)
      
      # Evaluate CV accuracy
      rf_stability <- evaluate_model_stability(X_norm, y)
      cv_mean <- rf_stability$cv_mean
      cv_std  <- rf_stability$cv_std
      
      pairwise_results <- pairwise_significance_tests_modified(input_df = df_input_test_acc,
                                                               group_col = group_for_significance,
                                                               metadata = metadata,
                                                               target_comp = target_comp,
                                                               start_col_index = start_col_index)
      sig_num <- max(sapply(pairwise_results$corrected, function(subdf) length(unique(subdf$Feature))))
      
      results_list[[length(results_list)+1]] <- data.frame(
        Imputation   = imp_name,
        Normalization= norm_name,
        ClusterRes    = cluster_res,
        CorrScore     = corr_score,
        KS_p          = ks_p,
        CVmean       = cv_mean,
        CVstd        = cv_std,
        sig_num = sig_num,
        TestAcc       = performance_output$Accuracy,
        Kappa = performance_output$Kappa,
        AUC = performance_output$AUC,
        F1_Weighted = performance_output$F1_Weighted,
        MCC_Multiclass = performance_output$MCC_Multiclass,
        stringsAsFactors=FALSE
      )
    }
  }
  
  df_results <- do.call(rbind, results_list)
  
  # Build an ordering index: primary by TestAcc, secondary by ClusterRes, etc.
  ord <- order(
     -df_results$TestAcc,
     -df_results$AUC,
    # -df_results$F1_Macro,
    -df_results$ClusterRes,
    -df_results$sig_num,
    # -df_results$CVmean,
    # df_results$CVstd,
    -df_results$CorrScore,   # smaller rank_corrScore = better CorrScore
    -df_results$KS_p,
    -df_results$F1_Weighted,
    -df_results$MCC_Multiclass
    # -df_results$Kappa,           # smaller  = better KS_p
  )
  
  # Initialize and fill combined_rank so that best row gets 1
  df_results$combined_rank <- NA_integer_
  df_results$combined_rank[ord] <- seq_along(ord)
  
  # Pick the best
  best_row <- df_results[ which.min(df_results$combined_rank) , ]
  
  # Return the full table + the best combo row
  return(list(
    results_table = df_results,
    best_combo    = best_row
  ))
}
```

# Exploratory Data Analysis

## Import important features from RFA RandomForest
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
rfa_important_features <- jsonlite::fromJSON("RFA_best_feature_withdatacompression_missforest_TSN.json")
rfe_important_features <- jsonlite::fromJSON("RFE_best_feature_withdatacompression_missforest_TSN.json")

pairwise_res <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/Arson project/Rproject/Arson/comparison of feature grouping methods and data compression via pair-wise test/13th Dec/withdatacompression_missforest_TSN.xlsx")

final_cleandf_pairwise <- cbind(df_percentage[, c(1,2,3,4,5)], 
                                df_percentage[, -c(1,2,3,4,5)] %>% 
                                  dplyr::select(unique(pairwise_res$Feature))) 
final_cleandf_RFA <- cbind(df_percentage[, c(1,2,3,4,5)], 
                                df_percentage[, -c(1,2,3,4,5)] %>% 
                                  dplyr::select(rfa_important_features)) 
final_cleandf_RFE <- cbind(df_percentage[, c(1,2,3,4,5)], 
                                df_percentage[, -c(1,2,3,4,5)] %>% 
                                  dplyr::select(rfe_important_features)) 
```

# Figure 2. Two subplots 
** left-subplot: percentage composition of chemical group in metadata; 
** right-subplot: horizontal stacked bar plot of signal intensity across gas stations of each chemical group in metadata
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
metadata_annotatedbyCaleb <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/metadata_withcompression_featureelimination_Caleb.xlsx")

# Convert Chemical_group to a factor
metadata_annotatedbyCaleb$Chemical_group <- as.factor(metadata_annotatedbyCaleb$Chemical_group)

## ** Left subplot: Percentage composition of each chemical group -----------
chemical_group_counts <- metadata_annotatedbyCaleb %>%
  dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE)) %>%
  dplyr::group_by(Chemical_group) %>%
  # count the occurence of compounds of each chemical group in all gasoline samples
  summarise(Count = n()) %>%
  dplyr::mutate(Percentage = (Count / sum(Count)) * 100)

p1 <- ggplot(chemical_group_counts, aes(x = reorder(Chemical_group, -Percentage), 
                                        y = Percentage, fill = reorder(Chemical_group, -Percentage) # Chemical_group
                                        )) +
  geom_bar(stat = "identity", width = 0.7) +
  labs(x = "Chemical Group", y = "Percentage composition of chemical groups (%)") + # , title = "Percentage Composition of Chemical Groups") +
  theme_classic(base_size = 25) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, color = "black"),
        # Axis ticks color
        axis.text.y = element_text(color = "black"),
        legend.position = "none")
p1

## **Right subplot: horizontal stacked bar plot of signal intensity across gas stations of each chemical group in metadata ----------
data_long <- metadata_annotatedbyCaleb %>%
  dplyr::select(Chemical_group, everything()) %>%
  pivot_longer(cols = 9:ncol(metadata_annotatedbyCaleb), names_to = "Sample_name", values_to = "Value") %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
                              ifelse(str_detect(Sample_name, "F002"), "Station 2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station 3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station 4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station 5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station 6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station 7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station 8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>%
  dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE)) %>%
  # dplyr::filter(Chemical_group %notin% "unknown") %>%
  group_by(gas_station, Chemical_group) %>%
  summarise(Value = sum(Value, na.rm=TRUE), .groups = "drop") %>% 
  mutate(
    Chemical_group = factor(Chemical_group, levels = levels(reorder(chemical_group_counts$Chemical_group, -chemical_group_counts$Percentage)))) %>% # c("Alkane", "Cycloalkane", "Branched Alkane",
                                                       # "Aromatics", "BTEX", "Alkene", "Castle Group",
                                                       # "Gang of Four", "Indane", "PAHs-Naphthalene"))
  # ) %>%
  mutate(
    gas_station = factor(gas_station, levels = c("Station 1", "Station 2", "Station 3", "Station 4",
                                                 "Station 5", "Station 6", "Station 7", "Station 8",
                                                 "Station 9", "Station 10"))
  )


p2 <- ggplot(data_long,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = "Proportion of total peak area (%)", y = "Gas Station", fill = "Chemical Group") +
  theme_minimal(base_size = 25) +
  theme(
    # axis.title.y = element_text(
    #   margin = ggplot2::margin(r = 0, unit = "pt")
    # ),
     # Legend text size
    legend.text = element_text(size = 25),
    
    # Axis ticks color
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    # Remove ticks on y-axis
    axis.ticks.length.y = unit(0, "cm"),
    # Remove gridlines
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  )
# ggplot(data_long, aes(x = Chemical_group, y = Mean, fill = Chemical_group)) +
  # geom_bar(stat = "identity", position = position_dodge(), width = 0.7) +
  # geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.7)) +
  # labs(x = "Chemical Group", y = "Mean ± SD") + # , title = "Mean and Standard Deviation of Values") +
  # theme_classic(base_size = 18) +
  # theme(axis.text.x = element_text(angle = 45, hjust = 1),
  #       legend.position = "none")
p2

library(cowplot)
figure2 <- plot_grid(
    p1, p2,
    labels = c("A)", "B)"),
    label_fontface = "bold",
    label_size = 25,
    label_x = c(0.01, 0.01),
    label_y = c(0.99, 0.99),
    ncol = 2,
    rel_widths = c(1, 2)
)

figure2

ggsave(filename = paste0("figure2_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = figure2, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in") 
```

# Stacked barplot of RFA compounds (normalize to the Area of RFA compounds instead of all the compounds)
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# for (r in 1:nrow(coredf_RFA)) {
#   coredf_RFA[r, which(base::is.na(coredf_RFA[r,]))] <- 0.001
# }
# 
# percentage_normalized <- as.data.frame(t(apply(coredf_RFA[, 6:ncol(coredf_RFA)],
#                                                MARGIN = 1, 
#                                                       function(row) {row/sum(row, na.rm = TRUE)}))) 
#   
# df_0.001_percentage_normalized_RFA <- percentage_normalized %>% 
#   mutate(gas_station = coredf_RFA$gas_station) %>%
#   mutate(Octane_rating = coredf_RFA$Octane_rating) %>%
#   mutate(sampling_season = coredf_RFA$sampling_season) %>%
#   mutate(Sample_name = coredf_RFA$Sample_name) %>%
#   mutate(supplier = coredf_RFA$supplier) %>%
#   relocate(Octane_rating, sampling_season, gas_station, Sample_name, supplier, .before = 1)
# 
# plotdf <- df_0.001_percentage_normalized_RFA %>% 
#   tidyr::pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Normalized_area")

plotdf <- cbind(coredf[, c(1,2,3,4,5)], coredf[, -c(1,2,3,4,5)] %>%
  dplyr::select(unique(pairwise_res$Feature))) %>%
  mutate(gas_station = factor(gas_station, levels = c("Station 1",  "Station 2", "Station 3", "Station 4", "Station 5", "Station 6", "Station 7",  "Station 8",  "Station 9", "Station 10" ))) 

plot_list <- list()
i <- 1
for (variable in sort(unique(plotdf$gas))) {
    plot_df <- plotdf %>%
        filter(supplier %in% variable)
    
    plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Normalized_area, fill = Chemical_group)) +
        geom_bar(stat = "identity") +
        labs(title = variable,
             x = "", 
             y = "") +
        theme_minimal(base_size = 18) +
        theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                         # size = 18,
                                         face = "bold"), 
              axis.ticks.length.x = unit(0, "cm"),
              # legend.text = element_text(size = 18),
              # legend.title = element_text(size = 18),
              # axis.title.y = element_text(size = 15),
              legend.position = "hidden",
              plot.title = element_text(hjust = 0.5),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.major.y = element_blank()
        ) + 
        scale_y_continuous(expand = c(0,0), limits = c(0, 1))
    i <- i + 1
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Normalized_area, fill = Chemical_group)) +
                                  geom_bar(stat = "identity") +
                                  facet_wrap(~sampling_season) +
                                  labs(fill = "Chemical Groups") +
                                  theme_minimal(base_size = 20) +
                                  theme(legend.text = element_text(size = 18),
                                        legend.title = element_text(size = 20),
                                        axis.title.y = element_text(size = 20),
                                        legend.position = "right"))

y <- textGrob("Normalized Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

stacked_barplot <- grid.arrange(grobs = plot_list, ncol = 3, nrow = 2, right = legend, left = y, bottom = x)
ggsave(filename = paste0("Stacked_barplot_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = stacked_barplot, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in") 
```

# Figure 3. Horizontal stacked bar plot of chemical groups of from Figure 1 in different gas stations - Before versus After best imp/norm 

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Before normalization ---------------
data_long <- metadata_annotatedbyCaleb %>%
  dplyr::select(Chemical_group, everything()) %>%
  pivot_longer(cols = 9:ncol(metadata_annotatedbyCaleb), names_to = "Sample_name", values_to = "Value") %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
                              ifelse(str_detect(Sample_name, "F002"), "Station 2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station 3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station 4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station 5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station 6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station 7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station 8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>%
  dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE)) %>%
  # dplyr::filter(Chemical_group %notin% "unknown") %>%
  group_by(gas_station, Chemical_group) %>%
  summarise(Value = sum(Value, na.rm=TRUE), .groups = "drop") %>% 
  mutate(
    Chemical_group = factor(Chemical_group, levels = levels(reorder(chemical_group_counts$Chemical_group, -chemical_group_counts$Percentage)))) %>% 
  mutate(
    gas_station = factor(gas_station, levels = c("Station 1", "Station 2", "Station 3", "Station 4",
                                                 "Station 5", "Station 6", "Station 7", "Station 8",
                                                 "Station 9", "Station 10"))
  )


plot1 <- ggplot(data_long,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = NULL, fill = "Chemical Group") +
  theme_minimal(base_size = 27) +
  theme(
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.ticks.length.y = unit(0, "cm"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(), legend.position = "hidden"
  )
plot1

# plotdf1 <- metadata_annotatedbyCaleb %>% 
#   dplyr::select(-c("RT1", "RT2", "Ion1", "Ion2", 
#                    "Stauffer_chemical_group", "Stauffer_compound_name")) %>%
#   pivot_longer(cols = 3:ncol(.), 
#                names_to = "Sample_name", 
#                values_to = "Peak_Area") %>%
#   mutate(
#     sampling_season = case_when(
#       str_detect(Sample_name, "b") ~ "blue",
#       str_detect(Sample_name, "p") ~ "purple",
#       TRUE ~ "orange"
#     ),
#     # Extract station number if pattern F00[1-9] is found, else default to 10
#     gas_station = ifelse(
#       str_detect(Sample_name, "F00[1-9]"),
#       paste("Station", str_remove(str_extract(Sample_name, "F00[1-9]"), "F00")),
#       "Station 10"
#     )
#   ) %>%
#   # Factor the gas_station with levels Station 1 to Station 10
#   mutate(gas_station = factor(gas_station, levels = paste0("Station ", 1:10)))
# 
# plot_list <- list()
# i <- 1
# for (variable in c("Station 3", "Station 7")) { # sort(unique(plotdf1$gas_station))) {
#   plot_df <- plotdf1 %>%
#     dplyr::filter(Chemical_group %notin% "unknown") %>%
#     dplyr::filter(gas_station %in% variable) %>%
#     mutate(
#     Chemical_group = factor(Chemical_group, levels = c("Alkane", "Cycloalkane", "Branched Alkane",
#                                                        "Aromatics", "BTEX", "Alkene", "Castle Group",
#                                                        "Gang of Four", "Indane", "PAHs-Naphthalene"))
#   )
#   
#   plot_list[[i]] <- ggplot(data = plot_df, aes(x = Chemical_group, y = Peak_Area)) +
#     geom_boxplot(aes(fill = Chemical_group), position = "dodge") +
#     labs(title = variable,
#          x = "", 
#          y = "") +
#     theme_classic(base_size = 25) +
#     theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.25,
#                                      size = 12, face = "bold"),
#           axis.ticks.length.x = unit(0, "cm"),
#           legend.position = "hidden",
#           plot.title = element_text(hjust = 0.5),
#           panel.grid = element_blank()
#     ) + 
#     scale_y_continuous(expand = c(0,0), limits = c(0, max(plotdf1$Peak_Area, na.rm = TRUE)))
#   i <- i + 1
# }
# 
# y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
# x <- textGrob("Feature", gp = gpar(fontsize = 20))
# 
# stauffer_boxplot <- grid.arrange(grobs = plot_list, ncol = 2, nrow = 1, left = y, bottom = x)

# ggsave(filename = paste0("Box_plot_RawPA","_chemicalgroup_by gas station", format(Sys.time(), "%d-%m-%Y"),".png"), 
#        plot = stauffer_boxplot, 
#        device = "png",
#        dpi = 600,        # Set DPI to 600
#        width = 20,       # Width in inches (adjust as needed)
#        height = 8,       # Height in inches (adjust as needed)
#        units = "in") 

# After normalization -----------
data <- input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
data[[type_col]] <- as.factor(data[[type_col]])
levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(data[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
data          <- data[data[[type_col]] %in% keep_classes, ]
data[[type_col]] <- droplevels(data[[type_col]])

data <- data %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(data) <- paste0(rownames(data), "_", data$supplier)

# Identify numeric columns from predictors (excluding remove_cols and target)
numeric_cols <- names(which(sapply(data, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))

X_original   <- data[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[["impute_random_uniform"]](X_original)
X_final      <- normalization_methods[["Percentage_Normalization"]](X_imputed)
data[, numeric_cols] <- X_final


plotdf2 <- left_join(data %>% 
            pivot_longer(cols = 5:ncol(.), names_to = "Feature", values_to = "Value"), 
          metadata_annotatedbyCaleb %>% 
            mutate(Feature = as.character(Feature)) %>% 
            dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE))
          , by = "Feature") %>%
  group_by(gas_station, Chemical_group) %>%
  summarise(Value = sum(Value, na.rm=TRUE), .groups = "drop") %>% 
  mutate(
    Chemical_group = factor(Chemical_group, levels = levels(reorder(chemical_group_counts$Chemical_group, -chemical_group_counts$Percentage)))) %>% 
  dplyr::mutate(gas_station = gsub(x=gas_station, replacement = " ", pattern =".", fixed=TRUE)) %>%
  mutate(
    gas_station = factor(gas_station, levels = c("Station 1", "Station 2", "Station 3", "Station 4",
                                                 "Station 5", "Station 6", "Station 7", "Station 8",
                                                 "Station 9", "Station 10"))
  )

plot2 <- ggplot(plotdf2,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = NULL, fill = "Chemical Group") +
  theme_minimal(base_size = 27) +
  theme(
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.ticks.length.y = unit(0, "cm"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(), legend.position = "hidden"
  )
plot2

legend <- cowplot::get_legend(ggplot(plotdf2,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = NULL, fill = "Chemical Group") +
  theme_minimal(base_size = 27) +
  theme(
    legend.text = element_text(size = 25),
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.ticks.length.y = unit(0, "cm"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(), legend.position = "right"
  ))

y <- textGrob("Gas Station", rot = 90, gp = gpar(fontsize = 25))
x <- textGrob("Proportion of total peak area (%)", gp = gpar(fontsize = 25))

# Put "A)" and "B)" in the top-left of each subplot
tag_size <- 27  # same as your base_size

plot1 <- plot1 +
  labs(tag = "A)") +
  theme(
    plot.tag = element_text(face = "bold", size = tag_size),
    plot.tag.position = c(0.02, 0.98),
    legend.position = "none"  # (optional) hide legend in this panel
  )

plot2 <- plot2 +
  labs(tag = "B)") +
  theme(
    plot.tag = element_text(face = "bold", size = tag_size),
    plot.tag.position = c(0.02, 0.98),
    legend.position = "none"  # (optional) hide legend in this panel
  )

figure3 <- grid.arrange(plot1, plot2, ncol = 2, widths = c(1,1), right = legend, left = y, bottom = x)
figure3

# Plotting
# plotdf2 <- data %>%
#   pivot_longer(cols = 5:ncol(.), 
#                names_to = "Feature", 
#                values_to = "Peak_Area") %>%
#   dplyr::mutate(Feature = as.numeric(Feature)) %>%
#   mutate(gas_station = gsub(".", " ", gas_station, fixed = TRUE)) %>%
#   mutate(gas_station = factor(gas_station, levels = paste0("Station ", 1:10)))
# 
# plotdf2_new <- left_join(plotdf2,
#                      metadata_annotatedbyCaleb %>% 
#                        dplyr::select(c("Feature", "Chemical_group")), by = "Feature")
# 
# plot_list2 <- list()
# i <- 1
# for (variable in c("Station 3", "Station 7")) { # sort(unique(plotdf2_new$gas_station))) {
#   plot_df <- plotdf2_new %>%
#     dplyr::filter(Chemical_group %notin% "unknown") %>%
#     dplyr::filter(gas_station %in% variable) %>%
#     mutate(
#     Chemical_group = factor(Chemical_group, levels = c("Alkane", "Cycloalkane", "Branched Alkane",
#                                                        "Aromatics", "BTEX", "Alkene", "Castle Group",
#                                                        "Gang of Four", "Indane", "PAHs-Naphthalene"))
#   )
#   
#   plot_list2[[i]] <- ggplot(data = plot_df, aes(x = Chemical_group, y = Peak_Area)) +
#     geom_boxplot(aes(fill = Chemical_group), position = "dodge") +
#     labs(title = variable,
#          x = "", 
#          y = "") +
#     theme_classic(base_size = 25) +
#     theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.25,
#                                      size = 12, face = "bold"),
#           axis.ticks.length.x = unit(0, "cm"),
#           legend.position = "hidden",
#           plot.title = element_text(hjust = 0.5),
#           panel.grid = element_blank()
#     ) + 
#     scale_y_continuous(expand = c(0,0), limits = c(0, max(plotdf2$Peak_Area, na.rm = TRUE)))
#   i <- i + 1
# }
# 
# y <- textGrob("Normalized Peak Area", rot = 90, gp = gpar(fontsize = 20))
# x <- textGrob("Feature", gp = gpar(fontsize = 20))
# 
# stauffer_boxplot2 <- grid.arrange(grobs = plot_list2, ncol = 2, nrow = 1, left = y, bottom = x)


ggsave(filename = paste0("Figure3_Duel_horizontal-stacked-bar-plot_before and after imp-norm","_chemicalgroup_by gas station_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = figure3, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in") 
```

# Barplot RFA - number of compounds in each chemical group
```{r, echo = FALSE, message = FALSE, warning = FALSE}
rfa_important_sheet <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/RFA_features_19thNov2024_HN.xlsx")
rfa_important_sheet$Feature <- as.character(rfa_important_sheet$Feature)

count_rfa <- rfa_important_sheet %>% 
  group_by(Chemical_group) %>%
  summarise(count = n())

ggplot(data = count_rfa, aes(x = Chemical_group, y = count)) +
  geom_bar(stat = "identity") +
  labs(x = "Chemical group") +
  theme_classic(base_size = 18) +
    theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                     # size = 18,
                                     face = "bold"), 
          axis.ticks.length.x = unit(0, "cm"),
          # legend.text = element_text(size = 18),
          # legend.title = element_text(size = 18),
          # axis.title.y = element_text(size = 15),
          legend.position = "hidden",
          plot.title = element_text(hjust = 0.5),
          panel.grid.minor.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_blank()
    ) + 
    scale_y_continuous(expand = c(0,0))

```

## Stacked bar plot of significant compounds and associated suppliers
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- p_nonunique_df %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Area") %>%
  filter(Feature %in% unique(combined_df$Feature))

rt1 <- c()
rt2 <- c()
major <- c()
minor <- c()
chem_grp <- c()
compound_name <- c()
for (row in 1:nrow(df)) {
  rt1 <- c(rt1, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$RT1)
  rt2 <- c(rt2,new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$RT2)
  major <- c(major, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$Ion1)
  minor <- c(minor, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$Ion2)
  chem_grp <- c(chem_grp, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$Chemical_group)
}

df$RT1 <- rt1
df$RT2 <- rt2
df$Ion1 <- major
df$Ion2 <- minor
df$Chemical_group <- chem_grp

## Get compound name 
compound_name <- c()
for (row in 1:nrow(df)) {
  idx <- which(abs(target_comp$RT1 - df[row,]$RT1) <= 0.1 &
                 abs(target_comp$RT2 - df[row,]$RT2) <= 0.1 &
                 abs(target_comp$Ion1 - df[row,]$Ion1) <= 0.5 &
                 abs(target_comp$Ion2 - df[row,]$Ion2) <= 0.5)
  # print(paste0(row, "_", idx))
  if (identical(idx, integer(0))) {
    compound_name <- c(compound_name, "unknown")
  }
  compound_name <- c(compound_name, target_comp[idx,]$Compound)
}

df$compound_name <- compound_name
df$Sample_name <- factor(df$Sample_name, levels = sort(unique(df$Sample_name)))
plotdf <- df %>% 
  filter(gas_station %in% c("Station 3", "Station 4", "Station 6", "Station 7")) %>%
  filter(compound_name %notin% "unknown")

plot_list <- list()
i <- 1
for (focus_supplier in unique(plotdf$supplier)) {
    plot_df <- plotdf %>%
        filter(supplier %in% focus_supplier)
    
    plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = compound_name)) +
        geom_bar(stat = "identity") +
        labs(title = focus_supplier,
             x = "", 
             y = "") +
        theme_minimal(base_size = 18) +
        theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                         # size = 18,
                                         face = "bold"), 
              axis.ticks.length.x = unit(0, "cm"),
              # legend.text = element_text(size = 18),
              # legend.title = element_text(size = 18),
              # axis.title.y = element_text(size = 15),
              legend.position = "hidden",
              plot.title = element_text(hjust = 0.5),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.major.y = element_blank()
        ) + 
        scale_y_continuous(expand = c(0,0), limits = c(0, 0.5))
    i <- i + 1
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = compound_name)) +
                                  geom_bar(stat = "identity") +
                                  facet_wrap(~supplier) +
                                  labs(fill = "Chemical Groups") +
                                  theme_minimal(base_size = 20) +
                                  theme(legend.text = element_text(size = 18),
                                        legend.title = element_text(size = 20),
                                        axis.title.y = element_text(size = 20),
                                        legend.position = "right"))

y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_list, ncol = 2, nrow = 2, right = legend, left = y, bottom = x)
```

# Stacked bar plot of each suppliers
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- percentage_normalized[[4]] %>% # df_step3  %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Area") %>%
  filter(Feature %in% unique(t_test_res$Feature))

# Obtain chemical group for each feature 
# Create a named vector for fast lookup
compound_name <- setNames(t_test_res$compound_name, t_test_res$Feature)

# Use the vector to directly map the Chemical_group to the Feature in df
df$compound_name <- compound_name[df$Feature]

df$Sample_name <- factor(df$Sample_name, levels = sort(unique(df$Sample_name)))

df <- df %>% 
  filter(gas_station %in% c("Station 3", "Station 4", "Station 6", "Station 7")) %>%
  filter(supplier %notin% "unknown") %>% 
  filter(Chemical_group %notin% "unknown")


plot_list <- list()
i <- 1
for (focus_supplier in unique(df$supplier)) {
  plot_df <- df %>%
    filter(supplier %in% focus_supplier)
  
  plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
    geom_bar(stat = "identity") +
    labs(title = focus_supplier,
         x = "", 
         y = "") +
    theme_minimal(base_size = 18) +
    theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                     # size = 18,
                                     face = "bold"), 
          axis.ticks.length.x = unit(0, "cm"),
          # legend.text = element_text(size = 18),
          # legend.title = element_text(size = 18),
          # axis.title.y = element_text(size = 15),
          legend.position = "hidden",
          plot.title = element_text(hjust = 0.5),
          panel.grid.minor.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_blank()
          ) + 
    scale_y_continuous(expand = c(0,0), limits = c(0, max(df$Area)))
  i <- i + 1
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
                                geom_bar(stat = "identity") +
                                facet_wrap(~supplier) +
                                labs(fill = "Chemical Groups") +
                                theme_minimal(base_size = 20) +
                                theme(legend.text = element_text(size = 18),
                                      legend.title = element_text(size = 20),
                                      axis.title.y = element_text(size = 20),
                                      legend.position = "right"))

y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_list, ncol = 3, nrow = 2, right = legend, left = y, bottom = x)
```

# Stacked bar plot of sampling seasons
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- df_step3  %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Area")

# Obtain chemical group for each feature 
# Create a named vector for fast lookup
chemical_groups <- setNames(metadata$Chemical_group, metadata$Feature)

# Use the vector to directly map the Chemical_group to the Feature in df
df$Chemical_group <- chemical_groups[df$Feature]

df$Sample_name <- factor(df$Sample_name, levels = sort(unique(df$Sample_name)))
df$Octane_rating <- factor(df$Octane_rating, levels = sort(unique(df$Octane_rating)))
df$sampling_season <- factor(df$sampling_season, levels = sort(unique(df$sampling_season)))
df <- df %>% filter(supplier %notin% "unknown") %>% filter(Chemical_group %notin% "unknown")

plot_list <- list()
i <- 1
for (focus_season in levels(df$sampling_season)) {
  for (focus_octane in levels(df$Octane_rating)) {
    plot_df <- df %>%
      filter(sampling_season %in% focus_season) %>%
      filter(Octane_rating %in% focus_octane)
    
    plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
      geom_bar(stat = "identity") +
      labs(title = paste0(focus_season, " - ", focus_octane),
           x = "", 
           y = "") +
      theme_minimal(base_size = 18) +
      theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, size = 15,
                                       face = "bold"), 
            axis.ticks.length.x = unit(0, "cm"),
            legend.text = element_text(size = 18),
            legend.title = element_text(size = 18),
            axis.title.y = element_text(size = 15),
            legend.position = "hidden",
            plot.title = element_text(hjust = 0.5),
            panel.grid.minor.x = element_blank(),
            panel.grid.minor.y = element_blank(),
            panel.grid.major.x = element_blank(),
            panel.grid.major.y = element_blank()
      ) + 
      scale_y_continuous(expand = c(0,0))
    i <- i + 1 
  }
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
                                geom_bar(stat = "identity") +
                                labs(fill = "Chemical Groups") +
                                theme_minimal(base_size = 20) +
                                theme(legend.text = element_text(size = 18),
                                      legend.title = element_text(size = 20),
                                      axis.title.y = element_text(size = 20),
                                      legend.position = "right"))

y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_list, ncol = 4, right = legend, left = y, bottom = x)
```


# Clustering analyses

## PCA and Cluster Resolution

(Update 15 April 2024:) When using either Octane_rating OR sampling_season, there is no clustering whatsoever between seasons OR Octane rate

!!! After removing unique cols
*** With global minimum imputation:

!! Tried top 150 compounds with highest cumulative peak area => still no clustering on both sampling seasons and octane rating.

!! Tried top 150 compounds with highest variance 

(Update 29th April:) 
**DATA COMPRESSION OR NOT??**

If I **compress all samples into a grouping variables**, such as Octane_rating or sampling_season, I can clearly see the differentiation between these grouping variables. 

**For gas station, ** all 3 imputation techniques all resulted in low Dim1 and Dim2. However, with Zero and Global Min, there seems to be a grouping of gas station no. 2,8,3 (group 1); no. 1, 5 (group 2), no. 10, 4, 9, 7, 6 (group 3).

***If I keep the samples separated*** , then I tried log normalization with 3 imputation techniques ==> Zero imputation will resulted in -Inf after Log-normalization so we can only do Global min and LOD. However, both global min and LOD does nto result in any clear grouping/clustering between "sampling_season", "Octane_rating", "gas_station".

**LOG NORMALIZATION VERSUS. PERCENTAGE-BASED NORMALIZATION**
I also tried remove the top 100 compounds that have really high mean and low variance across sample and perform zero//global min// LOD imputation on them and then tried Log-normalization ==> still no clustering between gas station!!

Also, I tried to run PCA with un-normalized data (after imputation) with 3 imputation methods but still no clustering of gas stations across all 3 imputation methods.

(Update 22nd May 2024:) 
 - Clustering again with 'optimized' alignment windows -> No cluster of gas station
 - Use only target compounds -> Still No cluster of gas station either with zero or LOD imputation coupled with TSN or Log normalization.

### PCA
```{r, echo = FALSE, message = FALSE, warning = FALSE}
pca_input <- data
# p_nonunique_df %>% 
# dplyr::select(-setdiff(colnames(p_nonunique_df[,6:ncol(p_nonunique_df)]), final_vec)) # %>% 
# filter(gas_station %in% "Station 1")

res.pca <- FactoMineR::PCA(
  pca_input[, 5:ncol(pca_input)],
  scale.unit = FALSE,
  graph = FALSE)

# Scree plot
# fviz_screeplot(res.pca, ncp=10)

# Biplot

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            # select.var = list(contrib = 3), # Select top x features with highest contribution to PC1/PC2
                            repel = TRUE,
                            labelsize = 5, 
                            habillage = factor(pca_input$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name", "supplier"
                            # palette = "ucscgb",
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            # xlim = c(-0.2, 0.2),
                            # ylim = c(-0.009, 0.009),
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(20, 'mm')) +
  theme(legend.position = 'bottom',
        panel.spacing = unit(0, "lines"))

# ggsave(filename = paste0("PCA_plot","_only_non-uniquefillNA0.001_TSN_sig_comp_nonunique_unionize", format(Sys.time(), "%d-%m-%Y"),".png"), 
#        plot = pcaplot, 
#        device = "png",
#        dpi = 600,        # Set DPI to 600
#        width = 13,       # Width in inches (adjust as needed)
#        height = 8,       # Height in inches (adjust as needed)
#        units = "in") 
```

## Figure 3: PCA biplot of gas station between best and worst ClusterRes score
```{r , echo=FALSE, warning = FALSE, message=FALSE}
### Worst impu/norm ---------
worst_impnorm <- input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
worst_impnorm[[type_col]] <- as.factor(worst_impnorm[[type_col]])
# levels(worst_impnorm[[type_col]]) <- make.names(levels(worst_impnorm[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(worst_impnorm[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
worst_impnorm          <- worst_impnorm[worst_impnorm[[type_col]] %in% keep_classes, ]
worst_impnorm[[type_col]] <- droplevels(worst_impnorm[[type_col]])

worst_impnorm <- worst_impnorm %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(worst_impnorm) <- paste0(rownames(worst_impnorm), "_", worst_impnorm$supplier)

# Identify numeric columns from predictors (excluding remove_cols and target)
numeric_cols <- names(which(sapply(worst_impnorm, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))

worst_imp  <- best_res$results_table$Imputation[which.max(best_res$results_table$combined_rank)]
worst_norm <- best_res$results_table$Normalization[which.max(best_res$results_table$combined_rank)]

X_original   <- worst_impnorm[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[[worst_imp]](X_original)
X_final      <- normalization_methods[[worst_norm]](X_imputed)
worst_impnorm[, numeric_cols] <- X_final

res.pca <- FactoMineR::PCA(
  worst_impnorm[, 5:ncol(worst_impnorm)],
  scale.unit = FALSE,
  graph = FALSE)

pca_worst_impnorm <- factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            # select.var = list(contrib = 3), # Select top x features with highest contribution to PC1/PC2
                            repel = TRUE,
                            labelsize = 5, 
                            habillage = factor(worst_impnorm$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name", "supplier"
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            # xlim = c(-0.2, 0.2),
                            # ylim = c(-0.009, 0.009),
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(20, 'mm')) +
  theme(legend.position = 'bottom',
        panel.spacing = unit(0, "lines"))

### Best impu/norm ---------
best_impnorm <- input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
best_impnorm[[type_col]] <- as.factor(best_impnorm[[type_col]])
# levels(best_impnorm[[type_col]]) <- make.names(levels(best_impnorm[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(best_impnorm[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
best_impnorm          <- best_impnorm[best_impnorm[[type_col]] %in% keep_classes, ]
best_impnorm[[type_col]] <- droplevels(best_impnorm[[type_col]])

best_impnorm <- best_impnorm %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(best_impnorm) <- paste0(rownames(best_impnorm), "_", best_impnorm$supplier)

# Identify numeric columns from predictors (excluding remove_cols and target)
numeric_cols <- names(which(sapply(best_impnorm, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))

best_imp  <- best_res$results_table$Imputation[which.min(best_res$results_table$combined_rank)]
best_norm <- best_res$results_table$Normalization[which.min(best_res$results_table$combined_rank)]

X_original   <- best_impnorm[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[[best_imp]](X_original)
X_final      <- normalization_methods[[best_norm]](X_imputed)
best_impnorm[, numeric_cols] <- X_final

res.pca <- FactoMineR::PCA(
  best_impnorm[, 5:ncol(best_impnorm)],
  scale.unit = FALSE,
  graph = FALSE)

pca_best_impnorm <- factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            # select.var = list(contrib = 3), # Select top x features with highest contribution to PC1/PC2
                            repel = TRUE,
                            labelsize = 5, 
                            habillage = factor(best_impnorm$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name", "supplier"
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            # xlim = c(-0.2, 0.2),
                            # ylim = c(-0.009, 0.009),
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(20, 'mm')) +
  theme(legend.position = 'bottom',
        panel.spacing = unit(0, "lines"))

ggsave(filename = paste0("Figure 3_PCA_plot","_worst_impnorm_by_gasstation_", format(Sys.time(), "%d-%m-%Y"),".png"),
       plot = pca_worst_impnorm,
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 13,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in")
```


### Barplot of Top 10 Variable contributions to first n dimensions

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Variable contributions to first n dimensions
## To PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
## To PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)

# Extract top 10 contribution to PC1
dim_df <- as.data.frame(res.pca$var$contrib) %>% 
  rownames_to_column(var = "Feature") %>%
  arrange(desc(Dim.1), desc(Dim.2))

# Extract top x highest loadings
top_pca_loadings <- dim_df[1:25,]$Feature
```

### Cluster resolution
```{r, echo = FALSE, message = FALSE, warning = FALSE}
data <- df_percentage[, -c(1,2,3,4,5)] # %>% 
    # dplyr::select(unique(sig_comp[[5]]$Feature))
group <- factor(df_percentage$supplier)
max_resolution <- 0
resolutions <- c()

pca_model <- prcomp(data, center = TRUE, scale. = FALSE)
scores <- pca_model$x

group_levels <- levels(group)
pair_resolutions <- c()

# Calculate cluster resolution for each pair of groups
for (g1 in 1:(length(group_levels) - 1)) {
  for (g2 in (g1 + 1):length(group_levels)) {
    data1 <- scores[group == group_levels[g1], 1:2]
    data2 <- scores[group == group_levels[g2], 1:2]
    resolution <- calculate_cluster_resolution(data1, data2)
    pair_resolutions <- c(pair_resolutions, resolution)
  }
}

avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
resolutions <- c(resolutions, avg_resolution)
print(resolutions)
```

# RANDOM FOREST
```{r , echo=FALSE, warning = FALSE, message=FALSE}
recursive_feature_addition <- function(X_train, y_train,
                                       X_test,  y_test,
                                       ntree_candidates,
                                       metric) {
  # 1) Fit full model to get initial feature importance
  full_res <- tune_rf_subset(
    X_train, y_train,
    X_test,  y_test, ntree_candidates, metric
  )
  # Extract importance from the randomForest final model
  imp_mat <- importance(full_res$model$finalModel)
  # Choose first top feature by MeanDecreaseAccuracy (or use MeanDecreaseGini) to use as base feature to initiate RFA
  init_feat <- rownames(imp_mat)[which.max(imp_mat[, "MeanDecreaseAccuracy"])]

  # Initialize feature sets
  best_feats <- init_feat
  remaining  <- setdiff(colnames(X_train), best_feats)

  # 2) Baseline performance with the single top feature
  base_res      <- tune_rf_subset(
    X_train[, best_feats, drop = FALSE], y_train,
    X_test [, best_feats, drop = FALSE], y_test,
    ntree_candidates, metric
  )
  best_accuracy <- base_res$Accuracy
  best_kappa    <- base_res$Kappa
  best_auc      <- base_res$AUC

  # 3) Iterative addition of remaining features
  while(length(remaining) > 0) {
    improved <- FALSE
    for(feat in remaining) {
      trial_feats <- c(best_feats, feat)
      res <- tune_rf_subset(
        X_train[, trial_feats, drop = FALSE], y_train,
        X_test [, trial_feats, drop = FALSE], y_test, ntree_candidates, metric
      )
      # Add feature only if all three metrics improve
      if(res$Accuracy > best_accuracy &&
         res$Kappa    > best_kappa    &&
         res$AUC      > best_auc) {
        best_accuracy <- res$Accuracy
        best_kappa    <- res$Kappa
        best_auc      <- res$AUC
        best_feats    <- trial_feats
        improved      <- TRUE
        break
      }
    }
    # Drop the last-tested feature from remaining
    remaining <- setdiff(remaining, feat)
    if(!improved) break
  }

  # 4) Final model on the chosen feature set
  final_res <- tune_rf_subset(
    X_train[, best_feats, drop = FALSE], y_train,
    X_test [, best_feats, drop = FALSE], y_test,
    ntree_candidates, metric
  )

  # 5) Return performance and selected features
  list(
    best_accuracy    = final_res$Accuracy,
    best_kappa =  final_res$Kappa,
    best_auc = final_res$AUC,
    best_features_rf = best_feats,
    final_model      = final_res$model,
    prob_matrix = final_res$p_m,
    predictions = final_res$preds,
    eval_metrics = final_res$eval_metrics
  )
}

#─────────────────────────────────────────────────────────────────────────────
# Helper: retune mtry (based on subset size) + ntree for any train/test split
tune_rf_subset <- function(X_train, y_train, X_test, y_test,
                           ntree_candidates, metric) {
  # 1) Check class counts
  k_inner <- min(table(y_train))
  
  # 2) Build the mtry grid
  p        <- ncol(X_train)
  mtry_vals <- unique(pmax(1, c(floor(sqrt(p)), floor(log2(p)))))
  
  # 3) Branch: enough data for CV?
  if (k_inner > 1) {
    # —> use caret CV
    ctrl <- trainControl(
      method         = "cv",
      number         = k_inner,
      classProbs     = TRUE,
      summaryFunction= defaultSummary
    )
    rf_grid <- expand.grid(mtry = mtry_vals)
    
    best_mod   <- NULL
    best_val   <- -Inf
    best_trees <- NA
    
    for (nt in ntree_candidates) {
      set.seed(123)
      tmp <- caret::train(
        x         = X_train,
        y         = as.factor(y_train),
        method    = "rf",
        trControl = ctrl,
        tuneGrid  = rf_grid,
        metric    = metric,
        ntree     = nt,
        importance= TRUE
      )
      this_val <- max(tmp$results[[metric]])
      if (this_val > best_val) {
        best_val   <- this_val
        best_mod   <- tmp
        best_trees <- nt
      }
    }
    
    caret_model <- best_mod
    final_rf    <- caret_model$finalModel
    
  } else {
    # —> too few per class: do OOB-based tuning
    best_oob   <- Inf
    best_mtry  <- NA
    best_trees <- NA
    best_rf    <- NULL
    
    for (nt in ntree_candidates) {
      for (m in mtry_vals) {
        set.seed(123)
        rf_mod <- randomForest(
          x          = X_train,
          y          = as.factor(y_train),
          mtry       = m,
          ntree      = nt,
          importance = TRUE
        )
        # final OOB error rate:
        oob_err <- tail(rf_mod$err.rate[ , "OOB"], 1)
        if (oob_err < best_oob) {
          best_oob   <- oob_err
          best_mtry  <- m
          best_trees <- nt
          best_rf    <- rf_mod
        }
      }
    }
    # wrap into a dummy caret-like object so that downstream calls to predict(<train>,…) and predict(<train>, type="prob") continue to work.
    caret_model <- list(
      finalModel = best_rf,
      # so that later code that uses predict(<train>,…) still works:
      methods = list(predict = "randomForest")
    )
    final_rf <- best_rf
  }
  
  # 4) Predict & score
  preds       <- predict(final_rf, newdata = X_test, type = "response")
  prob_matrix <- as.matrix(predict(final_rf, newdata = X_test, type = "prob"))
  ev          <- eval_metrics(true_labels = y_test, 
                                   prob_matrix = prob_matrix, 
                                   pred_labels = preds)
  
  return(list( model        = caret_model,
               ntree        = best_trees,
               preds        = preds,
               p_m          = prob_matrix,
               eval_metrics = ev,
               Accuracy = ev$Accuracy,
               Kappa = ev$Kappa,
               AUC = ev$AUC,
               F1_Macro     = ev$F1_Macro,
               F1_Weighted = ev$F1_Weighted,
               MCC          = ev$MCC_Multiclass
  ))
}

#─────────────────────────────────────────────────────────────────────────────
library(caret)
library(pROC)

eval_metrics <- function(true_labels, prob_matrix, pred_labels){
  # 0) Ensure factor levels align
  classes <- union(levels(true_labels), levels(pred_labels))
  y_true <- factor(true_labels, levels = classes)
  y_pred <- factor(pred_labels,  levels = classes)

  # 1) Build the confusion‐matrix table
  cm <- table(Actual = y_true, Predicted = y_pred)
  N  <- sum(cm)                       # total samples
  diag_cm <- diag(cm)

  # 2) Accuracy
  accuracy <- sum(diag_cm) / N

  # 3) Cohen’s Kappa (via caret)
  kappa <- tryCatch({
    cm_obj <- confusionMatrix(y_pred, y_true)
    as.numeric(cm_obj$overall["Kappa"])
  }, error = function(e) NA)

  # 4) One‐vs‐all multiclass AUC
  auc <- tryCatch({
    roc_obj <- multiclass.roc(response  = true_labels,
                              predictor = prob_matrix)
    as.numeric(roc_obj$auc)
  }, error = function(e) NA)

  # 5) Per‐class precision & recall
  col_sums <- colSums(cm)   # predicted totals for each class
  row_sums <- rowSums(cm)   # actual   totals for each class

  precision_per_class <- ifelse(col_sums > 0,
                                diag_cm / col_sums,
                                0)        # no preds → p=0
  recall_per_class    <- ifelse(row_sums > 0,
                                diag_cm / row_sums,
                                0)        # no actual → r=0

  # 6) Per‐class F1 & weighted F1
  f1_per_class <- ifelse(
    (precision_per_class + recall_per_class) > 0,
    2 * precision_per_class * recall_per_class /
      (precision_per_class + recall_per_class),
    0
  )
  support <- row_sums   # how many true examples of each class
  f1_weighted <- sum(f1_per_class * support) / sum(support)

  # 7) Multiclass MCC (Gorodkin’s formula)
  t_i <- row_sums
  p_j <- col_sums
  c_val <- sum(diag_cm)         # sum of true positives
  num   <- N * c_val - sum(t_i * p_j) # (TP×TN)-(FP×FN)
  den   <- sqrt((N^2 - sum(t_i^2)) * (N^2 - sum(p_j^2))) # sqrt((TP+FP)×(TP+FN)×(TN+FP)×(TN+FN)) 
  mcc   <- ifelse(den > 0, num / den, 0)

  return(list(
    Accuracy         = accuracy,
    Kappa            = kappa,
    AUC              = auc,
    F1_Weighted      = f1_weighted,
    MCC_Multiclass   = mcc
  ))
}

####################################################
##########        CONFUSION MATRIX       ###########
####################################################
conf_mat_plot <- function(y_test, preds, conf_mat_title, accuracy) {
  # 1) Compute raw counts table: rows = treated labels, cols = predicted pristine labels
  all_classes <- unique(c(levels(preds), levels(y_test)))
  y_f <- factor(y_test,  levels = all_classes)
  p_f <- factor(preds,  levels = all_classes)
  
  cm_tab <- table(Actual = y_f, Predicted = p_f)
  
  # 2) Turn it into a data.frame for ggplot
  cm_df <- as.data.frame(cm_tab, stringsAsFactors = FALSE) %>%
    dplyr::rename(Freq = Freq)
  
  # 3) Add total-per-row and percent-per-cell
  cm_df$Total <- NA
  cm_df$Percent <- NA
  for (label in unique(cm_df$Actual)){
    cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
  }
  
  cm_df <- cm_df %>%
    mutate(
      Percent = round(Freq / Total * 100, 2)
    )
  
  
  cm_df <- cm_df %>%
    mutate(
      Label = ifelse(Predicted == Actual,
                     paste0(round(Percent,1),"%"),
                     ""
      )
    )
  
  # 5) Plot
  conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
    geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
    # explicitly drop any size guide, keep only the fill (Recall) legend
    guides(size = "none") +
    labs(
      title = paste0("Confusion Matrix | ", conf_mat_title),
      subtitle = paste0(
        sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
        # # " Recall per treated class (N=", cm_df$Total[1], 
        # " replicates each)"
      ),
      x = "Predicted Class",
      y = "Actual Class",
      fill = "Percentage of Classification (%)" # "Recall %"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      axis.text.x  = element_text(angle = 45, hjust = 1),
      plot.title   = element_text(face = "bold", hjust = 0.5),
      plot.subtitle= element_text(hjust = 0.5)
    )
  return(conf_mat)
}

#─────────────────────────────────────────────────────────────────────────────

run_rf_analysis_arson <- function(data,
                                  type_col,
                                  remove_cols,
                                  train_proportion,
                                  seed             = 123,
                                  ntree_candidates = c(100, 500, 1000, 2500),
                                  metric           = "Accuracy",
                                  do_rfe           = FALSE,
                                  rfe_folds        = 5,
                                  do_sig           = FALSE,
                                  do_rfa           = FALSE,
                                  do_impute_norm_screen = TRUE,
                                  split_with_miscunknown_supplier = FALSE,
                                  excluding_supplier,
                                  group_for_significance,                     
                                  metadata,
                                  target_comp,
                                  start_col_index) {

  # 0) Optional set seed
  if(!is.null(seed)) set.seed(seed)
  
  # 1) Factorize the target column and remove classes with only one sample
  data[[type_col]] <- as.factor(data[[type_col]])
  levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique=TRUE)
  
  # Remove single-member classes
  class_counts  <- table(data[[type_col]])
  keep_classes  <- names(class_counts[class_counts > 1])
  data          <- data[data[[type_col]] %in% keep_classes, ]
  data[[type_col]] <- droplevels(data[[type_col]])
  cat("NAs in original data:", sum(is.na(data)), "\n")

  data <- data %>% 
    tibble::column_to_rownames(., "Sample_name") 
  rownames(data) <- paste0(rownames(data), "_", data$supplier)
  
  # 2) If do_impute_norm_screen=TRUE, find best combination
  best_imp  <- "No_imputation"
  best_norm <- "No_normalization"
  if(do_impute_norm_screen){
    cat("\n### Screening best Imputation + Normalization combo ###\n")
    start_time_imp_norm <- Sys.time()
    # Identify numeric columns from predictors (excluding remove_cols and target)
    numeric_cols <- names(which(sapply(data, is.numeric)))
    numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))
    
    best_res <- find_best_impute_normalize_new(
      data,
      type_col = type_col,
      remove_cols = remove_cols,
      group_for_significance = group_for_significance, 
      train_proportion = train_proportion,
      split_with_miscunknown_supplier = split_with_miscunknown_supplier,
      metadata = metadata,
      target_comp = target_comp,
      start_col_index = start_col_index)
    cat("\n*** Summary of all combos ***\n")
    print(best_res$results_table)
    
    cat("\n*** Best combo ***\n")
    print(best_res$best_combo)
    
    best_imp  <- best_res$best_combo$Imputation
    best_norm <- best_res$best_combo$Normalization
    
    X_original   <- data[, numeric_cols, drop = FALSE]
    X_imputed    <- imputation_methods[[best_imp]](X_original)
    X_final      <- normalization_methods[[best_norm]](X_imputed)
    data[, numeric_cols] <- X_final
    
  } else {
    cat("\n### Skipping impute+norm screen; using raw data ###\n")
  }
  
  cat("NAs after final imputation+normalization:", sum(is.na(data)), "\n")
  end_time_imp_norm   <- Sys.time()
  time_imp_norm      <- as.numeric(difftime(end_time_imp_norm,
                                                  start_time_imp_norm, 
                                                  units="secs"))
  
  # 3) Now proceed with your normal train/test code
  # X <- data %>%
  #   dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  
  y <- data[[type_col]]
  
  
  cat("\n### Using Original Train/Test Split ###\n")
  
  set.seed(123) # set.seed(sample.int(999, 1))
  train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
  train_data <- data[train_index, ]
  test_data  <- data[-train_index, ]
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 5) Grid for mtry
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  # 6) Grid search over (mtry, ntree)
  cat("\n### Train/test with Full original features ###\n")
  start_time_full_features <- Sys.time()
  
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test,
    ntree_candidates, metric
  )
  
  best_model_all_feats      <- all_feats$model$finalModel
  acc_full        <- all_feats$Accuracy
  
  # --- Generate Heatmap (Full Features) ---
  {prob_matrix_all_feats <- all_feats$p_m
   conf_mat_all_feats <- conf_mat_plot(y_test, all_feats$preds, conf_mat_title = paste0("All Features - ", type_col), accuracy = acc_full) 
    # pheatmap(
    #   prob_matrix,
    #   fontsize = 20,
    #   fontsize_number = 30,
    #   cluster_rows    = FALSE,
    #   cluster_cols    = FALSE,
    #   color           = viridis(100),
    #   display_numbers = TRUE,
    #   number_format   = "%.2f",
    #   main = paste("Classification Probability (All Features) -", sprintf("| Test Accuracy: %.2f", acc_full)),
    #   breaks = seq(0, 1, length.out = 101)
    # )
  }
  
  eval_metrics_all_feats <- all_feats$eval_metrics
  
  end_time_full_features   <- Sys.time()
  time_full_features       <- as.numeric(difftime(end_time_full_features,
                                                  start_time_full_features, 
                                                  units="secs"))
  
  # 9) Feature Importances
  # rf_importances         <- as.data.frame(final_rf$importance)
  # rf_importances$Feature <- rownames(rf_importances)
  # rf_importances         <- rf_importances[order(rf_importances$MeanDecreaseGini, decreasing=TRUE), ]
  
  # Significance-Based Feature Selection using pairwise tests
  if(do_sig){
    cat("\n### Train/test with Pair-wise features ###\n")
    acc_sig <- NA; sig_feats <- NULL; final_sig_model <- NULL
    start_time_sig <- Sys.time()
    
    pw <- pairwise_significance_tests_modified(
      input_df        = data,
      group_col       = group_for_significance,
      start_col_index = start_col_index,
      metadata        = metadata,
      target_comp     = target_comp
    )
    
    idx   <- which.max(sapply(pw$corrected,
                              function(df) length(unique(df$Feature))))
    feats0<- unique(pw$corrected[[idx]]$Feature)
    sig_feats <- intersect(feats0, colnames(X_train))
    if(length(sig_feats)>0) {
      tmp_sig <- tune_rf_subset(
        X_train[, sig_feats, drop=FALSE], y_train,
        X_test [, sig_feats, drop=FALSE], y_test,
        ntree_candidates, metric
      )
      acc_sig         <- tmp_sig$Accuracy
      final_sig_model <- tmp_sig$model$finalModel
    }

    {prob_matrix_sig <- tmp_sig$p_m
      conf_mat_sig <- conf_mat_plot(y_test, tmp_sig$preds, conf_mat_title = paste0("Pair-wise Significance-Based Features - ", type_col), accuracy = acc_sig)
      # pheatmap(
      #   prob_matrix_sig,
      #   fontsize = 20,
      #   fontsize_number = 30,
      #   cluster_rows    = FALSE,
      #   cluster_cols    = FALSE,
      #   color           = viridis(100),
      #   display_numbers = TRUE,
      #   number_format   = "%.2f",
      #   main = paste("Classification Probability (Pair-wise Significance-Based Features) -", sprintf("| Test Accuracy: %.2f", acc_sig)),
      #   breaks = seq(0, 1, length.out = 101)
      # )
    }
    
    eval_metrics_sig <- tmp_sig$eval_metrics
    
    end_time_sig   <- Sys.time()
    time_sig       <- as.numeric(difftime(end_time_sig, start_time_sig, units="secs"))
  }
  
  
  # Recursive Feature Addition
  if(do_rfa){
    cat("\n### Train/test with RFA features ###\n")
    start_time_rfa <- Sys.time()
    
    rfa_res   <- recursive_feature_addition(
      X_train, y_train,
      X_test,  y_test,
      ntree_candidates = ntree_candidates,
      metric           = metric
    )
    
    rfa_selected_feats <- rfa_res$best_features_rf
    acc_rfa                 <- rfa_res$best_accuracy
    final_rf_rfa            <- rfa_res$final_model$finalModel
    
    prob_matrix_rfa <- rfa_res$prob_matrix
    
    conf_mat_rfa <- conf_mat_plot(y_test, rfa_res$predictions, conf_mat_title = paste0("RFA Features - ", type_col), accuracy = acc_rfa)
    
    # {pheatmap(
    #     prob_matrix_rfa,
    #     fontsize = 20,
    #     fontsize_number = 30,
    #     cluster_rows    = FALSE,
    #     cluster_cols    = FALSE,
    #     color           = viridis(100),
    #     display_numbers = TRUE,
    #     number_format   = "%.2f",
    #     main = paste("Classification Probability (Recursive Feature Addition) -", sprintf("| Test Accuracy: %.2f", acc_rfa)),
    #     breaks = seq(0, 1, length.out = 101)
    #   )
    # }
    
    eval_metrics_rfa <- rfa_res$eval_metrics
    
    end_time_rfa   <- Sys.time()
    time_rfa       <- as.numeric(difftime(end_time_rfa, start_time_rfa, units="secs"))
  }
  
  # RFE with hyperparams retuning
  if(do_rfe){
    cat("\n### Train/test with RFE features ###\n")
    acc_rfe <- NA; rfe_feats <- NULL; final_rfe_model <- NULL
    start_time_rfe <- Sys.time()
    
    myFuncs <- rfFuncs
    myFuncs$fit <- function(x, y, first, last, ...) {
      res <- tune_rf_subset(
        X_train = x, y_train = y,
        X_test  = x, y_test  = y,
        ntree_candidates, metric
      )
      res$model$finalModel
    }
    
    # compute the smallest class count
    rfe_k <- min(table(train_data[[type_col]]))
    rfe_k <- if (rfe_k > 1) rfe_k else 2
    
    rfe_ctl <- rfeControl(
      functions   = myFuncs,
      method      = "cv",
      number      = rfe_k,
      saveDetails = TRUE,
      returnResamp= "final"
    )
    rfe_out <- rfe(
      x          = X_train,
      y          = y_train,
      sizes      = seq_len(ncol(X_train)),
      rfeControl = rfe_ctl
    )
    rfe_feats <- rfe_out$optVariables
    
    # final retune+test on RFE set
    tmp_rfe <- tune_rf_subset(
      X_train[, rfe_feats, drop=FALSE], y_train,
      X_test[, rfe_feats, drop=FALSE], y_test,
      ntree_candidates, metric
    )
    acc_rfe          <- tmp_rfe$Accuracy
    final_rfe_model  <- tmp_rfe$model$finalModel

    # --- Generate Heatmap (RFE) ---
    {prob_matrix_rfe <- tmp_rfe$p_m
      conf_mat_rfe <-  conf_mat_plot(y_test, tmp_rfe$preds, conf_mat_title = paste0("RFE Features - ", type_col), accuracy = acc_rfe)
      # pheatmap(
      #   prob_matrix_rfe,
      #   fontsize = 20,
      #   fontsize_number = 30,
      #   cluster_rows    = FALSE,
      #   cluster_cols    = FALSE,
      #   color           = viridis(100),
      #   display_numbers = TRUE,
      #   number_format   = "%.2f",
      #   main = paste("Classification Probability (RFE Features) -", sprintf("|  Test Accuracy: %.2f", acc_rfe)),
      #   breaks = seq(0, 1, length.out = 101)
      # )
      
    }
    
    eval_metrics_rfe <- tmp_rfe$eval_metrics
    
    end_time_rfe   <- Sys.time()
    time_rfe       <- as.numeric(difftime(end_time_rfe, start_time_rfe, units="secs"))
  }
  
  cat("\nFull-Feature Balanced Test Accuracy:", acc_full, "\n")
  if(do_rfe) cat("\nRFE-Optimized Balanced Test Accuracy:", acc_rfe, "\n")
  if(do_sig) cat("\nPairwise Selected Features Balanced Test Accuracy:", acc_sig, "\n")
  if(do_rfa) cat("\nRecursive Feature Addition Test Accuracy:", acc_rfa, "\n")

  # Return a list of final results
  result_list <- list(
    final_imp_norm_dat    = data,
    final_rf_all_feats      = best_model_all_feats,
    prob_matrix_all_feats = prob_matrix_all_feats,
    all_features_acc     = acc_full,
    time_full_features = time_full_features,
    eval_metrics_all_feats = eval_metrics_all_feats,
    conf_mat_all_feats = conf_mat_all_feats
  )
  
  if(do_impute_norm_screen) {
    result_list$time_imp_norm <- time_imp_norm
    result_list$imp_norm_res_table    <- best_res$results_table
    result_list$best_imputatation     <-  best_imp
    result_list$best_normalization    <- best_norm
  }
  
  if(do_sig){
    result_list$sig_model    <- final_sig_model
    result_list$sig_selected_feats <- sig_feats
    result_list$prob_matrix_sig <- prob_matrix_sig
    result_list$acc_sig <- acc_sig
    result_list$time_sig          <- time_sig
    result_list$eval_metrics_sig <- eval_metrics_sig
    result_list$conf_mat_sig <- conf_mat_sig
  }
  
  if(do_rfa) {
    result_list$rfa_model <- final_rf_rfa
    result_list$rfa_selected_feats <- rfa_selected_feats
    result_list$prob_matrix_rfa    <- prob_matrix_rfa
    result_list$acc_rfa  <- acc_rfa
    result_list$time_rfa                <- time_rfa
    result_list$eval_metrics_rfa <- eval_metrics_rfa
    result_list$conf_mat_rfa <- conf_mat_rfa
  } 
  if(do_rfe) {
    result_list$rfe_model             <- rfe_out
    result_list$final_rf_rfe          <- final_rfe_model
    result_list$rfe_selected_features <- rfe_feats
    result_list$prob_matrix_rfe    <- prob_matrix_rfe
    result_list$acc_rfe  <- acc_rfe
    result_list$time_rfe              <- time_rfe
    result_list$eval_metrics_rfe <- eval_metrics_rfe
    result_list$conf_mat_rfe <- conf_mat_rfe
  }
  
  return(result_list)
}
```


```{r , echo=FALSE, warning = FALSE, message=FALSE}
target_comp <- read_xlsx(path = paste0(getwd(), "/python/Shortened ILR Compound List PF001A 07-06-2024_Huy_modified_08Aug2024.xlsx"))

input_rf <- metadata_annotatedbyCaleb %>%
  dplyr::select(-c("RT1", "RT2", "Ion1", "Ion2", 
                   "Stauffer_chemical_group", "Stauffer_compound_name", "Chemical_group")) %>%
  pivot_longer(cols = 2:ncol(.), 
               names_to = "Sample_name", 
               values_to = "Peak_Area") %>%
  mutate(
    Octane_rating = case_when(
      str_detect(Sample_name, "A") ~ "Gas 87",
      str_detect(Sample_name, "B") ~ "Gas 89",
      str_detect(Sample_name, "C") ~ "Gas 91",
      TRUE ~ "Gas 94"
    ),
    sampling_season = case_when(
      str_detect(Sample_name, "b") ~ "blue",
      str_detect(Sample_name, "p") ~ "purple",
      TRUE ~ "orange"
    ),
    gas_station = case_when(
      str_detect(Sample_name, "F001") ~ "Station 1",
      str_detect(Sample_name, "F002") ~ "Station 2",
      str_detect(Sample_name, "F003") ~ "Station 3",
      str_detect(Sample_name, "F004") ~ "Station 4",
      str_detect(Sample_name, "F005") ~ "Station 5",
      str_detect(Sample_name, "F006") ~ "Station 6",
      str_detect(Sample_name, "F007") ~ "Station 7",
      str_detect(Sample_name, "F008") ~ "Station 8",
      str_detect(Sample_name, "F009") ~ "Station 9",
      TRUE ~ "Station 10"
    ),
    supplier = case_when(
      str_detect(Sample_name, "F001") ~ "Unbranded", # "Miscellanous",
      str_detect(Sample_name, "F002") ~ "Unbranded", # "Miscellanous",
      str_detect(Sample_name, "F003") ~ "Shell",
      str_detect(Sample_name, "F004") ~ "Cenovus",
      str_detect(Sample_name, "F005") ~ "Unbranded", # "Miscellanous",
      str_detect(Sample_name, "F006") ~ "Suncor",
      str_detect(Sample_name, "F007") ~ "Imperial",
      str_detect(Sample_name, "F008") ~ "Burnaby",
      str_detect(Sample_name, "F009") ~ "Imperial",
      TRUE ~ "Unbranded"
    )
  ) %>%
  relocate(Octane_rating, sampling_season, gas_station, supplier, .after = 1) %>% 
  pivot_wider(names_from = "Feature", values_from = "Peak_Area") #%>% 
  # column_to_rownames(., var = "Sample_name")


######################################################
# rf_results_by_supplier_df_no_na_list <- list()
rf_results_by_gas_station_with_miscnunknown_list <- list()
rf_results_by_supplier_with_miscnunknown_list <- list()

for (i in 1:10) {
  # rf_results_by_supplier_df_no_na_list[[i]] <- run_rf_analysis_arson(df_no_na %>%
  #                                                                      dplyr::filter(supplier %notin% "Unbranded"),
  #                                                                    train_proportion = 0.8,
  #                                                                    type_col = "supplier", 
  #                                                                    remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
  #                                                                    group_for_significance = "supplier",
  #                                                                    metadata = core_metadata,
  #                                                                    target_comp = target_comp, 
  #                                                                    start_col_index = 5,
  #                                                                    do_rfe = TRUE,
  #                                                                    do_gbfe = TRUE,
  #                                                                    do_sig = TRUE,
  #                                                                    do_rfa = TRUE)
  
    rf_results_by_gas_station_with_miscnunknown_list[[i]] <- run_rf_analysis_arson(input_rf,
                                                                                   train_proportion = 0.66,
                                                                                   type_col = "gas_station",
                                                                     remove_cols = c("sampling_season", "supplier", "Octane_rating"),
                                                                     group_for_significance = "gas_station",
                                                                     metadata = core_metadata,
                                                                     target_comp = target_comp,
                                                                     start_col_index = 5,
                                                                     do_rfe = TRUE,
                                                                     do_sig = TRUE,
                                                                     do_rfa = TRUE)
    
    rf_results_by_supplier_with_miscnunknown_list[[i]] <- run_rf_analysis_arson(input_rf,
                                                                                train_proportion = 0.8,
                                                                                type_col = "supplier",
                                                                     remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
                                                                     group_for_significance = "supplier",
                                                                     metadata = core_metadata,
                                                                     target_comp = target_comp,
                                                                     start_col_index = 5,
                                                                     do_rfe = TRUE,
                                                                     do_sig = TRUE,
                                                                     do_rfa = TRUE)
}

# Use these for final manuscript
rf_results_by_gas_station_with_miscnunknown_rfe
rf_results_by_gas_station_with_miscnunknown_sig_rfa
rf_results_by_supplier_with_miscnunknown_rfe 
rf_results_by_supplier_with_miscnunknown_sig_rfa 

## Plot RFE-CV Accuracy against number of Variables
# lattice::trellis.par.set(caret::caretTheme())
rfe_cv_accuracy_plot <- plot(rf_results_by_supplier_with_miscnunknown_list[[4]]$rfe_model, 
                             type = c("g", "o"),
                             xlab = "Subset sizes")
bigfont_plot <- update(
  rfe_cv_accuracy_plot,
  par.settings = list(
    par.main.text  = list(cex = 2.2),
    par.xlab.text  = list(cex = 1.6),
    par.ylab.text  = list(cex = 1.6),
    axis.text      = list(cex = 1.3),
    par.strip.text = list(cex = 1.5)
  )
)
png(
  filename = paste0("Figure S3_RFE-CV Accuracy across different subset sizes_", "supplier_", format(Sys.time(), "%d-%m-%Y"),".png"),
  width    = 20,      # in inches
  height   = 8,       # in inches
  units    = "in",    # interpret width/height as inches
  res      = 300      # DPI
)

print(bigfont_plot)  # send the trellis object to the device
dev.off()

# SI Table S5-S7
## Table S5: RFE
metadata_annotatedbyCaleb[] %>%
  dplyr::filter(Feature %in% as.numeric(rf_results_by_gas_station_with_miscnunknown_rfe$rfe_selected_features))


## Table S6: Pairwise

## Table S7: RFA
```

# k-fold CV - All vs. Pairwise vs. RFA vs. RFE: This k-fold cv process was done on week of 13th June 2025
```{r , echo=FALSE, warning = FALSE, message=FALSE}
data = input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
group_for_significance = "gas_station"
metadata = core_metadata
target_comp = target_comp
start_col_index = 5
do_rfe = TRUE
do_sig = TRUE
do_rfa = TRUE
rfe_folds = 5
ntree_candidates = c(100, 500, 1000, 2500)
metric           = "Accuracy"
set.seed(123)

data[[type_col]] <- as.factor(data[[type_col]])
levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(data[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
data          <- data[data[[type_col]] %in% keep_classes, ]
data[[type_col]] <- droplevels(data[[type_col]])

data <- data %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(data) <- paste0(rownames(data), "_", data$supplier)

numeric_cols <- names(which(sapply(data, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))
X_original   <- data[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[["impute_random_uniform"]](X_original)
X_final      <- normalization_methods[["Percentage_Normalization"]](X_imputed)
data[, numeric_cols] <- X_final

min_class_count_all <- min(table(data[[type_col]]))
K <- 3 # ifelse(min_class_count_all > 1, min_class_count_all, 2)
# stratified folds on the full target vector
K_folds <- createFolds(data[[type_col]], k = K, returnTrain = TRUE)

res_list_5repeated3fold <- vector("list", length = K * 5) 
conf_mat_list_5repeated3fold <- vector("list", length = K * 5) 
i <- 1

for(f in seq_along(K_folds)) {
  # split indices
  train_idx <- K_folds[[f]]
  test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
  
  train_data <- data[train_idx, ]
  test_data  <- data[test_idx, ]
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 4) Cross-validation control
  min_class_count <- min(table(y_train))
  folds_inner <- ifelse(min_class_count > 1, min_class_count, 2)
  cv_ctrl <- trainControl(
    method     = "cv",
    number     = folds_inner,
    classProbs = TRUE
  )
  
  # 5) Grid for mtry
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  # 6) Grid search over (mtry, ntree)
  cat("\n### Train/test with Full original features ###\n")
  start_time_all_feats <- Sys.time()
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test,
    cv_ctrl, ntree_candidates, metric
  )
  end_time_all_feats   <- Sys.time()
  time_all_feats       <- as.numeric(difftime(end_time_all_feats, start_time_all_feats, units="secs"))
  
  res_list_5repeated3fold[[i]] <- data.frame(
    fold   = f,
    method = "All",
    all_feats$eval_metrics, 
    time = time_all_feats, 
    num_selected_feats = ncol(X_train)
  )
  conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, all_feats$preds, 
                                      conf_mat_title = paste0("All Features - ", 
                                                              type_col), 
                                      accuracy = all_feats$Accuracy); i <- i + 1
  
  # Significance-Based Feature Selection using pairwise tests
  cat("\n### Train/test with Pair-wise features ###\n")
  start_time_sig <- Sys.time()
  pw <- pairwise_significance_tests_modified(
    input_df        = data,
    group_col       = group_for_significance,
    start_col_index = start_col_index,
    metadata        = metadata,
    target_comp     = target_comp
  )
  
  idx   <- which.max(sapply(pw$corrected,
                            function(df) length(unique(df$Feature))))
  feats0<- unique(pw$corrected[[idx]]$Feature)
  sig_feats <- intersect(feats0, colnames(X_train))
  if(length(sig_feats)>0) {
    tmp_sig <- tune_rf_subset(
      X_train[, sig_feats, drop=FALSE], y_train,
      X_test [, sig_feats, drop=FALSE], y_test,
      cv_ctrl, ntree_candidates, metric
    )
    end_time_sig   <- Sys.time()
    time_sig       <- as.numeric(difftime(end_time_sig, start_time_sig, units="secs"))
    res_list_5repeated3fold[[i]] <- data.frame(
      fold   = f,
      method = "Pairwise",
      tmp_sig$eval_metrics,
      time = time_sig, 
      num_selected_feats = length(sig_feats)
    )
    conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, tmp_sig$preds, 
                                        conf_mat_title = paste0("Pair-wise Significance-Based Features - ", type_col), 
                                        accuracy = tmp_sig$Accuracy); i <- i + 1
  }
  
  # Recursive Feature Addition
  cat("\n### Train/test with RFA features ###\n")
  start_time_rfa <- Sys.time()
  rfa_base  <- pw
  base_feats<- intersect(unique(rfa_base$corrected[[idx]]$Feature),
                         colnames(X_train))
  
  rfa_res   <- recursive_feature_addition(
    X_train, y_train,
    X_test,  y_test,
    base_features    = base_feats,
    cv_ctrl          = cv_ctrl,
    ntree_candidates = ntree_candidates,
    metric           = metric
  )
  end_time_rfa   <- Sys.time()
  time_rfa       <- as.numeric(difftime(end_time_rfa, start_time_rfa, units="secs"))
  
  res_list_5repeated3fold[[i]] <- data.frame(
    fold   = f,
    method = "RFA",
    rfa_res$eval_metrics,
    time=time_rfa, 
    num_selected_feats = length(rfa_res$best_features_rf)
  )
  conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, 
                                      rfa_res$predictions, 
                                      conf_mat_title = paste0("RFA Features - ", type_col), 
                                      accuracy = rfa_res$best_accuracy); i <- i + 1
  
  # RFE with hyperparams retuning
  cat("\n### Train/test with RFE features ###\n")
  start_time_rfe <- Sys.time()
  myFuncs <- rfFuncs
  myFuncs$fit <- function(x, y, first, last, ...) {
    res <- tune_rf_subset(
      X_train = x, y_train = y,
      X_test  = x, y_test  = y,
      cv_ctrl, ntree_candidates, metric
    )
    res$model$finalModel
  }
  rfe_ctl <- rfeControl(
    functions   = myFuncs,
    method      = "cv",
    number      = rfe_folds,
    saveDetails = TRUE,
    returnResamp= "final"
  )
  rfe_out <- rfe(
    x          = X_train,
    y          = y_train,
    sizes      = seq_len(ncol(X_train)),
    rfeControl = rfe_ctl
  )
  rfe_feats <- rfe_out$optVariables
  
  # final retune+test on RFE set
  tmp_rfe <- tune_rf_subset(
    X_train[, rfe_feats, drop=FALSE], y_train,
    X_test[, rfe_feats, drop=FALSE], y_test,
    cv_ctrl, ntree_candidates, metric
  )
  
  end_time_rfe   <- Sys.time()
  time_rfe       <- as.numeric(difftime(end_time_rfe, start_time_rfe, units="secs"))
  
  res_list_5repeated3fold[[i]] <- data.frame(
    fold   = f,
    method = "RFE",
    tmp_rfe$eval_metrics,
    time = time_rfe, 
    num_selected_feats = length(rfe_feats)
  )
  conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, 
                                      tmp_rfe$preds, 
                                      conf_mat_title = paste0("RFE Features - ", type_col), 
                                      accuracy = tmp_rfe$Accuracy); i <- i + 1
}

results_df <- do.call(rbind, res_list_5repeated3fold)

# Summarize and compare feature selection -----------------------
# get overall means + SDs
summary_table <- results_df %>%
  group_by(method) %>%
  summarise(
    accuracy_mean = round(mean(Accuracy), 2),
    accuracy_sd   = round(sd(Accuracy), 2),
    kappa_mean    = round(mean(Kappa,    na.rm=TRUE), 2),
    kappa_sd      = round(sd(Kappa,      na.rm=TRUE), 2),
    auc_mean      = round(mean(AUC), 2),
    auc_sd        = round(sd(AUC), 2),
    time_mean     = round(mean(time), 2),
    time_sd       = round(sd(time), 2),
    num_selected_feats_mean = mean(num_selected_feats),
    num_selected_feats_sd   = sd(num_selected_feats)
  )

View(summary_table)

# boxplot of accuracy by method
plot <- ggplot(results_df, aes(x=method, y=num_selected_feats)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_bw(base_size = 20) +
  labs(x = "Feature selection methods", y = "Number of selected features") #, title="Accuracy across folds by method")
plot

ggsave(filename = paste0("Figure S3_box_plot_Number of selected features across five feature selection methods_", "gasoline_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

# boxplot of AUC by method
ggplot(results_df, aes(x=method, y=AUC)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_minimal(base_size = 20) +
  labs(x = "Feature selection methods",
       title="AUC across folds by method")


# reshape to wide: one row per fold, columns = methods
wide_acc <- results_df %>%
  select(fold, method, AUC) %>%
  tidyr::pivot_wider(names_from = method, values_from = AUC)

stats::kruskal.test(list(wide_acc$All, wide_acc$Pairwise, wide_acc$RFA, wide_acc$RFE))

# Friedman test (nonparametric, repeated measures)
# friedman.test(as.matrix(wide_acc[,-1]))

## Confusion matrix
# Best fold for each feature selection ==> All fold, RFE fold 2; pair fold, RFA fold 1
```

## Figure 4: Boxplot of MCC across feature selection methods
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# boxplot of MCC by method
plot <- ggplot(results_df, aes(x=method, y=MCC_Multiclass)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  geom_signif(comparisons=list(c("RFA","All"),c("RFA","Pairwise"),c("RFA","RFE")),
              test=t.test, test.args = list(paired = F), 
              map_signif_level = function(p) sprintf("p = %.5f", p),  # ← show numeric p-values  # map_signif_level=TRUE, 
              step_increase=0.17, textsize = 8, vjust = -0.15) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw(base_size = 27) +
  labs(x = "Feature selection methods", y = "MCC score"
       # title="MCC across folds by method"
  ) +
  theme(axis.text.x = element_text(size = 22, color = "black", face = "bold"),
        axis.text.y = element_text(size = 22, color = "black"),
        axis.title.x = element_text(margin = ggplot2::margin(t = 15)),  # add space
        axis.title.y = element_text(margin = ggplot2::margin(r = 15)),  # add space
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
plot

ggsave(filename = paste0("Figure 4_box_plot_MCC score across feature selection methods_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 
```

## Figure 5: Confusion matrix

==> y_test_and_preds_list_5repeated3fold was created as a placeholder to get all gas station classes on Actual Class (y-axis) and Predicted Class (x-axis).

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Build confusion df from your y_test_and_preds_list element -----------------
make_cm_df <- function(obj, actual_slot, pred_slot) {
  y_true <- obj[[actual_slot]]
  y_pred <- obj[[pred_slot]]

  # standardize to character and trim whitespace
  y_true_chr <- str_squish(as.character(y_true))
  y_pred_chr <- str_squish(as.character(y_pred))

  # include *all* label combos (even zero-count) to be safe
  all_lvls <- sort(unique(c(y_true_chr, y_pred_chr)))
  grid <- expand.grid(Actual = all_lvls, Predicted = all_lvls, stringsAsFactors = FALSE)

  cm_counts <- as.data.frame(
    table(Actual = y_true_chr, Predicted = y_pred_chr),
    stringsAsFactors = FALSE
  )

  # join with full grid so missing pairs become 0
  cm_full <- grid %>%
    left_join(cm_counts, by = c("Actual","Predicted")) %>%
    mutate(Freq = dplyr::coalesce(Freq, 0L)) %>%
    arrange(Actual, Predicted)

  cm_full
}

# All feats - y_test_and_preds_list_5repeated3fold[[15]] --------------------
fold_id <- 15
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "all_feats_preds")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "All features",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_All feats_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")

# Pairwise -------------------------------------
fold_id <- 15
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "tmp_sig_preds")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "Pairwise",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_Pairwise_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")

# RFE----------------------------
fold_id <- 14
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "tmp_rfe_preds")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "RFE",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_RFE_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")

# RFA ------------------------------------
fold_id <- 15
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "rfa_res_predictions")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "RFA",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_RFA_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")
```

## Sensitivity Analysis  
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# nested the feature filtering as a part of the RF classifier
## gradient of 
```

# SI Table S5-S7: Find out identity of selected features
```{r , echo=FALSE, warning = FALSE, message=FALSE}
feature_df <- data.frame(Feature = unique(unlist(selected_features_5repeated3fold[seq(from = 4 , to = 60, by = 4)])))

rt1 <- c()
rt2 <- c()
Ion1 <- c()
Ion2 <- c()

for(r in 1:nrow(feature_df)) {
  feat <- as.numeric(feature_df[r, "Feature"])
  meta_row <- metadata[which(metadata$Feature == feat), ]
  if(nrow(meta_row) == 0) {
    rt1 <- c(rt1, NA)
    rt2 <- c(rt2, NA)
    Ion1 <- c(Ion1, NA)
    Ion2 <- c(Ion2, NA)
  } else {
    rt1 <- c(rt1, meta_row$RT1[1])
    rt2 <- c(rt2, meta_row$RT2[1])
    Ion1 <- c(Ion1, meta_row$Ion1[1])
    Ion2 <- c(Ion2, meta_row$Ion2[1])
  }
}

feature_df$RT1  <- rt1
feature_df$RT2  <- rt2
feature_df$Ion1 <- Ion1
feature_df$Ion2 <- Ion2

# Next, use the target_comp data frame to match compounds based on RT and ion values.
compound_name <- c()
chem_grp <- c()

for(r in 1:nrow(feature_df)) {
  idx <- which(abs(target_comp$RT1 - feature_df[r, "RT1"]) <= 0.1 &
                 abs(target_comp$RT2 - feature_df[r, "RT2"]) <= 0.1 &
                 abs(target_comp$Ion1 - feature_df[r, "Ion1"]) <= 0.5 &
                 abs(target_comp$Ion2 - feature_df[r, "Ion2"]) <= 0.5)
  if(length(idx) == 0) {
    chem_grp <- c(chem_grp, "unknown")
    compound_name <- c(compound_name, "unknown")
  } else {
    chem_grp <- c(chem_grp, target_comp$Group[idx[1]])
    compound_name <- c(compound_name, target_comp$Compound[idx[1]])
  }
}

feature_df$compound_name  <- compound_name
feature_df$Chemical_group <- chem_grp
View(feature_df)

write_xlsx(feature_df %>% 
             mutate(across(
               .cols = where(is.numeric),
               .fns  = ~ round(.x, 2)
             )), path = paste0("Table S5_RFE features_gas station classification",format(Sys.time(), "%d-%m-%Y"),".xlsx"))
```

## Figure S3: Comparison of computational time and number of selected features 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
## Computational time
plot <- ggplot(results_df, aes(x=method, y=time)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_bw(base_size = 20) +
  labs(x = "Feature selection methods", y = "Computational time")
plot

ggsave(filename = paste0("Figure S3_box_plot_Number of selected features across five feature selection methods_", "gasoline_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

## Number of selected features
plot <- ggplot(results_df, aes(x=method, y=num_selected_feats)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_bw(base_size = 20) +
  labs(x = "Feature selection methods", y = "Number of selected features")
plot

ggsave(filename = paste0("Figure S3_box_plot_Number of selected features across five feature selection methods_", "gasoline_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

```