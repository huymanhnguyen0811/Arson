---
title: "Arson Wildfire data analysis workflow"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Huy Nguyen/PhD_EnSciMan_Ryerson_University/Arson project/Rproject/data")
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation

This repo is accompanying the publication: "The Use of Computational Fingerprinting Techniques to Distinguish Sources of Accelerants Used in Wildfire Arson".

Users need to first install R with this [link](https://cran.r-project.org/mirrors.html) and Rstudio with this [link](https://posit.co/download/rstudio-desktop/).

This workflow ran on Windows 11 OS 11th Gen Intel(R) Core(TM) i7-11800H \@ 2.30GHz, 16 GB RAM;

THe RStudio version used in this demo is 2023.06.0+421 "Mountain Hydrangea" Release for Windows;

The R version used in this demo is 4.3.1

First, the following R packages are installed and loaded in the global environment along with in-house built functions to minimize repetitiveness in the code.

Details about these functions can be found in Data processing.R file in this repo.

## Loading packages
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(purrr)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)
library(stats)
library(FactoMineR)
library(factoextra)
library(ggforce)
library(caret)
library(xgboost)
library(lightgbm)
library(pROC)
library(dplyr)
library(missForest)      # for random forest imputation
library(VIM)             # for kNN imputation (kNN())
library(randomForest)
library(caret)           # for model training & cross-validation
library(stats)           # for basic stats tests, prcomp, etc.
library(pheatmap)
library(viridis)
library(ggsignif)
library(ggpubr)
# Notin function
`%notin%` <- Negate(`%in%`)
```


# STEP 1.1: Data import
```{r, echo = FALSE, message = FALSE, warning = FALSE}
target_comp <- read_xlsx(path = "Shortened ILR Compound List PF001A 07-06-2024.xlsx")
# ASTM <- read_xlsx(path = "ILR Compound List 05-15-2024_Without DieselASTM.xlsx")
file_path <- "Gasolines_BOP_110424.xlsx"

dfs <- excel_sheets(file_path) %>%
  set_names() %>%
  map(~ read_excel(file_path, sheet = .x) %>% mutate(Sample_name = .x))

df_step1.1 <- bind_rows(dfs) %>%
  dplyr::select(-c("RMF", "Area %")) %>%
  mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas_87", 
                                ifelse(str_detect(Sample_name, "B"), "Gas_89",
                                       ifelse(str_detect(Sample_name, "C"), "Gas_91", "Gas_94")))) %>%
  mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                  ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station_1",
                              ifelse(str_detect(Sample_name, "F002"), "Station_2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station_3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station_4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station_5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station_6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station_7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station_8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station_9", "Station_10")))))))))) 

# modify columns names
colnames(df_step1.1)[colnames(df_step1.1) == '<sup>1</sup>t<sub>R</sub>'] <- 'RT1'
colnames(df_step1.1)[colnames(df_step1.1) == '<sup>2</sup>t<sub>R</sub>'] <- 'RT2'
colnames(df_step1.1)[colnames(df_step1.1) == 'Major'] <- 'Ion1'
colnames(df_step1.1)[colnames(df_step1.1) == 'Qual'] <- 'Ion2'
```

# STEP 1.2: Filtering out column bleed, solvent and BTEX and MF = 0

```{r, echo = FALSE, message = FALSE, warning = FALSE}
filter_list <- c("^Carbon disulfide$", 
                "Cyclotrisiloxane..hexamethyl",
                "Cyclotetrasiloxane..octamethyl"
                # "^Benzene$",
                # "^Toluene$",
                # "^Ethylbenzene$",
                # "Xylene"
                )

# ^Carbon disulfide$ 75.890 - 75.959, 77.881 - 77.948

df_step1.2 <- copy(df_step1.1) # %>%
  # filter(MF > 0)

for (filter_comp in filter_list) {
  df_step1.2 <- df_step1.2 %>%
      filter(!grepl(filter_comp, Compound))
}

df_step2 <- df_step1.2 %>%
  filter(Area > 300000) %>%
  arrange(RT1, RT2)
```

#### Quality assurance: Histogram distribution of Peak values before data normalization
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
ggplot(data = df_step2) +
  geom_histogram(aes(x= Area)) +
  facet_wrap(~Sample_name) +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5) 
        # strip.text = element_blank()
        )
```

### Plot benchmark distribution for aligning compounds later
```{r, echo = FALSE, message = FALSE, warning = FALSE}
bm_df <- df_step1.1 %>% 
  filter(grepl("Toluene-D8", Compound)) %>%
  filter(RT2 >= 3 & RT2 <= 5)

# Examining Retention time fluctuation of Toluene benchmark 
# RT1 hist plot 
hist(as.numeric(bm_df$RT1), breaks = 10, 
     xlab = "RT1", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")

# RT2 hist plot 
hist(as.numeric(bm_df$RT2), 
     xlab = "RT2", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
     main = "Toluene")
```

# QUALITY CONTROL A OF STEP 1.2B: Plot Percentage coverage after removal of limit observation

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# df <- data.frame(list(thres = c(seq(from = 0, to = 300000, by = 50000)), cover = coverage_list))
# plot <- ggplot(data = df,
#        aes(x = thres, y = cover)) +
#   geom_col(fill = "skyblue") +
#   geom_text(aes(label = round(cover, digits = 3)), color = "black", angle = 90, hjust = 2, size = 10) +
#   scale_x_continuous(breaks = seq(from = 0, to = 300000, by = 50000),
#                      # remove space between plotted data and xy-axes
#                      expand = c(0,0)) +
#   scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
#                      # remove space between plotted data and xy-axes
#                      expand = c(0,0)) +
#   theme_classic(base_size = 20) +
#   theme(axis.text.x = element_text(size = 20),
#         axis.text.y = element_text(size = 20),
#         strip.text = element_text(size = 30, face = "bold"),   # Make facet label text bold
#         strip.background = element_blank()) +
#   labs(x = "Threshold of removal for limit observations", 
#        y = "Percentage coverage of remaining peaks after removal (%)") 
# 
# ggsave(filename = paste0("Percentage coverage_", format(Sys.time(), "%d-%m-%Y"),".png"), 
#        plot = plot, 
#        device = "png",
#        dpi = 600,        # Set DPI to 600
#        width = 13,       # Width in inches (adjust as needed)
#        height = 8,       # Height in inches (adjust as needed)
#        units = "in") 

# plot_a <- list()
# for (threshold in c(seq(from = 0, to = 300000, by = 50000))) {
# for (name in unique(df_step1.2$Sample_name)[50:70]) {
#   coverage <- c()
#   for (threshold in c(seq(from = 0, to = 300000, by = 50000))) {
#     temp <- df_step1.2 %>%
#       filter(Sample_name %in% name)
#     df_filter_area <- temp %>%
#       filter(Area > threshold)
#     coverage <- c(coverage, sum(df_filter_area$Area)*100/sum(temp$Area))
#   }
#   df <- data.frame(thres = seq(from = 0, to = 300000, by = 50000), cover = coverage)
#   plot_a[[i]] <- ggplot(data = df,
#                       aes(x = thres, y = cover)) +
#     geom_col() +
#     theme(text = element_text(size = 10)) +
#     geom_text(aes(label = round(cover, digits = 3)), color = "green", angle = 90, hjust = 1, size = 5) +
#     scale_x_continuous(breaks = seq(from = 0, to = 300000, by = 50000),
#                        # remove space between plotted data and xy-axes
#                        expand = c(0,0)) +
#     scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
#                        # remove space between plotted data and xy-axes
#                        expand = c(0,0)) +
#     ggtitle(name) +
#     labs(x = NULL, y = NULL) 
#   i <- i + 1
# }

# y <- textGrob("Percentage coverage of remaining peaks after removal", rot = 90, gp = gpar(fontsize = 20))
# x <- textGrob("Threshold of removal for limit observations", gp = gpar(fontsize = 20))
# 
# grid.arrange(grobs = plot_a, ncol = 5, 
#              left = y,
#              bottom = x)
```


# Figure 1 - QUALITY CONTROL OF STEP 1.2B: Plot number of peak remains after removal of limit observation with Percentage coverage after removal of limit observation (on top of each bar)

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Calculate the mean/median percentage of all sample in df_step1.2
coverage_list <- c()
for (threshold in c(seq(from = 0, to = 1000000, by = 50000))) {
  df_filter_area <- df_step1.2 %>%
    dplyr::filter(Area > threshold) %>%
    dplyr::group_by(Sample_name) %>%
    dplyr::summarise(across(Area, base::sum))
  
  coverage <- c()
  for (sample in df_filter_area$Sample_name) {
    coverage <- c(coverage, df_filter_area[which(df_filter_area$Sample_name == sample),]$Area*100/sum(df_step1.2[which(df_step1.2$Sample_name == sample),]$Area))
  }
  coverage_list <- c(coverage_list, mean(coverage))
}

num_comp_list <- c()

# Precompute sample counts for all thresholds using vectorized operations
for (threshold in seq(from = 0, to = 1000000, by = 50000)) {
  
  # Filter the data based on threshold
  df_filter_area <- df_step1.2 %>%
    dplyr::filter(Area > threshold) %>%
    dplyr::group_by(Sample_name) %>%
    dplyr::summarize(num_comp = n())   # Count occurrences per Sample_name
  
  # Compute the mean of num_comp and store it in num_comp_list
  num_comp_list <- c(num_comp_list, mean(df_filter_area$num_comp))
}

df <- data.frame(list(thres = c(seq(from = 0, to = 1000000, by = 50000)), remain = num_comp_list))
plot <- ggplot(data = df,
               aes(x = thres, y = remain)) +
  geom_col(fill = "skyblue") +
  geom_text(aes(label = round(remain, digits = 0)), color = "black", 
            angle = 90, hjust = 1.5, size = 9) +
  geom_text(aes(label = paste0(round(coverage_list, digits =1),"%"), 
                y = remain + 5),  # Adjust 'y = remain + 5' for positioning above bars
            color = "black", size = 9, vjust = 0) +     # Adjust vjust for fine-tuning
  scale_x_continuous(breaks = seq(from = 0, to = 1000000, by = 50000),
                     # remove space between plotted data and xy-axes
                     expand = c(0,0)) +
  # scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
  #                    # remove space between plotted data and xy-axes
  #                    expand = c(0,0)) +
  theme_classic(base_size = 25) +
  theme(axis.text.x = element_text(size = 25, angle = 90, vjust = 0.5),
        axis.text.y = element_text(size = 25),
        strip.text = element_text(size = 30, face = "bold"),   # Make facet label text bold
        strip.background = element_blank()) +
  labs(x = "Stepwise thresholds of peak area for removal of compounds with low signal to noise", 
       y = "Number of peak remains after applying thresholds of peak area") 
plot

ggsave(filename = paste0("Figure 1_Number of compound remain & Percentage coverage_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 25,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in")


# plot_b <- list()
# i <- 1
# for (name in unique(df_step1.2$Sample_name)[50:70]) {
#   peak_remain <- c()
#   for (threshold in c(seq(from = 0, to = 300000, by = 50000))) {
#     temp <- df_step1.2 %>%
#       filter(Sample_name %in% name)
#     df_filter_area <- temp %>%
#       filter(Area > threshold)
#     peak_remain <- c(peak_remain, dim(df_filter_area)[1])
#   }
#   df <- data.frame(thres = seq(from = 0, to = 300000, by = 50000), remain = peak_remain)
#   plot_b[[i]] <- ggplot(data = df,
#                       aes(x = thres, y = remain)) +
#     geom_col() +
#     geom_text(aes(label = remain), color = "green", vjust = 1.2, size = 5) +
#     scale_x_continuous(breaks = seq(from = 0, to = 300000, by = 50000), 
#                        # remove space between plotted data and xy-axes
#                        expand = c(0,0)) +
#     ggtitle(name) +
#     theme(axis.text.x = element_text(size = 20),
#           axis.text.y = element_text(size = 20)) +
#     labs(x = NULL, y = NULL) +
#     theme_classic()
#   i <- i + 1
# }

# y <- textGrob("Number of peak remains after removal of limit observation", rot = 90, gp = gpar(fontsize = 15))
# x <- textGrob("Threshold of removal for limit observations", gp = gpar(fontsize = 15))
# 
# grid.arrange(grobs = plot_b, ncol = 5, 
#              left = y,
#              bottom = x)
```



## QUALITY CONTROL TO SELECT ALIGNMENT WINDOW: Confirming targeted compounds at each step of Data processing

(Update 22nd May 2024:) Using the target compound list, we will review 18 samples (6 samples across 3 seasons (p,b,o)) to determine how many samples have the target compounds. This data will help determine the effectiveness of the compression - similar to your microplastic table. 

We want to make sure that it works for all potential types of fuels. 

For the reporting table, there will be cases where the target compounds might not occur in some samples. The best alignment window will be the one that have highest  **"Proportion"** = samples have the target compounds/ 71 (total number of samples in the dataset).

Then, I pick the combination of Rt1 and Rt2 that results in the highest cumulative sum of the **"Proportion"** from all target compound by using **rowSums**.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# For each target compounds,  Apply different alignment windows and record number of samples that have the =target compounds
combi <- tidyr::crossing(
  # RT1
  c(0.1 ,0.15, 0.2, 0.25, 0.3, 0.35, 0.4), 
  # RT2
  c(0.12,0.13,0.14,0.15, 0.16, 0.17, 0.18, 0.19, 0.2))

df <- data.frame(RT1_window=integer(), RT2_window=integer(), target=character(), proportion=integer())
  
for (i in 1:nrow(combi)) {
  for (j in 1:nrow(target_comp)) {
    # Catch all peaks in dataframe that falling into the window with target compounds as center of the window
    idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                    
                   (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                   df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
    
    temp <- df_step2[idx1,]
    minrt1 <- max(temp$RT1)
    maxrt1 <- min(temp$RT1)
    minrt2 <- max(temp$RT2)
    maxrt2 <- max(temp$RT2)
    
    
    idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + as.numeric(combi[i,][1])) & 
                    temp$RT1 >= (target_comp[j,]$RT1 - as.numeric(combi[i,][1])) &
                    temp$RT2 <= (target_comp[j,]$RT2 + as.numeric(combi[i,][2])) & 
                    temp$RT2 >= (target_comp[j,]$RT2 - as.numeric(combi[i,][2])))
    
    df[nrow(df) + 1,] <- c(as.numeric(combi[i,][1]),
                           as.numeric(combi[i,][2]),
                           target_comp[j,]$Compound,
                           # paste0(length(unique(temp[idx2,]$Sample_name)), "/", 71))
                           100*(length(unique(temp[idx2,]$Sample_name)) / length(unique(temp$Sample_name))))
  }
}

summary_df <- df %>% pivot_wider(names_from = target, values_from = proportion)

# Which window have the highest proportion of samples that have the target compounds
summary_df[, 3:ncol(summary_df)] <- lapply(summary_df[, 3:ncol(summary_df)], as.numeric)
max_row <- summary_df[which.max(rowSums(summary_df[, 3:ncol(summary_df)])), ]
print(max_row)

# Adding the number of samples where the compounds were found with matching Ion1 and Ion2 to colnames of each compound
i <- 3
for (j in 1:nrow(target_comp)) {
  idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                  
                  (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                     df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
  
  temp <- df_step2[idx1,]
  idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + as.numeric(max_row$RT1_window)) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - as.numeric(max_row$RT1_window)) &
                  temp$RT2 <= (target_comp[j,]$RT2 + as.numeric(max_row$RT2_window)) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - as.numeric(max_row$RT2_window)))
  
  colnames(summary_df)[i] <- paste0(target_comp[j,]$Compound, " (n = ", length(unique(temp$Sample_name)), ")")
  i <- i + 1 
}

writexl::write_xlsx(summary_df, path = paste0(getwd(), "/Testing combinations of Retention time window for compound alignment.xlsx"))
```


# QUALITY CONTROL: Examine the distribution of RT of target compounds after alignment with pF001A

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Get the column names from the existing data frame
column_names <- c( "Target compound", "RT", "Retention time of pF001A", "Min.", "1st Qu.", "Median",  "Mean", "3rd Qu.", "Max.", "Max. RT - Min. RT")
# Create an empty data frame with the same column names
summary_df <- data.frame(matrix(ncol = length(column_names), nrow = 0))
colnames(summary_df) <- column_names

summary_target_compounds <- list()
for (j in 1:nrow(target_comp)) {
    # First for each target compound, matching Major and minor ion 
    idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.2) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.2) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.2) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.2)) |
                    
                   (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.2) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.2) &
                   df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.2) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.2)))
    
    temp <- df_step2[idx1,]
    
    # Then, for each target compound, matching within RT +- 0.1 wrt the pF001A retention time. 
    idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + 0.1) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - 0.1) &
                  temp$RT2 <= (target_comp[j,]$RT2 + 0.1) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - 0.1))
    
    
    
    if (nrow(temp[idx2, ]) == 0) {
      summary_df[nrow(summary_df) + 1,] <- c(paste0(target_comp[j,]$Compound, " was not found with matching ions and within RT1/RT2 windows of 0.1 of pF001A"), NA, NA, NA, NA, NA, NA, NA, NA, NA)
    } else {
      
      # Make descriptive stats summary of RT1 of all peaks that was aligned to target compound in pF001A
      summary_df[nrow(summary_df) + 1,] <- c(target_comp[j,]$Compound,
                                             "RT1",
                                             as.numeric(target_comp[j,]$RT1),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[1]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[2]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[3]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[4]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[5]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[6]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT1))[[6]] - summary(as.numeric(temp[idx2,]$RT1))[[1]]))
      
      # Make descriptive stats summary of RT2 of all peaks that was aligned to target compound in pF001A
      summary_df[nrow(summary_df) + 1,] <- c(target_comp[j,]$Compound,
                                             "RT2",
                                             as.numeric(target_comp[j,]$RT2),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[1]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[2]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[3]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[4]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[5]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[6]]),
                                             as.numeric(summary(as.numeric(temp[idx2,]$RT2))[[6]] - summary(as.numeric(temp[idx2,]$RT2))[[1]]))
      # Make histogram of distribution of RT of these target compound
      # par(mfrow = c(1, 2))
      # hist(as.numeric(temp[idx2,]$RT1),
      #      xlab = "RT1", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 1,
      #      main = paste0("RT1 of ", target_comp[j,]$Compound, "\n with pF001A's RT1 = ", target_comp[j,]$RT1, " and RT1 window = 0.1"))
      # 
      # hist(as.numeric(temp[idx2,]$RT2), 
      #      xlab = "RT2", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 1,
      #      main = paste0("RT2 of ", target_comp[j,]$Compound, "\n in pF001A's RT2 = ", target_comp[j,]$RT2, " and RT2 window = 0.1"))
      
      # Reset the layout to default
      # par(mfrow = c(1, 1))
    }
}

View(summary_df)

writexl::write_xlsx(summary_df, path = paste0(getwd(), "/summary of target compounds matching by major and minor ions and rt1_rt2 of (0.1)_", format(Sys.Date(), "%d-%m-%y"), ".xlsx"))
```


# Make a table for the compounds that was not found in all samples, give the names of the samples where these target compounds are missing
***If the compound can be found in all samples then plot it with x-axis as 71 samples sorted from gas_station 1 to 10, the legend is the sampling season***

(Update 31 May 2024): For the bar plot of target compounds, if the compounds were not presented in a station -> it will have zeros values instead of NA -> it is show in the plot that the compound was not there in certain gas stations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary_df2 <- data.frame(target=character(), samples_that_missing_target_compound=character())

df_step2 <- df_step2 %>% 
  mutate(gas_station = factor(gas_station, levels = unique(df_step2$gas_station)))

for (j in 1:nrow(target_comp)) {
  idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                  
                  (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                     df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
  
  temp <- df_step2[idx1,]
  idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + as.numeric(max_row$RT1_window)) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - as.numeric(max_row$RT1_window)) &
                  temp$RT2 <= (target_comp[j,]$RT2 + as.numeric(max_row$RT2_window)) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - as.numeric(max_row$RT2_window)))
  
  if (length(unique(temp[idx2,]$Sample_name)) < 71) {
    summary_df2[nrow(summary_df2) + 1,] <- c(target_comp[j,]$Compound,
                                             paste(setdiff(unique(df_step2$Sample_name), 
                                                           unique(temp[idx2,]$Sample_name)), 
                                                   collapse = ", "))
    
    # Artificial add in the missing samples and put the Area of those sample to be 0
    empty_df <- temp[idx2,] %>% select(Area, Sample_name, Octane_rating, sampling_season, gas_station) %>% slice(0)
    new_rows <- data.frame(
      Area = rep(0, length(setdiff(unique(df_step2$Sample_name), 
                                   unique(temp[idx2,]$Sample_name)))),
      Sample_name = setdiff(unique(df_step2$Sample_name), 
                     unique(temp[idx2,]$Sample_name)),
      Octane_rating = rep(NA, length(setdiff(unique(df_step2$Sample_name), 
                                            unique(temp[idx2,]$Sample_name)))),
      sampling_season = rep(NA, length(setdiff(unique(df_step2$Sample_name), 
                                              unique(temp[idx2,]$Sample_name)))),
      gas_station = rep(NA, length(setdiff(unique(df_step2$Sample_name), 
                                          unique(temp[idx2,]$Sample_name))))
    )
    temp2 <- bind_rows(empty_df, new_rows) %>%
      mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas_87", 
                                    ifelse(str_detect(Sample_name, "B"), "Gas_89",
                                           ifelse(str_detect(Sample_name, "C"), "Gas_91", "Gas_94")))) %>%
      mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                      ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
      mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station_1",
                                  ifelse(str_detect(Sample_name, "F002"), "Station_2",
                                         ifelse(str_detect(Sample_name, "F003"), "Station_3",
                                                ifelse(str_detect(Sample_name, "F004"), "Station_4",
                                                       ifelse(str_detect(Sample_name, "F005"), "Station_5",
                                                              ifelse(str_detect(Sample_name, "F006"), "Station_6",
                                                                     ifelse(str_detect(Sample_name, "F007"), "Station_7",
                                                                            ifelse(str_detect(Sample_name, "F008"), "Station_8",
                                                                                   ifelse(str_detect(Sample_name, "F009"), "Station_9", "Station_10")))))))))) %>% 
      mutate(new_sample_name = paste0(Sample_name, "_", gas_station, "_", Octane_rating))
    
    plotdata <- bind_rows(temp2, temp[idx2,] %>% 
                            select(Area, Sample_name, Octane_rating, sampling_season, gas_station) %>%
                            mutate(new_sample_name = paste0(Sample_name, "_", gas_station, "_", Octane_rating)))
    
    print(ggplot(data = plotdata, aes(x = new_sample_name , y = Area, fill = sampling_season)) + 
            geom_bar(stat = "identity") +
            labs(title = target_comp[j,]$Compound, 
                 x = "Samples", y = "Peak Area",
                 fill = "") +
            theme_minimal(base_size = 15) +
            theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
                  plot.title = element_text(size = 25),
                  axis.ticks.length.x = unit(0, "cm"),
                  panel.grid.minor.x = element_blank(),
                  panel.grid.minor.y = element_blank()) + 
            scale_y_continuous(expand = c(0,0)))
  } else {
    temp2 <- temp[idx2,] %>%
      mutate(new_sample_name = paste0(Sample_name, "_", gas_station, "_", Octane_rating))
    print(ggplot(data = temp2, aes(x = new_sample_name , y = Area, fill = sampling_season)) + 
            geom_bar(stat = "identity") +
            labs(title = target_comp[j,]$Compound, 
                 x = "Samples", y = "Peak Area",
                 fill = "") +
            theme_minimal(base_size = 15) +
            theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
                  plot.title = element_text(size = 25),
                  axis.ticks.length.x = unit(0, "cm"),
                  panel.grid.minor.x = element_blank(),
                  panel.grid.minor.y = element_blank()) + 
            scale_y_continuous(expand = c(0,0)))
  }
}

writexl::write_xlsx(summary_df2, path = paste0(getwd(), "/Samples where target compounds are missing.xlsx"))
```

# STEP 2A: Grouping compounds based on RT1, RT2, Ion1, Ion2

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Define tolerances
tolerances <- list(RT1 = 0.1, RT2 = 0.1, Ion1 = 0.5, Ion2 = 0.5)

# Use pF001A as a base
df_all <- df_step2 %>% 
  filter(Sample_name %in% "pF001A") %>% 
  filter(`Signal to Noise` > 10)

df_all$Feature <- 1:nrow(df_all)

# Loop through the samples apart from pF001A
for (sample in setdiff(unique(df_step2$Sample_name), c("pF001A", "bF001A", "bF007B"))) {
  # print(sample)
  df <- df_step2 %>% 
    filter(Sample_name %in% sample) %>% 
    filter(`Signal to Noise` > 10)
  df$Feature <- NA
  
  # Go through each row
  for (i in 1:nrow(df)) {
    row <- df[i, ]
    mask <- (
      abs(df_all$RT1 - row$RT1) <= tolerances$RT1 &
      abs(df_all$RT2 - row$RT2) <= tolerances$RT2 & 
      abs(df_all$Ion1 - row$Ion1) <= tolerances$Ion1 &
      abs(df_all$Ion2 - row$Ion2) <= tolerances$Ion2
    )
    
    idx <- which(mask)
    # If there is a match between a peak and the existing peak list, then assign the same Feature number to that peak 
    if (any(mask)) {
      row$Feature <- unique(df_all[idx, ]$Feature)[1]
    } else { # If not a match, then create new identity for the new Feature 
      row$Feature <- max(df_all$Feature) + 1
    }
    
    # adding the peak that have matchs in df_all
    df_all <- bind_rows(df_all, row)
  }
}

df_all <- df_all %>%
  # remove all peak with RT1 > 36 (which does not belong Gasoline)
  filter(RT1 < 30)

# Create metadata for next data analysis
metadata <- df_all %>%
  dplyr::select(
    Feature,
    Sample_name,
    Area) %>%
  group_by(Feature,
           Sample_name) %>%
  dplyr::summarise(across(Area, base::mean)) %>% # If feature appear in 
  tidyr::pivot_wider(names_from = Sample_name,
                     values_from = Area) 

#### Adding avg RT and Ions to the master df for Gwen
mean_rt1 <- c()
mean_rt2 <- c()
mean_ion1 <- c()
mean_ion2 <- c()

for (i in 1:nrow(metadata)) {
  mean_rt1 <- c(mean_rt1, mean(df_all[which(df_all$Feature %in% metadata[i, ]$Feature),]$RT1))
  mean_rt2 <- c(mean_rt2, mean(df_all[which(df_all$Feature  %in% metadata[i, ]$Feature),]$RT2))
  mean_ion1 <- c(mean_ion1, max(df_all[which(df_all$Feature  %in% metadata[i, ]$Feature),]$Ion1)) 
  mean_ion2 <- c(mean_ion2, max(df_all[which(df_all$Feature  %in% metadata[i, ]$Feature),]$Ion2))
}

metadata$RT1 <- mean_rt1
metadata$RT2 <- mean_rt2
metadata$Ion1 <- mean_ion1
metadata$Ion2 <- mean_ion2

metadata <- metadata %>% relocate(RT1, RT2, Ion1, Ion2, .after = 1) # Ion2

#### Adding Chemical groups of  to the master df for Gwen
# chemical_group <- c()
# 
# for (i in 1:nrow(metadata)) {
#   idx <- which(abs(metadata[i,]$Ion1 -target_comp$Ion1) <=  0.5 & 
#                  # abs(metadata[i,]$Ion2 -target_comp$Ion2) <=  0.5 &
#                  abs(metadata[i,]$RT1 -target_comp$RT1) <=  0.1   &
#                  abs(metadata[i,]$RT2 -target_comp$RT2) <=  0.1
#   )
#   if (identical(idx, integer(0))) {
#     chemical_group <- c(chemical_group, "unknown")
#     next
#   } else {
#     chemical_group <- c(chemical_group, unique(target_comp[idx,]$Group))
#   }
# }
# 
# metadata$Chemical_group <- chemical_group

core_metadata <- metadata %>% 
  # relocate(Chemical_group, .after = 1) %>% 
  arrange(RT1, RT2)

# Export masterlist and metadata for Gwen
# writexl::write_xlsx(df_all, path = paste0("MasterList_", format(Sys.Date(), format = "%d-%b-%Y"), ".xlsx"))
# writexl::write_xlsx(metadata_export, path = paste0("PeakTable_", format(Sys.Date(), format = "%d-%b-%Y"), ".xlsx"))
```

# STEP 2B: Data compression of compounds that share Ion1 (m/z ranges) and RT1/RT2
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Ion signature of groups
mz_alkane <- c(43,57,71,85,99)
mz_cycloalkane <- c(41, 55, 69, 83, 97)
mz_aromatics <- c(91, 105, 106, 119, 120, 134)
mz_indane <- c(117,118, 131, 132, 145, 146)
mz_threemusketeer <- c(91,106)
mz_castlegroup <- c(91, 105, 117, 120)
mz_gangoffour <- c(91, 105, 115, 119, 134)
mz_tetris <- c(119,134)
mz_2_1 <- c(119,134)
mz_alkylnaphthalene <- c(128, 142, 156, 170)

new_meta_data <- copy(metadata)
new_meta_data$Ion1 <- round(new_meta_data$Ion1)
new_meta_data$Ion2 <- round(new_meta_data$Ion2)
new_meta_data$new_feature_name_rt <- NA
new_meta_data$new_feature_name_mz_layer1 <- NA
new_meta_data$new_feature_name_mz_layer2 <- NA
new_meta_data$Chemical_group <-  NA
new_meta_data <- new_meta_data %>% relocate(new_feature_name_rt, new_feature_name_mz_layer1,
                                            new_feature_name_mz_layer2, Chemical_group, .before = 1)
```

### Layer 1: Check if the next row has is within +- 0.05 of RT1 and RT2 and has m/z fall in either one of the Ion signature of groups => name them as same compound 
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
i <- 1
for (row in 1:nrow(new_meta_data)) {
  current_row_rt1 <- new_meta_data[row,]$RT1
  current_row_rt2 <- new_meta_data[row,]$RT2
  # Layer 1: Find all RT1, RT2 that is in the same range +-0.05
  idx1 <- which(abs(current_row_rt1 - new_meta_data$RT1) < 0.05 & 
                  abs(current_row_rt2 - new_meta_data$RT2) < 0.05 & 
                  is.na(new_meta_data$new_feature_name_rt))
  if (length(idx1) > 1) {
    new_meta_data[idx1, ]$new_feature_name_rt <- paste0("Feature_rt_", i)
    i <- i + 1
  } else {
    next
  }
}
```

### Layer 2: Check for similar ion1 and ion2, and switching of Ion1 and Ion2
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
i <- 1
for (feature in unique(new_meta_data[which(!is.na(new_meta_data$new_feature_name_rt)),]$new_feature_name_rt)) {
  mz_major <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion1
  mz_minor <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion2
  idx2 <- which(new_meta_data$new_feature_name_rt == feature)
  temp <- new_meta_data[idx2,]
  # Initialize a logical vector to store selection conditions
  selected <- rep(FALSE, nrow(temp))
  # Iterate through the rows and check for switching or perfect matching
  for (row in 1:(nrow(temp) - 1)) {
    if ((temp$Ion1[row] == temp$Ion2[row + 1] && temp$Ion2[row] == temp$Ion1[row + 1]) || 
        (temp$Ion1[row] == temp$Ion1[row + 1] && temp$Ion2[row] == temp$Ion2[row + 1])) {
      selected[row] <- TRUE
      selected[row + 1] <- TRUE
    }
  }
  new_idx2 <- idx2[selected]
  new_meta_data[new_idx2, ]$new_feature_name_mz_layer1 <- paste0("Feature_mz_layer1_", i)
  i <- i + 1
}
```

### Layer 3: Check if the ions fall into ions of signature groups
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
i <- 1
for (feature in unique(new_meta_data[which(!is.na(new_meta_data$new_feature_name_rt)),]$new_feature_name_rt)) {
  mz_major <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion1
  mz_minor <- new_meta_data[which(new_meta_data$new_feature_name_rt == feature),]$Ion2
  idx2 <- which(new_meta_data$new_feature_name_rt == feature)
  temp <- new_meta_data[idx2,]
  if (any(temp$Ion1 %in% mz_alkane & temp$Ion2 %in% mz_alkane)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_alkane & temp$Ion2 %in% mz_alkane]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Alkane"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_cycloalkane & temp$Ion2 %in% mz_cycloalkane)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_cycloalkane & temp$Ion2 %in% mz_cycloalkane]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Cycloalkane"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_aromatics & temp$Ion2 %in% mz_aromatics)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_aromatics & temp$Ion2 %in% mz_aromatics]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Aromatics"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_indane & temp$Ion2 %in% mz_indane)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_indane & temp$Ion2 %in% mz_indane]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Indane"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_threemusketeer & temp$Ion2 %in% mz_threemusketeer)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_threemusketeer & temp$Ion2 %in% mz_threemusketeer]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Three Musketeers"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_castlegroup & temp$Ion2 %in% mz_castlegroup)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_castlegroup & temp$Ion2 %in% mz_castlegroup]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Castle Group"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_tetris & temp$Ion2 %in% mz_tetris)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_tetris & temp$Ion2 %in% mz_tetris]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Tetris"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_2_1 & temp$Ion2 %in% mz_2_1)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_2_1 & temp$Ion2 %in% mz_2_1]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "2-1"
    i <- i + 1
  } else if (any(temp$Ion1 %in% mz_alkylnaphthalene & temp$Ion2 %in% mz_alkylnaphthalene)) {
    new_idx2 <- idx2[temp$Ion1 %in% mz_alkylnaphthalene & temp$Ion2 %in% mz_alkylnaphthalene]
    new_meta_data[new_idx2, ]$new_feature_name_mz_layer2 <- paste0("Feature_mz_layer2_", i)
    new_meta_data[new_idx2, ]$Chemical_group <- "Alkylnaphthalene"
    i <- i + 1
  } else {
    next
  }
}
```

### Data compression: Sum up values for each coeluting compounds --------------------------------------------------
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Case 1: have NA of both layer1 and layer2
case1 <- copy(new_meta_data %>% 
                dplyr::filter(!is.na(new_feature_name_rt) &
                                is.na(new_feature_name_mz_layer1) & 
                                is.na(new_feature_name_mz_layer2)))

# Case 2: Does Not have NA of layer1 but has NA of layer2
case2 <- copy(new_meta_data %>% 
                filter(!is.na(new_feature_name_mz_layer1)) %>% 
                filter(is.na(new_feature_name_mz_layer2))) 
case2_list <- list()
i <- 1
for (feature in unique(case2$new_feature_name_mz_layer1)) {
  idx2 <- which(case2$new_feature_name_mz_layer1 == feature) # Get idx coeluting features
  tempdf <- case2[idx2, ] # extract temp df of coeluting features
  # Take the sum for each column of samples
  case2_list[[i]] <- data.frame(
    new_feature_name_rt = paste0(unique(tempdf$new_feature_name_rt), collapse = "_"),
    new_feature_name_mz_layer1 = paste0(unique(tempdf$new_feature_name_mz_layer1), collapse = "_"),
    new_feature_name_mz_layer2 = paste0(unique(tempdf$new_feature_name_mz_layer2), collapse = "_"),
    Feature = t(max(tempdf$Feature[sapply(tempdf$Feature, is.numeric)])),
    Chemical_group = paste0(unique(tempdf$Chemical_group), collapse = "_"),
    t(colMeans(tempdf[, c(6,7)][sapply(tempdf[, c(6,7)], is.numeric)])), # Average of RT1, RT2
    Ion1 = paste0(unique(tempdf$Ion1), collapse = "_"), # create new Ion1 values that combine all unique Ion1 values of the coeluting compounds
    Ion2 = paste0(unique(tempdf$Ion2), collapse = "_"), # create new Ion2 values that combine all unique Ion2 values of the coeluting compounds
    t(sapply(tempdf[, 10:ncol(tempdf)], function(column) { 
      if (all(is.na(column))) {
        return(NA)
      } else {
        return(sum(column, na.rm = TRUE))
      }
    }))
  )
  i <- i + 1
}

# Case 3: Have NA of layer1 but does not have NA of layers
case3 <- copy(new_meta_data %>% 
                filter(is.na(new_feature_name_mz_layer1)) %>% 
                filter(!is.na(new_feature_name_mz_layer2))) 

case3_list <- list()
i <- 1
for (feature in unique(case3$new_feature_name_mz_layer2)) {
  idx2 <- which(case3$new_feature_name_mz_layer2 == feature) # Get idx coeluting features
  tempdf <- case3[idx2,] # extract temp df of coeluting features
  # Take the sum for each column of samples
  case3_list[[i]] <- data.frame(
    new_feature_name_rt = paste0(unique(tempdf$new_feature_name_rt), collapse = "_"),
    new_feature_name_mz_layer1 = paste0(unique(tempdf$new_feature_name_mz_layer1), collapse = "_"),
    new_feature_name_mz_layer2 = paste0(unique(tempdf$new_feature_name_mz_layer2), collapse = "_"),
    Feature = t(max(tempdf$Feature[sapply(tempdf$Feature, is.numeric)])),
    Chemical_group = paste0(unique(tempdf$Chemical_group), collapse = "_"),
    t(colMeans(tempdf[, c(6,7)][sapply(tempdf[, c(6,7)], is.numeric)])), # Average of RT1, RT2
    Ion1 = paste0(unique(tempdf$Ion1), collapse = "_"), # create new Ion1 values that combine all unique Ion1 values of the coeluting compounds
    Ion2 = paste0(unique(tempdf$Ion2), collapse = "_"), # create new Ion2 values that combine all unique Ion2 values of the coeluting compounds
    t(sapply(tempdf[, 10:ncol(tempdf)], function(column) { 
      if (all(is.na(column))) {
        return(NA)
      } else {
        return(sum(column, na.rm = TRUE))
      }
    }))
  )
  i <- i + 1
}

# Case 4:  Appear in both layer1 and layer2
not_case4_features <- c(unique(case1$Feature), 
                    unique(case2$Feature),
                    unique(case3$Feature))
case4 <- new_meta_data %>% dplyr::filter(Feature %notin% c(not_case4_features) & 
                                           !is.na(new_feature_name_mz_layer2))
case4_list <- list()
i <- 1
for (feature in unique(case4$new_feature_name_mz_layer2)) { # Can either iterate through layer 1 or 2, doesn't matter which
  idx2 <- which(case4$new_feature_name_mz_layer2 == feature) # Get idx coeluting features
  tempdf <- case4[idx2,] # extract temp df of coeluting features
  # Take the sum for each column of samples
  case4_list[[i]] <- data.frame(
    new_feature_name_rt = paste0(unique(tempdf$new_feature_name_rt), collapse = "_"),
    new_feature_name_mz_layer1 = paste0(unique(tempdf$new_feature_name_mz_layer1), collapse = "_"),
    new_feature_name_mz_layer2 = paste0(unique(tempdf$new_feature_name_mz_layer2), collapse = "_"),
    Feature = t(max(tempdf$Feature[sapply(tempdf$Feature, is.numeric)])),
    Chemical_group = paste0(unique(tempdf$Chemical_group), collapse = "_"),
    t(colMeans(tempdf[, c(6,7)][sapply(tempdf[, c(6,7)], is.numeric)])), # Average of RT1, RT2
    Ion1 = paste0(unique(tempdf$Ion1), collapse = "_"), # create new Ion1 values that combine all unique Ion1 values of the coeluting compounds
    Ion2 = paste0(unique(tempdf$Ion2), collapse = "_"), # create new Ion2 values that combine all unique Ion2 values of the coeluting compounds
    t(sapply(tempdf[, 10:ncol(tempdf)], function(column) { 
      if (all(is.na(column))) {
        return(NA)
      } else {
        return(sum(column, na.rm = TRUE))
      }
    }))
  )
  i <- i + 1
}


compressed_data <- rbind(new_meta_data %>% 
                         dplyr::filter(is.na(new_feature_name_rt) &
                                         is.na(new_feature_name_mz_layer1) & 
                                         is.na(new_feature_name_mz_layer2)) %>%
                         mutate(Ion1 = as.character(Ion1)) %>%
                         mutate(Ion2 = as.character(Ion2)), 
                       case1 %>%
                         mutate(Ion1 = as.character(Ion1)) %>%
                         mutate(Ion2 = as.character(Ion2)),
                       dplyr::bind_rows(case2_list), 
                       dplyr::bind_rows(case3_list), 
                       dplyr::bind_rows(case4_list)
                       ) # %>%
  # dplyr::mutate(Feature = paste0("RT1_", RT1, "_",
  #                                "RT2_", RT2, "_",
  #                                "Ion1_", Ion1, "_",
  #                                "Ion2_", Ion2))
# View(final_cleandf)

coredf <- as.data.frame(t(compressed_data[, -c(1,2,3,4,6,7,8,9)] %>% # 1,2,3,5,6,7,8,9
                            column_to_rownames(., var = "Feature"))) %>% 
    rownames_to_column(., var="Sample_name") %>% 
    mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas 87", 
                                  ifelse(str_detect(Sample_name, "B"), "Gas 89",
                                         ifelse(str_detect(Sample_name, "C"), "Gas 91", "Gas 94")))) %>%
    mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
                                    ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
    mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
                                ifelse(str_detect(Sample_name, "F002"), "Station 2",
                                       ifelse(str_detect(Sample_name, "F003"), "Station 3",
                                              ifelse(str_detect(Sample_name, "F004"), "Station 4",
                                                     ifelse(str_detect(Sample_name, "F005"), "Station 5",
                                                            ifelse(str_detect(Sample_name, "F006"), "Station 6",
                                                                   ifelse(str_detect(Sample_name, "F007"), "Station 7",
                                                                          ifelse(str_detect(Sample_name, "F008"), "Station 8",
                                                                                 ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>% 
    # Add info on Suppliers
    mutate(supplier = ifelse(str_detect(Sample_name, "F001"), "Miscellanous",
                             ifelse(str_detect(Sample_name, "F002"), "Miscellanous",
                                    ifelse(str_detect(Sample_name, "F003"), "Shell",
                                           ifelse(str_detect(Sample_name, "F004"), "Cenovus",
                                                  ifelse(str_detect(Sample_name, "F005"), "Miscellanous",
                                                         ifelse(str_detect(Sample_name, "F006"), "Suncor",
                                                                ifelse(str_detect(Sample_name, "F007"), "Imperial",
                                                                       ifelse(str_detect(Sample_name, "F008"), "Burnaby",
                                                                              ifelse(str_detect(Sample_name, "F009"), "Imperial", "unknown")))))))))) %>% 
    relocate(Octane_rating, sampling_season, gas_station, supplier, .after = 1) %>% 
  mutate(supplier = ifelse(supplier %in% c("Miscellanous", "unknown"), "Unbranded", supplier))
```

# Violin plot before and after data compression
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Function to overlay violin plots
overlay_violin_plots <- function(df1, df2) {
  # Check if the dataframes have the same columns
  if (!identical(names(df1), names(df2))) {
    stop("The two dataframes do not have the same columns.")
  }
  
  # Loop through each column
  for (col_name in names(df1)) {
    # Ensure the column is numeric
    if (!is.numeric(df1[[col_name]]) || !is.numeric(df2[[col_name]])) next
    
    # Combine the two columns into a single dataframe for plotting
    plot_data <- data.frame(
      value = c(df1[[col_name]], df2[[col_name]]),
      group = rep(c("df1", "df2"), each = nrow(df1))
    )
    
    # Create the violin plot
    p <- ggplot(plot_data, aes(x = group, y = value, fill = group)) +
      geom_violin(alpha = 0.4, na.rm = TRUE) + # Use alpha for light hue
      scale_fill_manual(values = c("skyblue", "red")) +
      labs(title = col_name, x = "Group", y = "Value") +
      theme_minimal() +
      theme(plot.title = element_text(hjust = 0.5))
    
    # Print the plot
    print(p)
  }
}

# Before data compression
grid.arrange(grobs = create_violin_plots(metadata[, -c(1,2,3,4,5)]), ncol = 10, nrow = 7)
grid.arrange(grobs = create_violin_plots(final_cleandf[, -c(1,2,3,4,5)]), ncol = 10, nrow = 7)
```


# STEP 3: Feature elimination/ Feature prioritization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- coredf

# Step 1: Remove features with values in only one sample (row) ---------
df_filtered_step1 <- df[, colSums(!is.na(df) & df != 0) > 1]

# Step 2: Remove features with values in only one gas_station label --------

# Identify feature columns (excluding the "gas_station" label)
feature_columns <- setdiff(names(df_filtered_step1), "gas_station")

# Logical vector indicating which features to keep
keep_features <- sapply(df_filtered_step1[, feature_columns], function(feature) {
  unique_labels_with_values <- unique(df_filtered_step1$gas_station[!is.na(feature) & feature != 0])
  length(unique_labels_with_values) > 1
})

# Subset the dataframe to retain the desired features and "gas_station"
df_filtered_step2 <- df_filtered_step1[, c(names(df_filtered_step1)[keep_features])]

# Step 3: Remove features with less than 3 data points per gas_station label  ---------
selected_col <- integer()
gas_station_combinations <- t(utils::combn(unique(df_filtered_step2$gas_station), 2))
# Loop through the feature columns
for (feature_col in 6:ncol(df_filtered_step2)) {      
  feature_data <- df_filtered_step2[[feature_col]]  # Extract column once for efficiency

  # Loop through the precomputed combinations of gas_station
  for (combo in seq_len(nrow(gas_station_combinations))) {
    p_1 <- gas_station_combinations[combo, 1]
    p_2 <- gas_station_combinations[combo, 2]

    vec1 <- as.numeric(feature_data[df_filtered_step2$gas_station == p_1])
    vec2 <- as.numeric(feature_data[df_filtered_step2$gas_station == p_2])
    
    if (sum(!is.na(vec1)) >= 3 & sum(!is.na(vec2)) >= 3) {
      selected_col <- union(selected_col, feature_col)  # Use union for deduplication
      break  # Break inner loop early since column is already selected
    }
  }
}

df_filtered_step3 <- cbind(df_filtered_step2[, c(1:5)], df_filtered_step2[, selected_col])

#-----------------------------#
# STEP 4: Identify & remove features that are >90% missing overall
#-----------------------------#
colid_90_missingvalues <- c()

for (i in 6:ncol(df_filtered_step3)) {
  if (length(df_filtered_step3[which(is.na(df_filtered_step3[, i])), i]) / nrow(df_filtered_step3) > 0.7) {
    colid_90_missingvalues <- c(colid_90_missingvalues, i)
  }
}

if (!is.null(colid_90_missingvalues)) {
  df_filtered_step4 <- df_filtered_step3[, -colid_90_missingvalues]
}
```


# STEP 4: Assessing Imputation and Normalization methods
```{r, echo = FALSE, message = FALSE, warning = FALSE}
##############################
## 1. New Imputation Methods ##
##############################

## Option 1: Replace missing values with 0.001 
impute_0.001 <- function(df) {
  # Work on a copy of the data so that metadata (columns 1:5) remain intact.
  df_0.001 <- copy(df)
  for (r in 1:nrow(df_0.001)) {
  df_0.001[r, which(base::is.na(df_0.001[r,]))] <- 0.001
  }
  return(df_0.001)
}

## 1.3 Mean Imputation
mean_imputation <- function(df){
  df_numeric <- df
  for(i in seq_along(df_numeric)){
    col_data <- df_numeric[[i]]
    # Replace NA with mean
    col_data[is.na(col_data)] <- mean(col_data, na.rm = TRUE)
    df_numeric[[i]] <- col_data
  }
  return(df_numeric)
}

## 1.4 kNN Imputation
knn_imputation <- function(df, k = 5){
  # VIM::kNN modifies the data in place; to avoid that, use a copy
  df_copy <- df
  # kNN returns a data.frame with the imputed columns appended with ".imp"
  # The original columns remain but with no missing values replaced in place
  # So we can do something like:
  imputed_data <- kNN(df_copy, k = k, imp_var = FALSE)  # no new columns
  return(imputed_data)
}

## 1.7 Fill with half-min of non-missing values
half_min_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    non_zero <- col_data[col_data > 0]
    if(length(non_zero) > 0){
      half_min_val <- min(non_zero, na.rm = TRUE) / 2
      col_data[is.na(col_data)] <- half_min_val
    } else {
      # if entire column is zero or NA, just set to 0 or small value
      col_data[is.na(col_data)] <- 0
    }
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.8 Fill with median
median_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    col_data[is.na(col_data)] <- median(col_data, na.rm = TRUE)
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## Option 3: Replace missing values with randomized values drawn from a uniform distribution 
## The random numbers are drawn with min = 0 and max = the global minimum among all numeric values.
impute_random_uniform <- function(df) {
  df_out <- df
  num_cols <- ncol(df_out)
  # Compute the global minimum across all numeric features (ignoring NAs)
  global_min <- min(as.matrix(df_out[, 1:num_cols]), na.rm = TRUE)
  for (r in 1:nrow(df_out)) {
    row_vals <- as.numeric(df_out[r, 1:num_cols])
    missing_idx <- which(is.na(row_vals))
    if (length(missing_idx) > 0) {
      # Draw a random number for each missing value in this row
      random_vals <- runif(length(missing_idx), min = 0, max = global_min)
      row_vals[missing_idx] <- random_vals
      df_out[r, 1:num_cols] <- row_vals
    }
  }
  return(df_out)
}

## Option 4: Imputation with missForest 
impute_missforest <- function(df){
  # missForest returns a list: $ximp is the completed data
  imputed <- missForest::missForest(as.data.frame(df), verbose = FALSE)$ximp
  return(imputed)
}

##############################
## 2. New Normalization Methods ##
##############################

## Normalization Method 1: Percentage-based normalization 
## For each row, the numeric features (columns 6 onward) are divided by the row sum.
normalize_percentage <- function(df) {
  percentage_normalized <- as.data.frame(t(apply(df[, 1:ncol(df)],
                                               MARGIN = 1, 
                                               function(row) {row/sum(row, na.rm = TRUE)}))) 
  return(percentage_normalized)
}

## Normalization Method 2: Log normalization 
## For each row, take the logarithm of each numeric feature.
normalize_log <- function(df) {
  df_out <- df
  log_normalized <- as.data.frame(t(apply(df_out[, 1:ncol(df_out)], 
                                        MARGIN = 1, function(x) log(x))))
  return(log_normalized)
}

no_normalization <- function(x){
  return(x)
}

## 2.2 Min-Max scaler 
scale_minmax <- function(df){
  # We can do it manually or use caret::preProcess
  # Here is a manual approach for numeric columns
  df_mat <- as.matrix(df)
  mins <- apply(df_mat, 2, min, na.rm = TRUE)
  maxs <- apply(df_mat, 2, max, na.rm = TRUE)
  
  scaled_mat <- sweep(df_mat, 2, mins, FUN = "-")
  ranges <- maxs - mins
  scaled_mat <- sweep(scaled_mat, 2, ranges, FUN = "/")
  
  # convert back to data frame
  df_scaled <- as.data.frame(scaled_mat)
  colnames(df_scaled) <- colnames(df)
  rownames(df_scaled) <- rownames(df)
  
  return(df_scaled)
}

## 2.3 Z-score normalization
z_score_normalization <- function(df){
  # scale() in R does mean-center and unit variance by column
  df_scaled <- scale(df, center = TRUE, scale = TRUE)
  return(as.data.frame(df_scaled))
}


###########################################
## 3. Downstream Evaluation Functions  ##
###########################################

## 3.4 Pairwise Significance Testing
# Modified pairwise significance testing function
pairwise_significance_tests_modified <- function(input_df,
                                                 group_col,
                                                 start_col_index,
                                                 metadata,
                                                 target_comp) {
  # Initialize the uncorrected results data frame
  df_results <- data.frame(Feature = character(),
                           comparison_pair = character(),
                           pval = numeric(),
                           stringsAsFactors = FALSE)
  
  # Get the unique groups and compute all pairwise combinations
  groups <- unique(input_df[[group_col]])
  group_pairs <- utils::combn(groups, 2)
  
  # Loop over each feature (assumed to be in columns start_col_index to ncol(input_df))
  for(feature_col in start_col_index:ncol(input_df)) {
    # Loop over each pair of groups
    for(col_idx in 1:ncol(group_pairs)) {
      p_1 <- group_pairs[1, col_idx]
      p_2 <- group_pairs[2, col_idx]
      
      # Extract the vectors corresponding to the two groups
      vec1 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_1), feature_col]))
      vec2 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_2), feature_col]))
      
      # Require at least 3 non-missing observations in each vector
      if(sum(!is.na(vec1)) >= 3 && sum(!is.na(vec2)) >= 3) {
        # Skip if all values in one vector are identical
        if(all(vec1 == vec1[1]) || all(vec2 == vec2[1])) {
          next
        } else {
          # Test for normality using the Shapiro–Wilk test
          shapiro1 <- tryCatch(shapiro.test(vec1)$p.value, error = function(e) NA)
          shapiro2 <- tryCatch(shapiro.test(vec2)$p.value, error = function(e) NA)
          
          if(is.na(shapiro1) || is.na(shapiro2)) next
          
          # Use wilcox.test if one or both groups are not normally distributed;
          # otherwise, use t.test.
          # here we used paired=TRUE, because each feature in one category also appears in the other category (we compare one feature at a time(.
          if(shapiro1 < 0.05 || shapiro2 < 0.05) {
            pval <- tryCatch(stats::wilcox.test(vec1, vec2, paired = TRUE)$p.value, error = function(e) NA)
          } else {
            pval <- tryCatch(stats::t.test(vec1, vec2, paired=TRUE)$p.value, error = function(e) NA)
          }
        }
      } else {
        next
      }
      
      # If an error occurred (or pval is NA), skip this pair
      if (is.na(pval)) next
      
      # Append the results: record the feature name and the comparison pair
      df_results[nrow(df_results) + 1, ] <- list(Feature = colnames(input_df)[feature_col],
                                                 comparison_pair = paste0(p_1, " & ", p_2),
                                                 pval = pval)
    }
  }
  
  # Now perform multiple testing corrections using several methods.
  pvalue_methods <- c("bonferroni", "holm", "hochberg", "hommel", "BH", "BY")
  corrected_list <- list()
  uncorrected_list <- list()
  j <- 1
  
  for(method in pvalue_methods) {
    temp <- df_results
    temp$adjusted_pvalue <- stats::p.adjust(as.numeric(temp$pval), method = method)
    
    # Filter for significant features (adjusted p-value < 0.05)
    significant <- temp %>% dplyr::filter(adjusted_pvalue < 0.05) %>% dplyr::arrange(adjusted_pvalue)
    corrected_list[[j]] <- significant
    uncorrected_list[[j]] <- temp
    
    if(nrow(significant) == 0) {
      print(paste0(method, " does not result in any significant compound"))
      j <- j + 1
      next
    } else {
      # print(method)
      
      # Retrieve additional metadata (RT1, RT2, Ion1, Ion2) for each significant feature.
      rt1 <- c()
      rt2 <- c()
      Ion1 <- c()
      Ion2 <- c()
      
      for(r in 1:nrow(significant)) {
        feat <- as.numeric(significant[r, "Feature"])
        meta_row <- metadata[which(metadata$Feature == feat), ]
        if(nrow(meta_row) == 0) {
          rt1 <- c(rt1, NA)
          rt2 <- c(rt2, NA)
          Ion1 <- c(Ion1, NA)
          Ion2 <- c(Ion2, NA)
        } else {
          rt1 <- c(rt1, meta_row$RT1[1])
          rt2 <- c(rt2, meta_row$RT2[1])
          Ion1 <- c(Ion1, meta_row$Ion1[1])
          Ion2 <- c(Ion2, meta_row$Ion2[1])
        }
      }
      
      significant$RT1  <- rt1
      significant$RT2  <- rt2
      significant$Ion1 <- Ion1
      significant$Ion2 <- Ion2
      
      # Next, use the target_comp data frame to match compounds based on RT and ion values.
      compound_name <- c()
      chem_grp <- c()
      
      for(r in 1:nrow(significant)) {
        idx <- which(abs(target_comp$RT1 - significant[r, "RT1"]) <= 0.1 &
                     abs(target_comp$RT2 - significant[r, "RT2"]) <= 0.1 &
                     abs(target_comp$Ion1 - significant[r, "Ion1"]) <= 0.5 &
                     abs(target_comp$Ion2 - significant[r, "Ion2"]) <= 0.5)
        if(length(idx) == 0) {
          chem_grp <- c(chem_grp, "unknown")
          compound_name <- c(compound_name, "unknown")
        } else {
          chem_grp <- c(chem_grp, target_comp$Group[idx[1]])
          compound_name <- c(compound_name, target_comp$Compound[idx[1]])
        }
      }
      
      significant$compound_name  <- compound_name
      significant$Chemical_group <- chem_grp
      
      # print(significant %>% dplyr::arrange(adjusted_pvalue))
      # print(paste0(method, " has ", length(unique(significant$Feature)), " significant features"))
      corrected_list[[j]] <- significant
      
      j <- j + 1
    }
  }
  
  return(list(
    uncorrected = df_results,
    corrected   = corrected_list,
    uncorrected_all = uncorrected_list
  ))
}

###############################
##  Cluster resolution  ##
###############################

## Calculate cluster resolution using the first two PCA components
calculate_cluster_resolution <- function(data1, data2) {
  centroid1 <- colMeans(data1)
  centroid2 <- colMeans(data2)
  
  # Computes the Euclidean distance With Dim.1 as x-axis and Dim.2 as y-axis
  centroid_distance <-  sqrt((centroid1[1][[1]] - centroid2[1][[1]])^2 + (centroid1[2][[1]] - centroid2[2][[1]])^2)
  
  avg_distance1 <- mean(sqrt(rowSums((as.matrix(data1) - 
                       matrix(centroid1, nrow = nrow(data1), ncol = ncol(data1), byrow = TRUE))^2)))
  avg_distance2 <- mean(sqrt(rowSums((as.matrix(data2) - 
                       matrix(centroid2, nrow = nrow(data2), ncol = ncol(data2), byrow = TRUE))^2)))
  
  resolution <- centroid_distance / (avg_distance1 + avg_distance2)
  return(resolution)
}

## Compute average cluster resolution over all pairs of groups
calculate_average_cluster_resolution <- function(X, group_vector) {
  pca_model <- res.pca <- FactoMineR::PCA(
    X,
    scale.unit = FALSE,
    graph = FALSE)
  scores <- as.data.frame(pca_model$ind$coord[, 1:2])
  
  group <- as.factor(group_vector)
  group_levels <- levels(group)
  
  pair_resolutions <- c()
  
  for (i in 1:(length(group_levels) - 1)) {
    for (j in (i + 1):length(group_levels)) {
      data1 <- scores[group == group_levels[i], , drop = FALSE]
      data2 <- scores[group == group_levels[j], , drop = FALSE]
      
      if (nrow(data1) > 1 && nrow(data2) > 1) {
        res <- calculate_cluster_resolution(data1, data2)
        pair_resolutions <- c(pair_resolutions, res)
      }
    }
  }
  
  avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
  return(avg_resolution)
}

evaluate_rf_performance <- function(data, 
                                    type_col, 
                                    remove_cols, 
                                    train_proportion,
                                    ntree_candidates = c(100, 500, 1000, 2500), 
                                    seed  = NULL, 
                                    metric = "Accuracy") {
  
  set.seed(123)
  train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
  train_data <- data[train_index, ]
  test_data  <- data[-train_index, ]
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test, ntree_candidates, metric
  )
  
  eval_metrics <- all_feats$eval_metrics
  
  return(eval_metrics)
}

evaluate_model_stability <- function(
  X, y,
  ntree_candidates = c(100, 500, 1000, 2500),
  mtry_candidates  = c(floor(sqrt(ncol(X))), floor(log2(ncol(X))))
) {
  # how many folds?
  folds <- min(table(y))
  if (folds < 2) {
    stop("Not enough samples per class to do CV.")
  }

  # combine predictors + response
  data_in <- data.frame(X, Class = factor(y))

  # stratified CV control
  ctrl <- trainControl(
    method = "cv",
    number = folds,
    classProbs = TRUE
  )

  # collect *all* per‑fold accuracies here
  all_accs <- c()

  # loop over each hyperparameter combo
  for (nt in ntree_candidates) {
    for (m in mtry_candidates) {
      set.seed(sample.int(1e6, 1))  # optional: vary seed per fit
      rf_fit <- train(
        Class ~ .,
        data      = data_in,
        method    = "rf",
        metric    = "Accuracy",
        trControl = ctrl,
        tuneGrid  = data.frame(mtry = m),
        ntree     = nt
      )

      # append this combo's fold‑wise accuracies
      all_accs <- c(all_accs, rf_fit$resample$Accuracy)
    }
  }

  # compute overall mean and sd across *every* individual accuracy
  return(list(
    cv_mean = mean(all_accs, na.rm = TRUE),
    cv_std  = sd(all_accs,   na.rm = TRUE)
  ))
}

##############################################
## 4. Function to Find the “Best” Combo    ##
##############################################
# Define lists of new imputation and normalization functions
imputation_methods <- list(
  impute_missforest    = impute_missforest,
  impute_random_uniform = impute_random_uniform,
  mean_imputation            = mean_imputation,
  knn_imputation             = knn_imputation,
  impute_0.001         = impute_0.001,
  half_min_imputation        = half_min_imputation,
  median_imputation          = median_imputation
)

normalization_methods <- list(
  Percentage_Normalization = normalize_percentage,
  Min_Max_Scaler       = scale_minmax,
  Z_Score_Normalization= z_score_normalization,
  Log_Normalization        = normalize_log,
  No_normalization     = no_normalization
)

find_best_impute_normalize_new <- function(df,
                                           type_col,
                                           group_for_significance,
                                           remove_cols,
                                           train_proportion,
                                           split_with_miscunknown_supplier,
                                           start_col_index,
                                           metadata,
                                           target_comp) {
  # 1) Extract numeric features X and target y
  X       <- df %>% dplyr::select(-all_of(c(remove_cols, type_col))) %>% as.data.frame()
  y       <- df[[type_col]]
  
  # We'll store the results
  results_list <- list()
  
  # 2) Loop over imputation + normalization
  for(imp_name in names(imputation_methods)){
    impute_fun <- imputation_methods[[imp_name]]
    
    for(norm_name in names(normalization_methods)){
      norm_fun <- normalization_methods[[norm_name]]
      
      # Impute
      if(imp_name == "No_imputation"){
        X_imputed <- X
      } else {
        X_imputed <- tryCatch(
          impute_fun(X),
          error = function(e) {
            message("Error in ", imp_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_imputed)) next
      }
      
      # Normalize
      if(norm_name == "No_normalization"){
        X_norm <- X_imputed
      } else {
        X_norm <- tryCatch(
          norm_fun(X_imputed),
          error = function(e){
            message("Error in ", norm_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_norm)) next
      }
      
      if(any(is.na(X_norm))){
        stop("Combo", " Imputation method ", imp_name, " and Normalization method ", norm_name, " produced NAs!")
      }
      if(any(!is.finite(as.matrix(X_norm)))){
        stop("Combo", " Imputation method", imp_name, " and Normalization method ", norm_name, " produced Inf/NaN!")
      }
      
      # Evaluate cluster resolution (using PCA on numeric features)
      cluster_res <- calculate_average_cluster_resolution(X_norm, y)
      
       ###############################
      ##  New: Correlation Score:Computes the Pearson correlation between the upper‐triangle elements of the correlation matrix of your original data (computed with pairwise complete observations) and that of your imputed/normalized data. A higher score means that the imputation has preserved the pairwise relationships better.      The correlation score is computed as a Pearson correlation coefficient, which can range from -1 to 1. In this context, a score close to 1 indicates that the imputed/normalized data have preserved the original pairwise relationships very well. A score of 0 would indicate no linear relationship between the two sets of correlation values, and a negative value would suggest that the dependency structure is inversely related.##
      ###############################
      # Compute correlation matrix on original data using pairwise complete observations
      orig_corr <- cor(X, use = "pairwise.complete.obs")
      # Compute correlation matrix on imputed/normalized data
      imp_corr <- cor(X_norm, use = "pairwise.complete.obs")
      # Extract upper-triangle values (excluding diagonal) for both matrices
      orig_upper <- orig_corr[upper.tri(orig_corr)]
      imp_upper  <- imp_corr[upper.tri(imp_corr)]
      # Compute correlation between the two sets of values
      corr_score <- cor(orig_upper, imp_upper, use = "complete.obs")
      
      ###############################
      ##  New: KS Test p-value: Compare the distribution of orignal data with missing values against data with imputed values ##
      ###############################
      # Combine observed values and imputed values across all numeric columns
      observed_all <- c()
      imputed_all  <- c()
      
      for(col in names(X)) {
        missing_idx <- which(is.na(X[[col]]))
        # Only consider columns that originally had missing values and at least one observed value
        if(length(missing_idx) >= 1 && length(X[[col]][!is.na(X[[col]])]) >= 1){
          # 1) original non‐missing values in this column
          obs_values <- X[[col]][!is.na(X[[col]])]
          
          # 2) entire column from the imputed dataset (includes both originally observed
          #    and newly imputed values)
          imp_values <- X_norm[[col]]
          
          # Append to the pooled vectors
          observed_all <- c(observed_all, obs_values)
          imputed_all  <- c(imputed_all, imp_values)
        }
      }
      
      if(length(observed_all) < 2 || length(imputed_all) < 2) {
        ks_p <- NA
      } else {
        ks_result <- tryCatch({
          ks.test(observed_all, imputed_all)
        }, error = function(e) NULL)
        if(!is.null(ks_result)){
          ks_p <- ks_result$p.value
        } else {
          ks_p <- NA
        }
      }
      
      ###############################
      ##  New: RF Test Accuracy    ##
      ###############################
      # Use the imputed/normalized data (X_norm) and class vector y to compute test accuracy.
      df_input_test_acc <- cbind(X_norm, df[,c(type_col, remove_cols)])

      performance_output <- evaluate_rf_performance(data = df_input_test_acc,
                                               type_col=type_col,
                                               remove_cols=remove_cols,
                                               train_proportion = train_proportion)
      
      # Evaluate CV accuracy
      rf_stability <- evaluate_model_stability(X_norm, y)
      cv_mean <- rf_stability$cv_mean
      cv_std  <- rf_stability$cv_std
      
      pairwise_results <- pairwise_significance_tests_modified(input_df = df_input_test_acc,
                                                               group_col = group_for_significance,
                                                               metadata = metadata,
                                                               target_comp = target_comp,
                                                               start_col_index = start_col_index)
      sig_num <- max(sapply(pairwise_results$corrected, function(subdf) length(unique(subdf$Feature))))
      
      results_list[[length(results_list)+1]] <- data.frame(
        Imputation   = imp_name,
        Normalization= norm_name,
        ClusterRes    = cluster_res,
        CorrScore     = corr_score,
        KS_p          = ks_p,
        CVmean       = cv_mean,
        CVstd        = cv_std,
        sig_num = sig_num,
        TestAcc       = performance_output$Accuracy,
        Kappa = performance_output$Kappa,
        AUC = performance_output$AUC,
        F1_Weighted = performance_output$F1_Weighted,
        MCC_Multiclass = performance_output$MCC_Multiclass,
        stringsAsFactors=FALSE
      )
    }
  }
  
  df_results <- do.call(rbind, results_list)
  
  # Build an ordering index: primary by TestAcc, secondary by ClusterRes, etc.
  ord <- order(
     -df_results$TestAcc,
     -df_results$AUC,
    # -df_results$F1_Macro,
    -df_results$ClusterRes,
    -df_results$sig_num,
    # -df_results$CVmean,
    # df_results$CVstd,
    -df_results$CorrScore,   # smaller rank_corrScore = better CorrScore
    -df_results$KS_p,
    -df_results$F1_Weighted,
    -df_results$MCC_Multiclass
    # -df_results$Kappa,           # smaller  = better KS_p
  )
  
  # Initialize and fill combined_rank so that best row gets 1
  df_results$combined_rank <- NA_integer_
  df_results$combined_rank[ord] <- seq_along(ord)
  
  # Pick the best
  best_row <- df_results[ which.min(df_results$combined_rank) , ]
  
  # Return the full table + the best combo row
  return(list(
    results_table = df_results,
    best_combo    = best_row
  ))
}
```

# Exploratory Data Analysis

## Import important features from RFA RandomForest
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
rfa_important_features <- jsonlite::fromJSON("RFA_best_feature_withdatacompression_missforest_TSN.json")
rfe_important_features <- jsonlite::fromJSON("RFE_best_feature_withdatacompression_missforest_TSN.json")

pairwise_res <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/Arson project/Rproject/Arson/comparison of feature grouping methods and data compression via pair-wise test/13th Dec/withdatacompression_missforest_TSN.xlsx")

final_cleandf_pairwise <- cbind(df_percentage[, c(1,2,3,4,5)], 
                                df_percentage[, -c(1,2,3,4,5)] %>% 
                                  dplyr::select(unique(pairwise_res$Feature))) 
final_cleandf_RFA <- cbind(df_percentage[, c(1,2,3,4,5)], 
                                df_percentage[, -c(1,2,3,4,5)] %>% 
                                  dplyr::select(rfa_important_features)) 
final_cleandf_RFE <- cbind(df_percentage[, c(1,2,3,4,5)], 
                                df_percentage[, -c(1,2,3,4,5)] %>% 
                                  dplyr::select(rfe_important_features)) 
```

# Figure 1. Two subplots 
** left-subplot: percentage composition of chemical group in metadata; 
** right-subplot: horizontal stacked bar plot of signal intensity across gas stations of each chemical group in metadata
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
metadata_annotatedbyCaleb <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/metadata_withcompression_featureelimination_Caleb.xlsx")

# Convert Chemical_group to a factor
metadata_annotatedbyCaleb$Chemical_group <- as.factor(metadata_annotatedbyCaleb$Chemical_group)

## ** Left subplot: Percentage composition of each chemical group -----------
chemical_group_counts <- metadata_annotatedbyCaleb %>%
  dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE)) %>%
  dplyr::group_by(Chemical_group) %>%
  # count the occurence of compounds of each chemical group in all gasoline samples
  summarise(Count = n()) %>%
  dplyr::mutate(Percentage = (Count / sum(Count)) * 100)

p1 <- ggplot(chemical_group_counts, aes(x = reorder(Chemical_group, -Percentage), 
                                        y = Percentage, fill = reorder(Chemical_group, -Percentage) # Chemical_group
                                        )) +
  geom_bar(stat = "identity", width = 0.7) +
  labs(x = "Chemical Group", y = "Percentage composition of chemical groups (%)") + # , title = "Percentage Composition of Chemical Groups") +
  theme_classic(base_size = 25) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, color = "black"),
        # Axis ticks color
        axis.text.y = element_text(color = "black"),
        legend.position = "none")
p1

## **Right subplot: horizontal stacked bar plot of signal intensity across gas stations of each chemical group in metadata ----------
data_long <- metadata_annotatedbyCaleb %>%
  dplyr::select(Chemical_group, everything()) %>%
  pivot_longer(cols = 9:ncol(metadata_annotatedbyCaleb), names_to = "Sample_name", values_to = "Value") %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
                              ifelse(str_detect(Sample_name, "F002"), "Station 2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station 3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station 4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station 5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station 6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station 7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station 8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>%
  dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE)) %>%
  # dplyr::filter(Chemical_group %notin% "unknown") %>%
  group_by(gas_station, Chemical_group) %>%
  summarise(Value = sum(Value, na.rm=TRUE), .groups = "drop") %>% 
  mutate(
    Chemical_group = factor(Chemical_group, levels = levels(reorder(chemical_group_counts$Chemical_group, -chemical_group_counts$Percentage)))) %>% # c("Alkane", "Cycloalkane", "Branched Alkane",
                                                       # "Aromatics", "BTEX", "Alkene", "Castle Group",
                                                       # "Gang of Four", "Indane", "PAHs-Naphthalene"))
  # ) %>%
  mutate(
    gas_station = factor(gas_station, levels = c("Station 1", "Station 2", "Station 3", "Station 4",
                                                 "Station 5", "Station 6", "Station 7", "Station 8",
                                                 "Station 9", "Station 10"))
  )


p2 <- ggplot(data_long,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = "Proportion of total peak area (%)", y = "Gas Station", fill = "Chemical Group") +
  theme_minimal(base_size = 25) +
  theme(
    # axis.title.y = element_text(
    #   margin = ggplot2::margin(r = 0, unit = "pt")
    # ),
     # Legend text size
    legend.text = element_text(size = 25),
    
    # Axis ticks color
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    # Remove ticks on y-axis
    axis.ticks.length.y = unit(0, "cm"),
    # Remove gridlines
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  )
# ggplot(data_long, aes(x = Chemical_group, y = Mean, fill = Chemical_group)) +
  # geom_bar(stat = "identity", position = position_dodge(), width = 0.7) +
  # geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.7)) +
  # labs(x = "Chemical Group", y = "Mean ± SD") + # , title = "Mean and Standard Deviation of Values") +
  # theme_classic(base_size = 18) +
  # theme(axis.text.x = element_text(angle = 45, hjust = 1),
  #       legend.position = "none")
p2

# Combine plots using gridExtra
figure2 <- grid.arrange(p1, p2, ncol = 2, widths = c(1,2))
figure2
ggsave(filename = paste0("figure1_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = figure2, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in") 
```

# Stacked barplot of RFA compounds (normalize to the Area of RFA compounds instead of all the compounds)
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# for (r in 1:nrow(coredf_RFA)) {
#   coredf_RFA[r, which(base::is.na(coredf_RFA[r,]))] <- 0.001
# }
# 
# percentage_normalized <- as.data.frame(t(apply(coredf_RFA[, 6:ncol(coredf_RFA)],
#                                                MARGIN = 1, 
#                                                       function(row) {row/sum(row, na.rm = TRUE)}))) 
#   
# df_0.001_percentage_normalized_RFA <- percentage_normalized %>% 
#   mutate(gas_station = coredf_RFA$gas_station) %>%
#   mutate(Octane_rating = coredf_RFA$Octane_rating) %>%
#   mutate(sampling_season = coredf_RFA$sampling_season) %>%
#   mutate(Sample_name = coredf_RFA$Sample_name) %>%
#   mutate(supplier = coredf_RFA$supplier) %>%
#   relocate(Octane_rating, sampling_season, gas_station, Sample_name, supplier, .before = 1)
# 
# plotdf <- df_0.001_percentage_normalized_RFA %>% 
#   tidyr::pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Normalized_area")

plotdf <- cbind(coredf[, c(1,2,3,4,5)], coredf[, -c(1,2,3,4,5)] %>%
  dplyr::select(unique(pairwise_res$Feature))) %>%
  mutate(gas_station = factor(gas_station, levels = c("Station 1",  "Station 2", "Station 3", "Station 4", "Station 5", "Station 6", "Station 7",  "Station 8",  "Station 9", "Station 10" ))) 

plot_list <- list()
i <- 1
for (variable in sort(unique(plotdf$gas))) {
    plot_df <- plotdf %>%
        filter(supplier %in% variable)
    
    plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Normalized_area, fill = Chemical_group)) +
        geom_bar(stat = "identity") +
        labs(title = variable,
             x = "", 
             y = "") +
        theme_minimal(base_size = 18) +
        theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                         # size = 18,
                                         face = "bold"), 
              axis.ticks.length.x = unit(0, "cm"),
              # legend.text = element_text(size = 18),
              # legend.title = element_text(size = 18),
              # axis.title.y = element_text(size = 15),
              legend.position = "hidden",
              plot.title = element_text(hjust = 0.5),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.major.y = element_blank()
        ) + 
        scale_y_continuous(expand = c(0,0), limits = c(0, 1))
    i <- i + 1
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Normalized_area, fill = Chemical_group)) +
                                  geom_bar(stat = "identity") +
                                  facet_wrap(~sampling_season) +
                                  labs(fill = "Chemical Groups") +
                                  theme_minimal(base_size = 20) +
                                  theme(legend.text = element_text(size = 18),
                                        legend.title = element_text(size = 20),
                                        axis.title.y = element_text(size = 20),
                                        legend.position = "right"))

y <- textGrob("Normalized Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

stacked_barplot <- grid.arrange(grobs = plot_list, ncol = 3, nrow = 2, right = legend, left = y, bottom = x)
ggsave(filename = paste0("Stacked_barplot_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = stacked_barplot, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in") 
```

# Figure 3. Horizontal stacked bar plot of chemical groups of from Figure 1 in different gas stations - Before versus After best imp/norm 

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Before normalization ---------------
data_long <- metadata_annotatedbyCaleb %>%
  dplyr::select(Chemical_group, everything()) %>%
  pivot_longer(cols = 9:ncol(metadata_annotatedbyCaleb), names_to = "Sample_name", values_to = "Value") %>%
  mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
                              ifelse(str_detect(Sample_name, "F002"), "Station 2",
                                     ifelse(str_detect(Sample_name, "F003"), "Station 3",
                                            ifelse(str_detect(Sample_name, "F004"), "Station 4",
                                                   ifelse(str_detect(Sample_name, "F005"), "Station 5",
                                                          ifelse(str_detect(Sample_name, "F006"), "Station 6",
                                                                 ifelse(str_detect(Sample_name, "F007"), "Station 7",
                                                                        ifelse(str_detect(Sample_name, "F008"), "Station 8",
                                                                               ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>%
  dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE)) %>%
  # dplyr::filter(Chemical_group %notin% "unknown") %>%
  group_by(gas_station, Chemical_group) %>%
  summarise(Value = sum(Value, na.rm=TRUE), .groups = "drop") %>% 
  mutate(
    Chemical_group = factor(Chemical_group, levels = levels(reorder(chemical_group_counts$Chemical_group, -chemical_group_counts$Percentage)))) %>% 
  mutate(
    gas_station = factor(gas_station, levels = c("Station 1", "Station 2", "Station 3", "Station 4",
                                                 "Station 5", "Station 6", "Station 7", "Station 8",
                                                 "Station 9", "Station 10"))
  )


plot1 <- ggplot(data_long,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = NULL, fill = "Chemical Group") +
  theme_minimal(base_size = 27) +
  theme(
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.ticks.length.y = unit(0, "cm"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(), legend.position = "hidden"
  )
plot1

# plotdf1 <- metadata_annotatedbyCaleb %>% 
#   dplyr::select(-c("RT1", "RT2", "Ion1", "Ion2", 
#                    "Stauffer_chemical_group", "Stauffer_compound_name")) %>%
#   pivot_longer(cols = 3:ncol(.), 
#                names_to = "Sample_name", 
#                values_to = "Peak_Area") %>%
#   mutate(
#     sampling_season = case_when(
#       str_detect(Sample_name, "b") ~ "blue",
#       str_detect(Sample_name, "p") ~ "purple",
#       TRUE ~ "orange"
#     ),
#     # Extract station number if pattern F00[1-9] is found, else default to 10
#     gas_station = ifelse(
#       str_detect(Sample_name, "F00[1-9]"),
#       paste("Station", str_remove(str_extract(Sample_name, "F00[1-9]"), "F00")),
#       "Station 10"
#     )
#   ) %>%
#   # Factor the gas_station with levels Station 1 to Station 10
#   mutate(gas_station = factor(gas_station, levels = paste0("Station ", 1:10)))
# 
# plot_list <- list()
# i <- 1
# for (variable in c("Station 3", "Station 7")) { # sort(unique(plotdf1$gas_station))) {
#   plot_df <- plotdf1 %>%
#     dplyr::filter(Chemical_group %notin% "unknown") %>%
#     dplyr::filter(gas_station %in% variable) %>%
#     mutate(
#     Chemical_group = factor(Chemical_group, levels = c("Alkane", "Cycloalkane", "Branched Alkane",
#                                                        "Aromatics", "BTEX", "Alkene", "Castle Group",
#                                                        "Gang of Four", "Indane", "PAHs-Naphthalene"))
#   )
#   
#   plot_list[[i]] <- ggplot(data = plot_df, aes(x = Chemical_group, y = Peak_Area)) +
#     geom_boxplot(aes(fill = Chemical_group), position = "dodge") +
#     labs(title = variable,
#          x = "", 
#          y = "") +
#     theme_classic(base_size = 25) +
#     theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.25,
#                                      size = 12, face = "bold"),
#           axis.ticks.length.x = unit(0, "cm"),
#           legend.position = "hidden",
#           plot.title = element_text(hjust = 0.5),
#           panel.grid = element_blank()
#     ) + 
#     scale_y_continuous(expand = c(0,0), limits = c(0, max(plotdf1$Peak_Area, na.rm = TRUE)))
#   i <- i + 1
# }
# 
# y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
# x <- textGrob("Feature", gp = gpar(fontsize = 20))
# 
# stauffer_boxplot <- grid.arrange(grobs = plot_list, ncol = 2, nrow = 1, left = y, bottom = x)

# ggsave(filename = paste0("Box_plot_RawPA","_chemicalgroup_by gas station", format(Sys.time(), "%d-%m-%Y"),".png"), 
#        plot = stauffer_boxplot, 
#        device = "png",
#        dpi = 600,        # Set DPI to 600
#        width = 20,       # Width in inches (adjust as needed)
#        height = 8,       # Height in inches (adjust as needed)
#        units = "in") 

# After normalization -----------
data <- input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
data[[type_col]] <- as.factor(data[[type_col]])
levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(data[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
data          <- data[data[[type_col]] %in% keep_classes, ]
data[[type_col]] <- droplevels(data[[type_col]])

data <- data %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(data) <- paste0(rownames(data), "_", data$supplier)

# Identify numeric columns from predictors (excluding remove_cols and target)
numeric_cols <- names(which(sapply(data, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))

X_original   <- data[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[["impute_random_uniform"]](X_original)
X_final      <- normalization_methods[["Percentage_Normalization"]](X_imputed)
data[, numeric_cols] <- X_final


plotdf2 <- left_join(data %>% 
            pivot_longer(cols = 5:ncol(.), names_to = "Feature", values_to = "Value"), 
          metadata_annotatedbyCaleb %>% 
            mutate(Feature = as.character(Feature)) %>% 
            dplyr::mutate(Chemical_group = gsub(x= Chemical_group, pattern = "unknown", replacement = "ungrouped", fixed = TRUE))
          , by = "Feature") %>%
  group_by(gas_station, Chemical_group) %>%
  summarise(Value = sum(Value, na.rm=TRUE), .groups = "drop") %>% 
  mutate(
    Chemical_group = factor(Chemical_group, levels = levels(reorder(chemical_group_counts$Chemical_group, -chemical_group_counts$Percentage)))) %>% 
  dplyr::mutate(gas_station = gsub(x=gas_station, replacement = " ", pattern =".", fixed=TRUE)) %>%
  mutate(
    gas_station = factor(gas_station, levels = c("Station 1", "Station 2", "Station 3", "Station 4",
                                                 "Station 5", "Station 6", "Station 7", "Station 8",
                                                 "Station 9", "Station 10"))
  )

plot2 <- ggplot(plotdf2,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = NULL, fill = "Chemical Group") +
  theme_minimal(base_size = 27) +
  theme(
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.ticks.length.y = unit(0, "cm"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(), legend.position = "hidden"
  )
plot2

legend <- cowplot::get_legend(ggplot(plotdf2,
             aes(x = Value, y = gas_station, fill = Chemical_group)) +
  geom_col(position = "fill") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = NULL, fill = "Chemical Group") +
  theme_minimal(base_size = 27) +
  theme(
    legend.text = element_text(size = 25),
    axis.text.x = element_text(color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.ticks.length.y = unit(0, "cm"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(), legend.position = "right"
  ))

y <- textGrob("Gas Station", rot = 90, gp = gpar(fontsize = 25))
x <- textGrob("Proportion of total peak area (%)", gp = gpar(fontsize = 25))

figure3 <- grid.arrange(plot1, plot2, ncol = 2, widths = c(1,1), right = legend, left = y, bottom = x)
figure3

# Plotting
# plotdf2 <- data %>%
#   pivot_longer(cols = 5:ncol(.), 
#                names_to = "Feature", 
#                values_to = "Peak_Area") %>%
#   dplyr::mutate(Feature = as.numeric(Feature)) %>%
#   mutate(gas_station = gsub(".", " ", gas_station, fixed = TRUE)) %>%
#   mutate(gas_station = factor(gas_station, levels = paste0("Station ", 1:10)))
# 
# plotdf2_new <- left_join(plotdf2,
#                      metadata_annotatedbyCaleb %>% 
#                        dplyr::select(c("Feature", "Chemical_group")), by = "Feature")
# 
# plot_list2 <- list()
# i <- 1
# for (variable in c("Station 3", "Station 7")) { # sort(unique(plotdf2_new$gas_station))) {
#   plot_df <- plotdf2_new %>%
#     dplyr::filter(Chemical_group %notin% "unknown") %>%
#     dplyr::filter(gas_station %in% variable) %>%
#     mutate(
#     Chemical_group = factor(Chemical_group, levels = c("Alkane", "Cycloalkane", "Branched Alkane",
#                                                        "Aromatics", "BTEX", "Alkene", "Castle Group",
#                                                        "Gang of Four", "Indane", "PAHs-Naphthalene"))
#   )
#   
#   plot_list2[[i]] <- ggplot(data = plot_df, aes(x = Chemical_group, y = Peak_Area)) +
#     geom_boxplot(aes(fill = Chemical_group), position = "dodge") +
#     labs(title = variable,
#          x = "", 
#          y = "") +
#     theme_classic(base_size = 25) +
#     theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.25,
#                                      size = 12, face = "bold"),
#           axis.ticks.length.x = unit(0, "cm"),
#           legend.position = "hidden",
#           plot.title = element_text(hjust = 0.5),
#           panel.grid = element_blank()
#     ) + 
#     scale_y_continuous(expand = c(0,0), limits = c(0, max(plotdf2$Peak_Area, na.rm = TRUE)))
#   i <- i + 1
# }
# 
# y <- textGrob("Normalized Peak Area", rot = 90, gp = gpar(fontsize = 20))
# x <- textGrob("Feature", gp = gpar(fontsize = 20))
# 
# stauffer_boxplot2 <- grid.arrange(grobs = plot_list2, ncol = 2, nrow = 1, left = y, bottom = x)


ggsave(filename = paste0("Figure3_Duel_horizontal-stacked-bar-plot_before and after imp-norm","_chemicalgroup_by gas station_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = figure3, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 12,       # Height in inches (adjust as needed)
       units = "in") 
```

# Barplot RFA - number of compounds in each chemical group
```{r, echo = FALSE, message = FALSE, warning = FALSE}
rfa_important_sheet <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/RFA_features_19thNov2024_HN.xlsx")
rfa_important_sheet$Feature <- as.character(rfa_important_sheet$Feature)

count_rfa <- rfa_important_sheet %>% 
  group_by(Chemical_group) %>%
  summarise(count = n())

ggplot(data = count_rfa, aes(x = Chemical_group, y = count)) +
  geom_bar(stat = "identity") +
  labs(x = "Chemical group") +
  theme_classic(base_size = 18) +
    theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                     # size = 18,
                                     face = "bold"), 
          axis.ticks.length.x = unit(0, "cm"),
          # legend.text = element_text(size = 18),
          # legend.title = element_text(size = 18),
          # axis.title.y = element_text(size = 15),
          legend.position = "hidden",
          plot.title = element_text(hjust = 0.5),
          panel.grid.minor.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_blank()
    ) + 
    scale_y_continuous(expand = c(0,0))

```

## Stacked bar plot of significant compounds and associated suppliers
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- p_nonunique_df %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Area") %>%
  filter(Feature %in% unique(combined_df$Feature))

rt1 <- c()
rt2 <- c()
major <- c()
minor <- c()
chem_grp <- c()
compound_name <- c()
for (row in 1:nrow(df)) {
  rt1 <- c(rt1, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$RT1)
  rt2 <- c(rt2,new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$RT2)
  major <- c(major, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$Ion1)
  minor <- c(minor, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$Ion2)
  chem_grp <- c(chem_grp, new_df[which(new_df$Feature == as.numeric(df[row,]$Feature)),]$Chemical_group)
}

df$RT1 <- rt1
df$RT2 <- rt2
df$Ion1 <- major
df$Ion2 <- minor
df$Chemical_group <- chem_grp

## Get compound name 
compound_name <- c()
for (row in 1:nrow(df)) {
  idx <- which(abs(target_comp$RT1 - df[row,]$RT1) <= 0.1 &
                 abs(target_comp$RT2 - df[row,]$RT2) <= 0.1 &
                 abs(target_comp$Ion1 - df[row,]$Ion1) <= 0.5 &
                 abs(target_comp$Ion2 - df[row,]$Ion2) <= 0.5)
  # print(paste0(row, "_", idx))
  if (identical(idx, integer(0))) {
    compound_name <- c(compound_name, "unknown")
  }
  compound_name <- c(compound_name, target_comp[idx,]$Compound)
}

df$compound_name <- compound_name
df$Sample_name <- factor(df$Sample_name, levels = sort(unique(df$Sample_name)))
plotdf <- df %>% 
  filter(gas_station %in% c("Station 3", "Station 4", "Station 6", "Station 7")) %>%
  filter(compound_name %notin% "unknown")

plot_list <- list()
i <- 1
for (focus_supplier in unique(plotdf$supplier)) {
    plot_df <- plotdf %>%
        filter(supplier %in% focus_supplier)
    
    plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = compound_name)) +
        geom_bar(stat = "identity") +
        labs(title = focus_supplier,
             x = "", 
             y = "") +
        theme_minimal(base_size = 18) +
        theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                         # size = 18,
                                         face = "bold"), 
              axis.ticks.length.x = unit(0, "cm"),
              # legend.text = element_text(size = 18),
              # legend.title = element_text(size = 18),
              # axis.title.y = element_text(size = 15),
              legend.position = "hidden",
              plot.title = element_text(hjust = 0.5),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.major.y = element_blank()
        ) + 
        scale_y_continuous(expand = c(0,0), limits = c(0, 0.5))
    i <- i + 1
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = compound_name)) +
                                  geom_bar(stat = "identity") +
                                  facet_wrap(~supplier) +
                                  labs(fill = "Chemical Groups") +
                                  theme_minimal(base_size = 20) +
                                  theme(legend.text = element_text(size = 18),
                                        legend.title = element_text(size = 20),
                                        axis.title.y = element_text(size = 20),
                                        legend.position = "right"))

y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_list, ncol = 2, nrow = 2, right = legend, left = y, bottom = x)
```

# Stacked bar plot of each suppliers
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- percentage_normalized[[4]] %>% # df_step3  %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Area") %>%
  filter(Feature %in% unique(t_test_res$Feature))

# Obtain chemical group for each feature 
# Create a named vector for fast lookup
compound_name <- setNames(t_test_res$compound_name, t_test_res$Feature)

# Use the vector to directly map the Chemical_group to the Feature in df
df$compound_name <- compound_name[df$Feature]

df$Sample_name <- factor(df$Sample_name, levels = sort(unique(df$Sample_name)))

df <- df %>% 
  filter(gas_station %in% c("Station 3", "Station 4", "Station 6", "Station 7")) %>%
  filter(supplier %notin% "unknown") %>% 
  filter(Chemical_group %notin% "unknown")


plot_list <- list()
i <- 1
for (focus_supplier in unique(df$supplier)) {
  plot_df <- df %>%
    filter(supplier %in% focus_supplier)
  
  plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
    geom_bar(stat = "identity") +
    labs(title = focus_supplier,
         x = "", 
         y = "") +
    theme_minimal(base_size = 18) +
    theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25,
                                     # size = 18,
                                     face = "bold"), 
          axis.ticks.length.x = unit(0, "cm"),
          # legend.text = element_text(size = 18),
          # legend.title = element_text(size = 18),
          # axis.title.y = element_text(size = 15),
          legend.position = "hidden",
          plot.title = element_text(hjust = 0.5),
          panel.grid.minor.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_blank()
          ) + 
    scale_y_continuous(expand = c(0,0), limits = c(0, max(df$Area)))
  i <- i + 1
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
                                geom_bar(stat = "identity") +
                                facet_wrap(~supplier) +
                                labs(fill = "Chemical Groups") +
                                theme_minimal(base_size = 20) +
                                theme(legend.text = element_text(size = 18),
                                      legend.title = element_text(size = 20),
                                      axis.title.y = element_text(size = 20),
                                      legend.position = "right"))

y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_list, ncol = 3, nrow = 2, right = legend, left = y, bottom = x)
```

# Stacked bar plot of sampling seasons
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- df_step3  %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Area")

# Obtain chemical group for each feature 
# Create a named vector for fast lookup
chemical_groups <- setNames(metadata$Chemical_group, metadata$Feature)

# Use the vector to directly map the Chemical_group to the Feature in df
df$Chemical_group <- chemical_groups[df$Feature]

df$Sample_name <- factor(df$Sample_name, levels = sort(unique(df$Sample_name)))
df$Octane_rating <- factor(df$Octane_rating, levels = sort(unique(df$Octane_rating)))
df$sampling_season <- factor(df$sampling_season, levels = sort(unique(df$sampling_season)))
df <- df %>% filter(supplier %notin% "unknown") %>% filter(Chemical_group %notin% "unknown")

plot_list <- list()
i <- 1
for (focus_season in levels(df$sampling_season)) {
  for (focus_octane in levels(df$Octane_rating)) {
    plot_df <- df %>%
      filter(sampling_season %in% focus_season) %>%
      filter(Octane_rating %in% focus_octane)
    
    plot_list[[i]] <- ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
      geom_bar(stat = "identity") +
      labs(title = paste0(focus_season, " - ", focus_octane),
           x = "", 
           y = "") +
      theme_minimal(base_size = 18) +
      theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, size = 15,
                                       face = "bold"), 
            axis.ticks.length.x = unit(0, "cm"),
            legend.text = element_text(size = 18),
            legend.title = element_text(size = 18),
            axis.title.y = element_text(size = 15),
            legend.position = "hidden",
            plot.title = element_text(hjust = 0.5),
            panel.grid.minor.x = element_blank(),
            panel.grid.minor.y = element_blank(),
            panel.grid.major.x = element_blank(),
            panel.grid.major.y = element_blank()
      ) + 
      scale_y_continuous(expand = c(0,0))
    i <- i + 1 
  }
}

legend <- cowplot::get_legend(ggplot(data = plot_df, aes(x = Sample_name, y = Area, fill = Chemical_group)) +
                                geom_bar(stat = "identity") +
                                labs(fill = "Chemical Groups") +
                                theme_minimal(base_size = 20) +
                                theme(legend.text = element_text(size = 18),
                                      legend.title = element_text(size = 20),
                                      axis.title.y = element_text(size = 20),
                                      legend.position = "right"))

y <- textGrob("Peak Area", rot = 90, gp = gpar(fontsize = 20))
x <- textGrob("Samples", gp = gpar(fontsize = 20))

grid.arrange(grobs = plot_list, ncol = 4, right = legend, left = y, bottom = x)
```


# Clustering analyses

## PCA and Cluster Resolution

(Update 15 April 2024:) When using either Octane_rating OR sampling_season, there is no clustering whatsoever between seasons OR Octane rate

!!! After removing unique cols
*** With global minimum imputation:

!! Tried top 150 compounds with highest cumulative peak area => still no clustering on both sampling seasons and octane rating.

!! Tried top 150 compounds with highest variance 

(Update 29th April:) 
**DATA COMPRESSION OR NOT??**

If I **compress all samples into a grouping variables**, such as Octane_rating or sampling_season, I can clearly see the differentiation between these grouping variables. 

**For gas station, ** all 3 imputation techniques all resulted in low Dim1 and Dim2. However, with Zero and Global Min, there seems to be a grouping of gas station no. 2,8,3 (group 1); no. 1, 5 (group 2), no. 10, 4, 9, 7, 6 (group 3).

***If I keep the samples separated*** , then I tried log normalization with 3 imputation techniques ==> Zero imputation will resulted in -Inf after Log-normalization so we can only do Global min and LOD. However, both global min and LOD does nto result in any clear grouping/clustering between "sampling_season", "Octane_rating", "gas_station".

**LOG NORMALIZATION VERSUS. PERCENTAGE-BASED NORMALIZATION**
I also tried remove the top 100 compounds that have really high mean and low variance across sample and perform zero//global min// LOD imputation on them and then tried Log-normalization ==> still no clustering between gas station!!

Also, I tried to run PCA with un-normalized data (after imputation) with 3 imputation methods but still no clustering of gas stations across all 3 imputation methods.

(Update 22nd May 2024:) 
 - Clustering again with 'optimized' alignment windows -> No cluster of gas station
 - Use only target compounds -> Still No cluster of gas station either with zero or LOD imputation coupled with TSN or Log normalization.

### PCA
```{r, echo = FALSE, message = FALSE, warning = FALSE}
pca_input <- data
# p_nonunique_df %>% 
# dplyr::select(-setdiff(colnames(p_nonunique_df[,6:ncol(p_nonunique_df)]), final_vec)) # %>% 
# filter(gas_station %in% "Station 1")

res.pca <- FactoMineR::PCA(
  pca_input[, 5:ncol(pca_input)],
  scale.unit = FALSE,
  graph = FALSE)

# Scree plot
# fviz_screeplot(res.pca, ncp=10)

# Biplot

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            # select.var = list(contrib = 3), # Select top x features with highest contribution to PC1/PC2
                            repel = TRUE,
                            labelsize = 5, 
                            habillage = factor(pca_input$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name", "supplier"
                            # palette = "ucscgb",
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            # xlim = c(-0.2, 0.2),
                            # ylim = c(-0.009, 0.009),
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(20, 'mm')) +
  theme(legend.position = 'bottom',
        panel.spacing = unit(0, "lines"))

# ggsave(filename = paste0("PCA_plot","_only_non-uniquefillNA0.001_TSN_sig_comp_nonunique_unionize", format(Sys.time(), "%d-%m-%Y"),".png"), 
#        plot = pcaplot, 
#        device = "png",
#        dpi = 600,        # Set DPI to 600
#        width = 13,       # Width in inches (adjust as needed)
#        height = 8,       # Height in inches (adjust as needed)
#        units = "in") 
```

## Figure 3: PCA biplot of gas station between best and worst ClusterRes score
```{r , echo=FALSE, warning = FALSE, message=FALSE}
### Worst impu/norm ---------
worst_impnorm <- input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
worst_impnorm[[type_col]] <- as.factor(worst_impnorm[[type_col]])
# levels(worst_impnorm[[type_col]]) <- make.names(levels(worst_impnorm[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(worst_impnorm[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
worst_impnorm          <- worst_impnorm[worst_impnorm[[type_col]] %in% keep_classes, ]
worst_impnorm[[type_col]] <- droplevels(worst_impnorm[[type_col]])

worst_impnorm <- worst_impnorm %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(worst_impnorm) <- paste0(rownames(worst_impnorm), "_", worst_impnorm$supplier)

# Identify numeric columns from predictors (excluding remove_cols and target)
numeric_cols <- names(which(sapply(worst_impnorm, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))

worst_imp  <- best_res$results_table$Imputation[which.max(best_res$results_table$combined_rank)]
worst_norm <- best_res$results_table$Normalization[which.max(best_res$results_table$combined_rank)]

X_original   <- worst_impnorm[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[[worst_imp]](X_original)
X_final      <- normalization_methods[[worst_norm]](X_imputed)
worst_impnorm[, numeric_cols] <- X_final

res.pca <- FactoMineR::PCA(
  worst_impnorm[, 5:ncol(worst_impnorm)],
  scale.unit = FALSE,
  graph = FALSE)

pca_worst_impnorm <- factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            # select.var = list(contrib = 3), # Select top x features with highest contribution to PC1/PC2
                            repel = TRUE,
                            labelsize = 5, 
                            habillage = factor(worst_impnorm$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name", "supplier"
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            # xlim = c(-0.2, 0.2),
                            # ylim = c(-0.009, 0.009),
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(20, 'mm')) +
  theme(legend.position = 'bottom',
        panel.spacing = unit(0, "lines"))

### Best impu/norm ---------
best_impnorm <- input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
best_impnorm[[type_col]] <- as.factor(best_impnorm[[type_col]])
# levels(best_impnorm[[type_col]]) <- make.names(levels(best_impnorm[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(best_impnorm[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
best_impnorm          <- best_impnorm[best_impnorm[[type_col]] %in% keep_classes, ]
best_impnorm[[type_col]] <- droplevels(best_impnorm[[type_col]])

best_impnorm <- best_impnorm %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(best_impnorm) <- paste0(rownames(best_impnorm), "_", best_impnorm$supplier)

# Identify numeric columns from predictors (excluding remove_cols and target)
numeric_cols <- names(which(sapply(best_impnorm, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))

best_imp  <- best_res$results_table$Imputation[which.min(best_res$results_table$combined_rank)]
best_norm <- best_res$results_table$Normalization[which.min(best_res$results_table$combined_rank)]

X_original   <- best_impnorm[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[[best_imp]](X_original)
X_final      <- normalization_methods[[best_norm]](X_imputed)
best_impnorm[, numeric_cols] <- X_final

res.pca <- FactoMineR::PCA(
  best_impnorm[, 5:ncol(best_impnorm)],
  scale.unit = FALSE,
  graph = FALSE)

pca_best_impnorm <- factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            # select.var = list(contrib = 3), # Select top x features with highest contribution to PC1/PC2
                            repel = TRUE,
                            labelsize = 5, 
                            habillage = factor(best_impnorm$gas_station), # "sampling_season", "Octane_rating", "gas_station", "Sample_name", "supplier"
                            addEllipses = TRUE,
                            ellipse.level = 0.95,
                            # xlim = c(-0.2, 0.2),
                            # ylim = c(-0.009, 0.009),
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = ""
) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(20, 'mm')) +
  theme(legend.position = 'bottom',
        panel.spacing = unit(0, "lines"))

ggsave(filename = paste0("Figure 3_PCA_plot","_worst_impnorm_by_gasstation_", format(Sys.time(), "%d-%m-%Y"),".png"),
       plot = pca_worst_impnorm,
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 13,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in")
```


### Barplot of Top 10 Variable contributions to first n dimensions

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Variable contributions to first n dimensions
## To PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
## To PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)

# Extract top 10 contribution to PC1
dim_df <- as.data.frame(res.pca$var$contrib) %>% 
  rownames_to_column(var = "Feature") %>%
  arrange(desc(Dim.1), desc(Dim.2))

# Extract top x highest loadings
top_pca_loadings <- dim_df[1:25,]$Feature
```

### Cluster resolution
```{r, echo = FALSE, message = FALSE, warning = FALSE}
data <- df_percentage[, -c(1,2,3,4,5)] # %>% 
    # dplyr::select(unique(sig_comp[[5]]$Feature))
group <- factor(df_percentage$supplier)
max_resolution <- 0
resolutions <- c()

pca_model <- prcomp(data, center = TRUE, scale. = FALSE)
scores <- pca_model$x

group_levels <- levels(group)
pair_resolutions <- c()

# Calculate cluster resolution for each pair of groups
for (g1 in 1:(length(group_levels) - 1)) {
  for (g2 in (g1 + 1):length(group_levels)) {
    data1 <- scores[group == group_levels[g1], 1:2]
    data2 <- scores[group == group_levels[g2], 1:2]
    resolution <- calculate_cluster_resolution(data1, data2)
    pair_resolutions <- c(pair_resolutions, resolution)
  }
}

avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
resolutions <- c(resolutions, avg_resolution)
print(resolutions)
```

# RANDOM FOREST
```{r , echo=FALSE, warning = FALSE, message=FALSE}
recursive_feature_addition <- function(X_train, y_train,
                                       X_test,  y_test,
                                       ntree_candidates,
                                       metric) {
  # 1) Fit full model to get initial feature importance
  full_res <- tune_rf_subset(
    X_train, y_train,
    X_test,  y_test, ntree_candidates, metric
  )
  # Extract importance from the randomForest final model
  imp_mat <- importance(full_res$model$finalModel)
  # Choose first top feature by MeanDecreaseAccuracy (or use MeanDecreaseGini) to use as base feature to initiate RFA
  init_feat <- rownames(imp_mat)[which.max(imp_mat[, "MeanDecreaseAccuracy"])]

  # Initialize feature sets
  best_feats <- init_feat
  remaining  <- setdiff(colnames(X_train), best_feats)

  # 2) Baseline performance with the single top feature
  base_res      <- tune_rf_subset(
    X_train[, best_feats, drop = FALSE], y_train,
    X_test [, best_feats, drop = FALSE], y_test,
    ntree_candidates, metric
  )
  best_accuracy <- base_res$Accuracy
  best_kappa    <- base_res$Kappa
  best_auc      <- base_res$AUC

  # 3) Iterative addition of remaining features
  while(length(remaining) > 0) {
    improved <- FALSE
    for(feat in remaining) {
      trial_feats <- c(best_feats, feat)
      res <- tune_rf_subset(
        X_train[, trial_feats, drop = FALSE], y_train,
        X_test [, trial_feats, drop = FALSE], y_test, ntree_candidates, metric
      )
      # Add feature only if all three metrics improve
      if(res$Accuracy > best_accuracy &&
         res$Kappa    > best_kappa    &&
         res$AUC      > best_auc) {
        best_accuracy <- res$Accuracy
        best_kappa    <- res$Kappa
        best_auc      <- res$AUC
        best_feats    <- trial_feats
        improved      <- TRUE
        break
      }
    }
    # Drop the last-tested feature from remaining
    remaining <- setdiff(remaining, feat)
    if(!improved) break
  }

  # 4) Final model on the chosen feature set
  final_res <- tune_rf_subset(
    X_train[, best_feats, drop = FALSE], y_train,
    X_test [, best_feats, drop = FALSE], y_test,
    ntree_candidates, metric
  )

  # 5) Return performance and selected features
  list(
    best_accuracy    = final_res$Accuracy,
    best_kappa =  final_res$Kappa,
    best_auc = final_res$AUC,
    best_features_rf = best_feats,
    final_model      = final_res$model,
    prob_matrix = final_res$p_m,
    predictions = final_res$preds,
    eval_metrics = final_res$eval_metrics
  )
}

#─────────────────────────────────────────────────────────────────────────────
# Helper: retune mtry (based on subset size) + ntree for any train/test split
tune_rf_subset <- function(X_train, y_train, X_test, y_test,
                           ntree_candidates, metric) {
  # 1) Check class counts
  k_inner <- min(table(y_train))
  
  # 2) Build the mtry grid
  p        <- ncol(X_train)
  mtry_vals <- unique(pmax(1, c(floor(sqrt(p)), floor(log2(p)))))
  
  # 3) Branch: enough data for CV?
  if (k_inner > 1) {
    # —> use caret CV
    ctrl <- trainControl(
      method         = "cv",
      number         = k_inner,
      classProbs     = TRUE,
      summaryFunction= defaultSummary
    )
    rf_grid <- expand.grid(mtry = mtry_vals)
    
    best_mod   <- NULL
    best_val   <- -Inf
    best_trees <- NA
    
    for (nt in ntree_candidates) {
      set.seed(123)
      tmp <- caret::train(
        x         = X_train,
        y         = as.factor(y_train),
        method    = "rf",
        trControl = ctrl,
        tuneGrid  = rf_grid,
        metric    = metric,
        ntree     = nt,
        importance= TRUE
      )
      this_val <- max(tmp$results[[metric]])
      if (this_val > best_val) {
        best_val   <- this_val
        best_mod   <- tmp
        best_trees <- nt
      }
    }
    
    caret_model <- best_mod
    final_rf    <- caret_model$finalModel
    
  } else {
    # —> too few per class: do OOB-based tuning
    best_oob   <- Inf
    best_mtry  <- NA
    best_trees <- NA
    best_rf    <- NULL
    
    for (nt in ntree_candidates) {
      for (m in mtry_vals) {
        set.seed(123)
        rf_mod <- randomForest(
          x          = X_train,
          y          = as.factor(y_train),
          mtry       = m,
          ntree      = nt,
          importance = TRUE
        )
        # final OOB error rate:
        oob_err <- tail(rf_mod$err.rate[ , "OOB"], 1)
        if (oob_err < best_oob) {
          best_oob   <- oob_err
          best_mtry  <- m
          best_trees <- nt
          best_rf    <- rf_mod
        }
      }
    }
    # wrap into a dummy caret-like object so that downstream calls to predict(<train>,…) and predict(<train>, type="prob") continue to work.
    caret_model <- list(
      finalModel = best_rf,
      # so that later code that uses predict(<train>,…) still works:
      methods = list(predict = "randomForest")
    )
    final_rf <- best_rf
  }
  
  # 4) Predict & score
  preds       <- predict(final_rf, newdata = X_test, type = "response")
  prob_matrix <- as.matrix(predict(final_rf, newdata = X_test, type = "prob"))
  ev          <- eval_metrics(true_labels = y_test, 
                                   prob_matrix = prob_matrix, 
                                   pred_labels = preds)
  
  return(list( model        = caret_model,
               ntree        = best_trees,
               preds        = preds,
               p_m          = prob_matrix,
               eval_metrics = ev,
               Accuracy = ev$Accuracy,
               Kappa = ev$Kappa,
               AUC = ev$AUC,
               F1_Macro     = ev$F1_Macro,
               F1_Weighted = ev$F1_Weighted,
               MCC          = ev$MCC_Multiclass
  ))
}

#─────────────────────────────────────────────────────────────────────────────
library(caret)
library(pROC)

eval_metrics <- function(true_labels, prob_matrix, pred_labels){
  # 0) Ensure factor levels align
  classes <- union(levels(true_labels), levels(pred_labels))
  y_true <- factor(true_labels, levels = classes)
  y_pred <- factor(pred_labels,  levels = classes)

  # 1) Build the confusion‐matrix table
  cm <- table(Actual = y_true, Predicted = y_pred)
  N  <- sum(cm)                       # total samples
  diag_cm <- diag(cm)

  # 2) Accuracy
  accuracy <- sum(diag_cm) / N

  # 3) Cohen’s Kappa (via caret)
  kappa <- tryCatch({
    cm_obj <- confusionMatrix(y_pred, y_true)
    as.numeric(cm_obj$overall["Kappa"])
  }, error = function(e) NA)

  # 4) One‐vs‐all multiclass AUC
  auc <- tryCatch({
    roc_obj <- multiclass.roc(response  = true_labels,
                              predictor = prob_matrix)
    as.numeric(roc_obj$auc)
  }, error = function(e) NA)

  # 5) Per‐class precision & recall
  col_sums <- colSums(cm)   # predicted totals for each class
  row_sums <- rowSums(cm)   # actual   totals for each class

  precision_per_class <- ifelse(col_sums > 0,
                                diag_cm / col_sums,
                                0)        # no preds → p=0
  recall_per_class    <- ifelse(row_sums > 0,
                                diag_cm / row_sums,
                                0)        # no actual → r=0

  # 6) Per‐class F1 & weighted F1
  f1_per_class <- ifelse(
    (precision_per_class + recall_per_class) > 0,
    2 * precision_per_class * recall_per_class /
      (precision_per_class + recall_per_class),
    0
  )
  support <- row_sums   # how many true examples of each class
  f1_weighted <- sum(f1_per_class * support) / sum(support)

  # 7) Multiclass MCC (Gorodkin’s formula)
  t_i <- row_sums
  p_j <- col_sums
  c_val <- sum(diag_cm)         # sum of true positives
  num   <- N * c_val - sum(t_i * p_j) # (TP×TN)-(FP×FN)
  den   <- sqrt((N^2 - sum(t_i^2)) * (N^2 - sum(p_j^2))) # sqrt((TP+FP)×(TP+FN)×(TN+FP)×(TN+FN)) 
  mcc   <- ifelse(den > 0, num / den, 0)

  return(list(
    Accuracy         = accuracy,
    Kappa            = kappa,
    AUC              = auc,
    F1_Weighted      = f1_weighted,
    MCC_Multiclass   = mcc
  ))
}

####################################################
##########        CONFUSION MATRIX       ###########
####################################################
conf_mat_plot <- function(y_test, preds, conf_mat_title, accuracy) {
  # 1) Compute raw counts table: rows = treated labels, cols = predicted pristine labels
  all_classes <- unique(c(levels(preds), levels(y_test)))
  y_f <- factor(y_test,  levels = all_classes)
  p_f <- factor(preds,  levels = all_classes)
  
  cm_tab <- table(Actual = y_f, Predicted = p_f)
  
  # 2) Turn it into a data.frame for ggplot
  cm_df <- as.data.frame(cm_tab, stringsAsFactors = FALSE) %>%
    dplyr::rename(Freq = Freq)
  
  # 3) Add total-per-row and percent-per-cell
  cm_df$Total <- NA
  cm_df$Percent <- NA
  for (label in unique(cm_df$Actual)){
    cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
  }
  
  cm_df <- cm_df %>%
    mutate(
      Percent = round(Freq / Total * 100, 2)
    )
  
  
  cm_df <- cm_df %>%
    mutate(
      Label = ifelse(Predicted == Actual,
                     paste0(round(Percent,1),"%"),
                     ""
      )
    )
  
  # 5) Plot
  conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
    geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
    # explicitly drop any size guide, keep only the fill (Recall) legend
    guides(size = "none") +
    labs(
      title = paste0("Confusion Matrix | ", conf_mat_title),
      subtitle = paste0(
        sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
        # # " Recall per treated class (N=", cm_df$Total[1], 
        # " replicates each)"
      ),
      x = "Predicted Class",
      y = "Actual Class",
      fill = "Percentage of Classification (%)" # "Recall %"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      axis.text.x  = element_text(angle = 45, hjust = 1),
      plot.title   = element_text(face = "bold", hjust = 0.5),
      plot.subtitle= element_text(hjust = 0.5)
    )
  return(conf_mat)
}

#─────────────────────────────────────────────────────────────────────────────

run_rf_analysis_arson <- function(data,
                                  type_col,
                                  remove_cols,
                                  train_proportion,
                                  seed             = 123,
                                  ntree_candidates = c(100, 500, 1000, 2500),
                                  metric           = "Accuracy",
                                  do_rfe           = FALSE,
                                  rfe_folds        = 5,
                                  do_sig           = FALSE,
                                  do_rfa           = FALSE,
                                  do_impute_norm_screen = TRUE,
                                  split_with_miscunknown_supplier = FALSE,
                                  excluding_supplier,
                                  group_for_significance,                     
                                  metadata,
                                  target_comp,
                                  start_col_index) {

  # 0) Optional set seed
  if(!is.null(seed)) set.seed(seed)
  
  # 1) Factorize the target column and remove classes with only one sample
  data[[type_col]] <- as.factor(data[[type_col]])
  levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique=TRUE)
  
  # Remove single-member classes
  class_counts  <- table(data[[type_col]])
  keep_classes  <- names(class_counts[class_counts > 1])
  data          <- data[data[[type_col]] %in% keep_classes, ]
  data[[type_col]] <- droplevels(data[[type_col]])
  cat("NAs in original data:", sum(is.na(data)), "\n")

  data <- data %>% 
    tibble::column_to_rownames(., "Sample_name") 
  rownames(data) <- paste0(rownames(data), "_", data$supplier)
  
  # 2) If do_impute_norm_screen=TRUE, find best combination
  best_imp  <- "No_imputation"
  best_norm <- "No_normalization"
  if(do_impute_norm_screen){
    cat("\n### Screening best Imputation + Normalization combo ###\n")
    start_time_imp_norm <- Sys.time()
    # Identify numeric columns from predictors (excluding remove_cols and target)
    numeric_cols <- names(which(sapply(data, is.numeric)))
    numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))
    
    best_res <- find_best_impute_normalize_new(
      data,
      type_col = type_col,
      remove_cols = remove_cols,
      group_for_significance = group_for_significance, 
      train_proportion = train_proportion,
      split_with_miscunknown_supplier = split_with_miscunknown_supplier,
      metadata = metadata,
      target_comp = target_comp,
      start_col_index = start_col_index)
    cat("\n*** Summary of all combos ***\n")
    print(best_res$results_table)
    
    cat("\n*** Best combo ***\n")
    print(best_res$best_combo)
    
    best_imp  <- best_res$best_combo$Imputation
    best_norm <- best_res$best_combo$Normalization
    
    X_original   <- data[, numeric_cols, drop = FALSE]
    X_imputed    <- imputation_methods[[best_imp]](X_original)
    X_final      <- normalization_methods[[best_norm]](X_imputed)
    data[, numeric_cols] <- X_final
    
  } else {
    cat("\n### Skipping impute+norm screen; using raw data ###\n")
  }
  
  cat("NAs after final imputation+normalization:", sum(is.na(data)), "\n")
  end_time_imp_norm   <- Sys.time()
  time_imp_norm      <- as.numeric(difftime(end_time_imp_norm,
                                                  start_time_imp_norm, 
                                                  units="secs"))
  
  # 3) Now proceed with your normal train/test code
  # X <- data %>%
  #   dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  
  y <- data[[type_col]]
  
  
  cat("\n### Using Original Train/Test Split ###\n")
  
  set.seed(123) # set.seed(sample.int(999, 1))
  train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
  train_data <- data[train_index, ]
  test_data  <- data[-train_index, ]
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 5) Grid for mtry
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  # 6) Grid search over (mtry, ntree)
  cat("\n### Train/test with Full original features ###\n")
  start_time_full_features <- Sys.time()
  
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test,
    ntree_candidates, metric
  )
  
  best_model_all_feats      <- all_feats$model$finalModel
  acc_full        <- all_feats$Accuracy
  
  # --- Generate Heatmap (Full Features) ---
  {prob_matrix_all_feats <- all_feats$p_m
   conf_mat_all_feats <- conf_mat_plot(y_test, all_feats$preds, conf_mat_title = paste0("All Features - ", type_col), accuracy = acc_full) 
    # pheatmap(
    #   prob_matrix,
    #   fontsize = 20,
    #   fontsize_number = 30,
    #   cluster_rows    = FALSE,
    #   cluster_cols    = FALSE,
    #   color           = viridis(100),
    #   display_numbers = TRUE,
    #   number_format   = "%.2f",
    #   main = paste("Classification Probability (All Features) -", sprintf("| Test Accuracy: %.2f", acc_full)),
    #   breaks = seq(0, 1, length.out = 101)
    # )
  }
  
  eval_metrics_all_feats <- all_feats$eval_metrics
  
  end_time_full_features   <- Sys.time()
  time_full_features       <- as.numeric(difftime(end_time_full_features,
                                                  start_time_full_features, 
                                                  units="secs"))
  
  # 9) Feature Importances
  # rf_importances         <- as.data.frame(final_rf$importance)
  # rf_importances$Feature <- rownames(rf_importances)
  # rf_importances         <- rf_importances[order(rf_importances$MeanDecreaseGini, decreasing=TRUE), ]
  
  # Significance-Based Feature Selection using pairwise tests
  if(do_sig){
    cat("\n### Train/test with Pair-wise features ###\n")
    acc_sig <- NA; sig_feats <- NULL; final_sig_model <- NULL
    start_time_sig <- Sys.time()
    
    pw <- pairwise_significance_tests_modified(
      input_df        = data,
      group_col       = group_for_significance,
      start_col_index = start_col_index,
      metadata        = metadata,
      target_comp     = target_comp
    )
    
    idx   <- which.max(sapply(pw$corrected,
                              function(df) length(unique(df$Feature))))
    feats0<- unique(pw$corrected[[idx]]$Feature)
    sig_feats <- intersect(feats0, colnames(X_train))
    if(length(sig_feats)>0) {
      tmp_sig <- tune_rf_subset(
        X_train[, sig_feats, drop=FALSE], y_train,
        X_test [, sig_feats, drop=FALSE], y_test,
        ntree_candidates, metric
      )
      acc_sig         <- tmp_sig$Accuracy
      final_sig_model <- tmp_sig$model$finalModel
    }

    {prob_matrix_sig <- tmp_sig$p_m
      conf_mat_sig <- conf_mat_plot(y_test, tmp_sig$preds, conf_mat_title = paste0("Pair-wise Significance-Based Features - ", type_col), accuracy = acc_sig)
      # pheatmap(
      #   prob_matrix_sig,
      #   fontsize = 20,
      #   fontsize_number = 30,
      #   cluster_rows    = FALSE,
      #   cluster_cols    = FALSE,
      #   color           = viridis(100),
      #   display_numbers = TRUE,
      #   number_format   = "%.2f",
      #   main = paste("Classification Probability (Pair-wise Significance-Based Features) -", sprintf("| Test Accuracy: %.2f", acc_sig)),
      #   breaks = seq(0, 1, length.out = 101)
      # )
    }
    
    eval_metrics_sig <- tmp_sig$eval_metrics
    
    end_time_sig   <- Sys.time()
    time_sig       <- as.numeric(difftime(end_time_sig, start_time_sig, units="secs"))
  }
  
  
  # Recursive Feature Addition
  if(do_rfa){
    cat("\n### Train/test with RFA features ###\n")
    start_time_rfa <- Sys.time()
    
    rfa_res   <- recursive_feature_addition(
      X_train, y_train,
      X_test,  y_test,
      ntree_candidates = ntree_candidates,
      metric           = metric
    )
    
    rfa_selected_feats <- rfa_res$best_features_rf
    acc_rfa                 <- rfa_res$best_accuracy
    final_rf_rfa            <- rfa_res$final_model$finalModel
    
    prob_matrix_rfa <- rfa_res$prob_matrix
    
    conf_mat_rfa <- conf_mat_plot(y_test, rfa_res$predictions, conf_mat_title = paste0("RFA Features - ", type_col), accuracy = acc_rfa)
    
    # {pheatmap(
    #     prob_matrix_rfa,
    #     fontsize = 20,
    #     fontsize_number = 30,
    #     cluster_rows    = FALSE,
    #     cluster_cols    = FALSE,
    #     color           = viridis(100),
    #     display_numbers = TRUE,
    #     number_format   = "%.2f",
    #     main = paste("Classification Probability (Recursive Feature Addition) -", sprintf("| Test Accuracy: %.2f", acc_rfa)),
    #     breaks = seq(0, 1, length.out = 101)
    #   )
    # }
    
    eval_metrics_rfa <- rfa_res$eval_metrics
    
    end_time_rfa   <- Sys.time()
    time_rfa       <- as.numeric(difftime(end_time_rfa, start_time_rfa, units="secs"))
  }
  
  # RFE with hyperparams retuning
  if(do_rfe){
    cat("\n### Train/test with RFE features ###\n")
    acc_rfe <- NA; rfe_feats <- NULL; final_rfe_model <- NULL
    start_time_rfe <- Sys.time()
    
    myFuncs <- rfFuncs
    myFuncs$fit <- function(x, y, first, last, ...) {
      res <- tune_rf_subset(
        X_train = x, y_train = y,
        X_test  = x, y_test  = y,
        ntree_candidates, metric
      )
      res$model$finalModel
    }
    
    # compute the smallest class count
    rfe_k <- min(table(train_data[[type_col]]))
    rfe_k <- if (rfe_k > 1) rfe_k else 2
    
    rfe_ctl <- rfeControl(
      functions   = myFuncs,
      method      = "cv",
      number      = rfe_k,
      saveDetails = TRUE,
      returnResamp= "final"
    )
    rfe_out <- rfe(
      x          = X_train,
      y          = y_train,
      sizes      = seq_len(ncol(X_train)),
      rfeControl = rfe_ctl
    )
    rfe_feats <- rfe_out$optVariables
    
    # final retune+test on RFE set
    tmp_rfe <- tune_rf_subset(
      X_train[, rfe_feats, drop=FALSE], y_train,
      X_test[, rfe_feats, drop=FALSE], y_test,
      ntree_candidates, metric
    )
    acc_rfe          <- tmp_rfe$Accuracy
    final_rfe_model  <- tmp_rfe$model$finalModel

    # --- Generate Heatmap (RFE) ---
    {prob_matrix_rfe <- tmp_rfe$p_m
      conf_mat_rfe <-  conf_mat_plot(y_test, tmp_rfe$preds, conf_mat_title = paste0("RFE Features - ", type_col), accuracy = acc_rfe)
      # pheatmap(
      #   prob_matrix_rfe,
      #   fontsize = 20,
      #   fontsize_number = 30,
      #   cluster_rows    = FALSE,
      #   cluster_cols    = FALSE,
      #   color           = viridis(100),
      #   display_numbers = TRUE,
      #   number_format   = "%.2f",
      #   main = paste("Classification Probability (RFE Features) -", sprintf("|  Test Accuracy: %.2f", acc_rfe)),
      #   breaks = seq(0, 1, length.out = 101)
      # )
      
    }
    
    eval_metrics_rfe <- tmp_rfe$eval_metrics
    
    end_time_rfe   <- Sys.time()
    time_rfe       <- as.numeric(difftime(end_time_rfe, start_time_rfe, units="secs"))
  }
  
  cat("\nFull-Feature Balanced Test Accuracy:", acc_full, "\n")
  if(do_rfe) cat("\nRFE-Optimized Balanced Test Accuracy:", acc_rfe, "\n")
  if(do_sig) cat("\nPairwise Selected Features Balanced Test Accuracy:", acc_sig, "\n")
  if(do_rfa) cat("\nRecursive Feature Addition Test Accuracy:", acc_rfa, "\n")

  # Return a list of final results
  result_list <- list(
    final_imp_norm_dat    = data,
    final_rf_all_feats      = best_model_all_feats,
    prob_matrix_all_feats = prob_matrix_all_feats,
    all_features_acc     = acc_full,
    time_full_features = time_full_features,
    eval_metrics_all_feats = eval_metrics_all_feats,
    conf_mat_all_feats = conf_mat_all_feats
  )
  
  if(do_impute_norm_screen) {
    result_list$time_imp_norm <- time_imp_norm
    result_list$imp_norm_res_table    <- best_res$results_table
    result_list$best_imputatation     <-  best_imp
    result_list$best_normalization    <- best_norm
  }
  
  if(do_sig){
    result_list$sig_model    <- final_sig_model
    result_list$sig_selected_feats <- sig_feats
    result_list$prob_matrix_sig <- prob_matrix_sig
    result_list$acc_sig <- acc_sig
    result_list$time_sig          <- time_sig
    result_list$eval_metrics_sig <- eval_metrics_sig
    result_list$conf_mat_sig <- conf_mat_sig
  }
  
  if(do_rfa) {
    result_list$rfa_model <- final_rf_rfa
    result_list$rfa_selected_feats <- rfa_selected_feats
    result_list$prob_matrix_rfa    <- prob_matrix_rfa
    result_list$acc_rfa  <- acc_rfa
    result_list$time_rfa                <- time_rfa
    result_list$eval_metrics_rfa <- eval_metrics_rfa
    result_list$conf_mat_rfa <- conf_mat_rfa
  } 
  if(do_rfe) {
    result_list$rfe_model             <- rfe_out
    result_list$final_rf_rfe          <- final_rfe_model
    result_list$rfe_selected_features <- rfe_feats
    result_list$prob_matrix_rfe    <- prob_matrix_rfe
    result_list$acc_rfe  <- acc_rfe
    result_list$time_rfe              <- time_rfe
    result_list$eval_metrics_rfe <- eval_metrics_rfe
    result_list$conf_mat_rfe <- conf_mat_rfe
  }
  
  return(result_list)
}
```


```{r , echo=FALSE, warning = FALSE, message=FALSE}
target_comp <- read_xlsx(path = paste0(getwd(), "/python/Shortened ILR Compound List PF001A 07-06-2024_Huy_modified_08Aug2024.xlsx"))

input_rf <- metadata_annotatedbyCaleb %>%
  dplyr::select(-c("RT1", "RT2", "Ion1", "Ion2", 
                   "Stauffer_chemical_group", "Stauffer_compound_name", "Chemical_group")) %>%
  pivot_longer(cols = 2:ncol(.), 
               names_to = "Sample_name", 
               values_to = "Peak_Area") %>%
  mutate(
    Octane_rating = case_when(
      str_detect(Sample_name, "A") ~ "Gas 87",
      str_detect(Sample_name, "B") ~ "Gas 89",
      str_detect(Sample_name, "C") ~ "Gas 91",
      TRUE ~ "Gas 94"
    ),
    sampling_season = case_when(
      str_detect(Sample_name, "b") ~ "blue",
      str_detect(Sample_name, "p") ~ "purple",
      TRUE ~ "orange"
    ),
    gas_station = case_when(
      str_detect(Sample_name, "F001") ~ "Station 1",
      str_detect(Sample_name, "F002") ~ "Station 2",
      str_detect(Sample_name, "F003") ~ "Station 3",
      str_detect(Sample_name, "F004") ~ "Station 4",
      str_detect(Sample_name, "F005") ~ "Station 5",
      str_detect(Sample_name, "F006") ~ "Station 6",
      str_detect(Sample_name, "F007") ~ "Station 7",
      str_detect(Sample_name, "F008") ~ "Station 8",
      str_detect(Sample_name, "F009") ~ "Station 9",
      TRUE ~ "Station 10"
    ),
    supplier = case_when(
      str_detect(Sample_name, "F001") ~ "Unbranded", # "Miscellanous",
      str_detect(Sample_name, "F002") ~ "Unbranded", # "Miscellanous",
      str_detect(Sample_name, "F003") ~ "Shell",
      str_detect(Sample_name, "F004") ~ "Cenovus",
      str_detect(Sample_name, "F005") ~ "Unbranded", # "Miscellanous",
      str_detect(Sample_name, "F006") ~ "Suncor",
      str_detect(Sample_name, "F007") ~ "Imperial",
      str_detect(Sample_name, "F008") ~ "Burnaby",
      str_detect(Sample_name, "F009") ~ "Imperial",
      TRUE ~ "Unbranded"
    )
  ) %>%
  relocate(Octane_rating, sampling_season, gas_station, supplier, .after = 1) %>% 
  pivot_wider(names_from = "Feature", values_from = "Peak_Area") #%>% 
  # column_to_rownames(., var = "Sample_name")


######################################################
# rf_results_by_supplier_df_no_na_list <- list()
rf_results_by_gas_station_with_miscnunknown_list <- list()
rf_results_by_supplier_with_miscnunknown_list <- list()

for (i in 1:10) {
  # rf_results_by_supplier_df_no_na_list[[i]] <- run_rf_analysis_arson(df_no_na %>%
  #                                                                      dplyr::filter(supplier %notin% "Unbranded"),
  #                                                                    train_proportion = 0.8,
  #                                                                    type_col = "supplier", 
  #                                                                    remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
  #                                                                    group_for_significance = "supplier",
  #                                                                    metadata = core_metadata,
  #                                                                    target_comp = target_comp, 
  #                                                                    start_col_index = 5,
  #                                                                    do_rfe = TRUE,
  #                                                                    do_gbfe = TRUE,
  #                                                                    do_sig = TRUE,
  #                                                                    do_rfa = TRUE)
  
    rf_results_by_gas_station_with_miscnunknown_list[[i]] <- run_rf_analysis_arson(input_rf,
                                                                                   train_proportion = 0.66,
                                                                                   type_col = "gas_station",
                                                                     remove_cols = c("sampling_season", "supplier", "Octane_rating"),
                                                                     group_for_significance = "gas_station",
                                                                     metadata = core_metadata,
                                                                     target_comp = target_comp,
                                                                     start_col_index = 5,
                                                                     do_rfe = TRUE,
                                                                     do_sig = TRUE,
                                                                     do_rfa = TRUE)
    
    rf_results_by_supplier_with_miscnunknown_list[[i]] <- run_rf_analysis_arson(input_rf,
                                                                                train_proportion = 0.8,
                                                                                type_col = "supplier",
                                                                     remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
                                                                     group_for_significance = "supplier",
                                                                     metadata = core_metadata,
                                                                     target_comp = target_comp,
                                                                     start_col_index = 5,
                                                                     do_rfe = TRUE,
                                                                     do_sig = TRUE,
                                                                     do_rfa = TRUE)
}

# Use these for final manuscript
rf_results_by_gas_station_with_miscnunknown_rfe
rf_results_by_gas_station_with_miscnunknown_sig_rfa
rf_results_by_supplier_with_miscnunknown_rfe 
rf_results_by_supplier_with_miscnunknown_sig_rfa 

## Plot RFE-CV Accuracy against number of Variables
# lattice::trellis.par.set(caret::caretTheme())
rfe_cv_accuracy_plot <- plot(rf_results_by_supplier_with_miscnunknown_list[[4]]$rfe_model, 
                             type = c("g", "o"),
                             xlab = "Subset sizes")
bigfont_plot <- update(
  rfe_cv_accuracy_plot,
  par.settings = list(
    par.main.text  = list(cex = 2.2),
    par.xlab.text  = list(cex = 1.6),
    par.ylab.text  = list(cex = 1.6),
    axis.text      = list(cex = 1.3),
    par.strip.text = list(cex = 1.5)
  )
)
png(
  filename = paste0("Figure S3_RFE-CV Accuracy across different subset sizes_", "supplier_", format(Sys.time(), "%d-%m-%Y"),".png"),
  width    = 20,      # in inches
  height   = 8,       # in inches
  units    = "in",    # interpret width/height as inches
  res      = 300      # DPI
)

print(bigfont_plot)  # send the trellis object to the device
dev.off()

# SI Table S5-S7
## Table S5: RFE
metadata_annotatedbyCaleb[] %>%
  dplyr::filter(Feature %in% as.numeric(rf_results_by_gas_station_with_miscnunknown_rfe$rfe_selected_features))


## Table S6: Pairwise

## Table S7: RFA
```

# k-fold CV - All vs. Pairwise vs. RFA vs. RFE: This k-fold cv process was done on week of 13th June 2025
```{r , echo=FALSE, warning = FALSE, message=FALSE}
data = input_rf
type_col = "gas_station"
remove_cols = c("sampling_season", "supplier", "Octane_rating")
group_for_significance = "gas_station"
metadata = core_metadata
target_comp = target_comp
start_col_index = 5
do_rfe = TRUE
do_sig = TRUE
do_rfa = TRUE
rfe_folds = 5
ntree_candidates = c(100, 500, 1000, 2500)
metric           = "Accuracy"
set.seed(123)

data[[type_col]] <- as.factor(data[[type_col]])
levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique=TRUE)

# Remove single-member classes
class_counts  <- table(data[[type_col]])
keep_classes  <- names(class_counts[class_counts > 1])
data          <- data[data[[type_col]] %in% keep_classes, ]
data[[type_col]] <- droplevels(data[[type_col]])

data <- data %>% 
  tibble::column_to_rownames(., "Sample_name") 
rownames(data) <- paste0(rownames(data), "_", data$supplier)

numeric_cols <- names(which(sapply(data, is.numeric)))
numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))
X_original   <- data[, numeric_cols, drop = FALSE]
X_imputed    <- imputation_methods[["impute_random_uniform"]](X_original)
X_final      <- normalization_methods[["Percentage_Normalization"]](X_imputed)
data[, numeric_cols] <- X_final

min_class_count_all <- min(table(data[[type_col]]))
K <- 3 # ifelse(min_class_count_all > 1, min_class_count_all, 2)
# stratified folds on the full target vector
K_folds <- createFolds(data[[type_col]], k = K, returnTrain = TRUE)

res_list_5repeated3fold <- vector("list", length = K * 5) 
conf_mat_list_5repeated3fold <- vector("list", length = K * 5) 
i <- 1

for(f in seq_along(K_folds)) {
  # split indices
  train_idx <- K_folds[[f]]
  test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
  
  train_data <- data[train_idx, ]
  test_data  <- data[test_idx, ]
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 4) Cross-validation control
  min_class_count <- min(table(y_train))
  folds_inner <- ifelse(min_class_count > 1, min_class_count, 2)
  cv_ctrl <- trainControl(
    method     = "cv",
    number     = folds_inner,
    classProbs = TRUE
  )
  
  # 5) Grid for mtry
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  # 6) Grid search over (mtry, ntree)
  cat("\n### Train/test with Full original features ###\n")
  start_time_all_feats <- Sys.time()
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test,
    cv_ctrl, ntree_candidates, metric
  )
  end_time_all_feats   <- Sys.time()
  time_all_feats       <- as.numeric(difftime(end_time_all_feats, start_time_all_feats, units="secs"))
  
  res_list_5repeated3fold[[i]] <- data.frame(
    fold   = f,
    method = "All",
    all_feats$eval_metrics, 
    time = time_all_feats, 
    num_selected_feats = ncol(X_train)
  )
  conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, all_feats$preds, 
                                      conf_mat_title = paste0("All Features - ", 
                                                              type_col), 
                                      accuracy = all_feats$Accuracy); i <- i + 1
  
  # Significance-Based Feature Selection using pairwise tests
  cat("\n### Train/test with Pair-wise features ###\n")
  start_time_sig <- Sys.time()
  pw <- pairwise_significance_tests_modified(
    input_df        = data,
    group_col       = group_for_significance,
    start_col_index = start_col_index,
    metadata        = metadata,
    target_comp     = target_comp
  )
  
  idx   <- which.max(sapply(pw$corrected,
                            function(df) length(unique(df$Feature))))
  feats0<- unique(pw$corrected[[idx]]$Feature)
  sig_feats <- intersect(feats0, colnames(X_train))
  if(length(sig_feats)>0) {
    tmp_sig <- tune_rf_subset(
      X_train[, sig_feats, drop=FALSE], y_train,
      X_test [, sig_feats, drop=FALSE], y_test,
      cv_ctrl, ntree_candidates, metric
    )
    end_time_sig   <- Sys.time()
    time_sig       <- as.numeric(difftime(end_time_sig, start_time_sig, units="secs"))
    res_list_5repeated3fold[[i]] <- data.frame(
      fold   = f,
      method = "Pairwise",
      tmp_sig$eval_metrics,
      time = time_sig, 
      num_selected_feats = length(sig_feats)
    )
    conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, tmp_sig$preds, 
                                        conf_mat_title = paste0("Pair-wise Significance-Based Features - ", type_col), 
                                        accuracy = tmp_sig$Accuracy); i <- i + 1
  }
  
  # Recursive Feature Addition
  cat("\n### Train/test with RFA features ###\n")
  start_time_rfa <- Sys.time()
  rfa_base  <- pw
  base_feats<- intersect(unique(rfa_base$corrected[[idx]]$Feature),
                         colnames(X_train))
  
  rfa_res   <- recursive_feature_addition(
    X_train, y_train,
    X_test,  y_test,
    base_features    = base_feats,
    cv_ctrl          = cv_ctrl,
    ntree_candidates = ntree_candidates,
    metric           = metric
  )
  end_time_rfa   <- Sys.time()
  time_rfa       <- as.numeric(difftime(end_time_rfa, start_time_rfa, units="secs"))
  
  res_list_5repeated3fold[[i]] <- data.frame(
    fold   = f,
    method = "RFA",
    rfa_res$eval_metrics,
    time=time_rfa, 
    num_selected_feats = length(rfa_res$best_features_rf)
  )
  conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, 
                                      rfa_res$predictions, 
                                      conf_mat_title = paste0("RFA Features - ", type_col), 
                                      accuracy = rfa_res$best_accuracy); i <- i + 1
  
  # RFE with hyperparams retuning
  cat("\n### Train/test with RFE features ###\n")
  start_time_rfe <- Sys.time()
  myFuncs <- rfFuncs
  myFuncs$fit <- function(x, y, first, last, ...) {
    res <- tune_rf_subset(
      X_train = x, y_train = y,
      X_test  = x, y_test  = y,
      cv_ctrl, ntree_candidates, metric
    )
    res$model$finalModel
  }
  rfe_ctl <- rfeControl(
    functions   = myFuncs,
    method      = "cv",
    number      = rfe_folds,
    saveDetails = TRUE,
    returnResamp= "final"
  )
  rfe_out <- rfe(
    x          = X_train,
    y          = y_train,
    sizes      = seq_len(ncol(X_train)),
    rfeControl = rfe_ctl
  )
  rfe_feats <- rfe_out$optVariables
  
  # final retune+test on RFE set
  tmp_rfe <- tune_rf_subset(
    X_train[, rfe_feats, drop=FALSE], y_train,
    X_test[, rfe_feats, drop=FALSE], y_test,
    cv_ctrl, ntree_candidates, metric
  )
  
  end_time_rfe   <- Sys.time()
  time_rfe       <- as.numeric(difftime(end_time_rfe, start_time_rfe, units="secs"))
  
  res_list_5repeated3fold[[i]] <- data.frame(
    fold   = f,
    method = "RFE",
    tmp_rfe$eval_metrics,
    time = time_rfe, 
    num_selected_feats = length(rfe_feats)
  )
  conf_mat_list_5repeated3fold[[i]] <- conf_mat_plot(y_test, 
                                      tmp_rfe$preds, 
                                      conf_mat_title = paste0("RFE Features - ", type_col), 
                                      accuracy = tmp_rfe$Accuracy); i <- i + 1
}

results_df <- do.call(rbind, res_list_5repeated3fold)

# Summarize and compare feature selection -----------------------
# get overall means + SDs
summary_table <- results_df %>%
  group_by(method) %>%
  summarise(
    accuracy_mean = round(mean(Accuracy), 2),
    accuracy_sd   = round(sd(Accuracy), 2),
    kappa_mean    = round(mean(Kappa,    na.rm=TRUE), 2),
    kappa_sd      = round(sd(Kappa,      na.rm=TRUE), 2),
    auc_mean      = round(mean(AUC), 2),
    auc_sd        = round(sd(AUC), 2),
    time_mean     = round(mean(time), 2),
    time_sd       = round(sd(time), 2),
    num_selected_feats_mean = mean(num_selected_feats),
    num_selected_feats_sd   = sd(num_selected_feats)
  )

View(summary_table)

# boxplot of accuracy by method
plot <- ggplot(results_df, aes(x=method, y=num_selected_feats)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_bw(base_size = 20) +
  labs(x = "Feature selection methods", y = "Number of selected features") #, title="Accuracy across folds by method")
plot

ggsave(filename = paste0("Figure S3_box_plot_Number of selected features across five feature selection methods_", "gasoline_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

# boxplot of AUC by method
ggplot(results_df, aes(x=method, y=AUC)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_minimal(base_size = 20) +
  labs(x = "Feature selection methods",
       title="AUC across folds by method")


# reshape to wide: one row per fold, columns = methods
wide_acc <- results_df %>%
  select(fold, method, AUC) %>%
  tidyr::pivot_wider(names_from = method, values_from = AUC)

stats::kruskal.test(list(wide_acc$All, wide_acc$Pairwise, wide_acc$RFA, wide_acc$RFE))

# Friedman test (nonparametric, repeated measures)
# friedman.test(as.matrix(wide_acc[,-1]))

## Confusion matrix
# Best fold for each feature selection ==> All fold, RFE fold 2; pair fold, RFA fold 1
```

## Figure 4: Boxplot of MCC across feature selection methods
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# boxplot of MCC by method
plot <- ggplot(results_df, aes(x=method, y=MCC_Multiclass)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  geom_signif(comparisons=list(c("RFA","All"),c("RFA","Pairwise"),c("RFA","RFE")),
              test=t.test, test.args = list(paired = F), 
              map_signif_level = function(p) sprintf("p = %.5f", p),  # ← show numeric p-values  # map_signif_level=TRUE, 
              step_increase=0.17, textsize = 8, vjust = -0.15) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw(base_size = 27) +
  labs(x = "Feature selection methods", y = "MCC score"
       # title="MCC across folds by method"
  ) +
  theme(axis.text.x = element_text(size = 22, color = "black", face = "bold"),
        axis.text.y = element_text(size = 22, color = "black"),
        axis.title.x = element_text(margin = ggplot2::margin(t = 15)),  # add space
        axis.title.y = element_text(margin = ggplot2::margin(r = 15)),  # add space
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
plot

ggsave(filename = paste0("Figure 4_box_plot_MCC score across feature selection methods_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 
```

## Figure 5: Confusion matrix

==> y_test_and_preds_list_5repeated3fold was created as a placeholder to get all gas station classes on Actual Class (y-axis) and Predicted Class (x-axis).

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Build confusion df from your y_test_and_preds_list element -----------------
make_cm_df <- function(obj, actual_slot, pred_slot) {
  y_true <- obj[[actual_slot]]
  y_pred <- obj[[pred_slot]]

  # standardize to character and trim whitespace
  y_true_chr <- str_squish(as.character(y_true))
  y_pred_chr <- str_squish(as.character(y_pred))

  # include *all* label combos (even zero-count) to be safe
  all_lvls <- sort(unique(c(y_true_chr, y_pred_chr)))
  grid <- expand.grid(Actual = all_lvls, Predicted = all_lvls, stringsAsFactors = FALSE)

  cm_counts <- as.data.frame(
    table(Actual = y_true_chr, Predicted = y_pred_chr),
    stringsAsFactors = FALSE
  )

  # join with full grid so missing pairs become 0
  cm_full <- grid %>%
    left_join(cm_counts, by = c("Actual","Predicted")) %>%
    mutate(Freq = dplyr::coalesce(Freq, 0L)) %>%
    arrange(Actual, Predicted)

  cm_full
}

# All feats - y_test_and_preds_list_5repeated3fold[[15]] --------------------
fold_id <- 15
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "all_feats_preds")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "All features",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_All feats_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")

# Pairwise -------------------------------------
fold_id <- 15
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "tmp_sig_preds")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "Pairwise",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_Pairwise_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")

# RFE----------------------------
fold_id <- 14
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "tmp_rfe_preds")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "RFE",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_RFE_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")

# RFA ------------------------------------
fold_id <- 15
cm_df <- make_cm_df(y_test_and_preds_list_5repeated3fold[[fold_id]],
           actual_slot = "y_test",
           pred_slot = "rfa_res_predictions")

# 3) Add total-per-row and percent-per-cell
cm_df$Total <- NA
cm_df$Percent <- NA
for (label in unique(cm_df$Actual)){
  cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
}

cm_df <- cm_df %>%
  dplyr::mutate(Actual = factor(Actual, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  dplyr::mutate(Predicted = factor(Predicted, levels = c("Station.1",  "Station.2",  "Station.3", "Station.4",  "Station.5",  "Station.6",  "Station.7", "Station.8", "Station.9", "Station.10"))) %>%
  mutate(
    Percent = round(Freq / Total * 100, 2)
  )


cm_df <- cm_df %>%
  mutate(
    Label = ifelse(Predicted == Actual,
                   paste0(round(Percent,1),"%"),
                   ""
    )
  )

# 5) Plot
conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
  geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
  # explicitly drop any size guide, keep only the fill (Recall) legend
  guides(size = "none") +
  labs(
    title = "RFA",
    # subtitle = paste0(
    #   sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
    # # " Recall per treated class (N=", cm_df$Total[1], 
    # " replicates each)"
    x = "Predicted Class",
    y = "Actual Class",
    fill = "Percentage of\nClassification (%)" # "Recall %"
  ) +
  theme_minimal(base_size = 20) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(face = "bold", hjust = 0.5),
    plot.subtitle= element_text(hjust = 0.5)
  )
conf_mat

ggsave(filename = paste0("Figure 5_confusion matrix_RFA_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = conf_mat, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 5,       # Height in inches (adjust as needed)
       units = "in")
```

## Sensitivity Analysis  
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# nested the feature filtering as a part of the RF classifier
## gradient of 
```

# SI Table S5-S7: Find out identity of selected features
```{r , echo=FALSE, warning = FALSE, message=FALSE}
feature_df <- data.frame(Feature = unique(unlist(selected_features_5repeated3fold[seq(from = 4 , to = 60, by = 4)])))

rt1 <- c()
rt2 <- c()
Ion1 <- c()
Ion2 <- c()

for(r in 1:nrow(feature_df)) {
  feat <- as.numeric(feature_df[r, "Feature"])
  meta_row <- metadata[which(metadata$Feature == feat), ]
  if(nrow(meta_row) == 0) {
    rt1 <- c(rt1, NA)
    rt2 <- c(rt2, NA)
    Ion1 <- c(Ion1, NA)
    Ion2 <- c(Ion2, NA)
  } else {
    rt1 <- c(rt1, meta_row$RT1[1])
    rt2 <- c(rt2, meta_row$RT2[1])
    Ion1 <- c(Ion1, meta_row$Ion1[1])
    Ion2 <- c(Ion2, meta_row$Ion2[1])
  }
}

feature_df$RT1  <- rt1
feature_df$RT2  <- rt2
feature_df$Ion1 <- Ion1
feature_df$Ion2 <- Ion2

# Next, use the target_comp data frame to match compounds based on RT and ion values.
compound_name <- c()
chem_grp <- c()

for(r in 1:nrow(feature_df)) {
  idx <- which(abs(target_comp$RT1 - feature_df[r, "RT1"]) <= 0.1 &
                 abs(target_comp$RT2 - feature_df[r, "RT2"]) <= 0.1 &
                 abs(target_comp$Ion1 - feature_df[r, "Ion1"]) <= 0.5 &
                 abs(target_comp$Ion2 - feature_df[r, "Ion2"]) <= 0.5)
  if(length(idx) == 0) {
    chem_grp <- c(chem_grp, "unknown")
    compound_name <- c(compound_name, "unknown")
  } else {
    chem_grp <- c(chem_grp, target_comp$Group[idx[1]])
    compound_name <- c(compound_name, target_comp$Compound[idx[1]])
  }
}

feature_df$compound_name  <- compound_name
feature_df$Chemical_group <- chem_grp
View(feature_df)

write_xlsx(feature_df %>% 
             mutate(across(
               .cols = where(is.numeric),
               .fns  = ~ round(.x, 2)
             )), path = paste0("Table S5_RFE features_gas station classification",format(Sys.time(), "%d-%m-%Y"),".xlsx"))
```

## Figure S3: Comparison of computational time and number of selected features 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
## Computational time
plot <- ggplot(results_df, aes(x=method, y=time)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_bw(base_size = 20) +
  labs(x = "Feature selection methods", y = "Computational time")
plot

ggsave(filename = paste0("Figure S3_box_plot_Number of selected features across five feature selection methods_", "gasoline_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

## Number of selected features
plot <- ggplot(results_df, aes(x=method, y=num_selected_feats)) +
  geom_boxplot() +
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "black", linewidth = 1) +
  theme_bw(base_size = 20) +
  labs(x = "Feature selection methods", y = "Number of selected features")
plot

ggsave(filename = paste0("Figure S3_box_plot_Number of selected features across five feature selection methods_", "gasoline_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Feature without data compression &feature filtration-prioritization (n=797)_by_supplier_FullfeatureMode
n_797_rf_res_list <- list()
for (i in 1:20){
  n_797_rf_res_list[[i]] <- run_rf_analysis_arson(coredf %>%
                                                    dplyr::filter(supplier %notin% c("Unbranded")),
                                                  train_proportion = 0.8,
                                                  type_col = "supplier", 
                                                  remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
                                                  group_for_significance = "supplier",
                                                  metadata = core_metadata,
                                                  target_comp = target_comp)
}
# Feature appears in all samples (n=23)_by_supplier_FullfeatureMode
n_23_rf_res_list <- list()
for(i in 1:20) {
  n_23_rf_res_list[[i]] <- run_rf_analysis_arson(df_no_na %>%
                                                   dplyr::filter(supplier %notin% "Unbranded"),
                                                 train_proportion = 0.8,
                                                 type_col = "supplier", 
                                                 remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
                                                 group_for_significance = "supplier",
                                                 metadata = core_metadata,
                                                 target_comp = target_comp, 
                                                 do_rfe = TRUE,
                                                 do_gbfe = TRUE,
                                                 do_sig = TRUE,
                                                 do_rfa = TRUE)
  
}

# Feature after data compression &feature filtration-prioritization (n=230)_by_supplier_FullfeatureMode
n_230_rf_res_list <- list()
for(i in 1:20){
  n_230_rf_res_list[[i]] <- run_rf_analysis_arson(input_rf %>%
                                                    dplyr::filter(supplier %notin% c("Miscellanous", "unknown")),
                                                  train_proportion = 0.8,
                                                  type_col = "supplier", 
                                                  remove_cols = c("sampling_season", "gas_station", "Octane_rating"),
                                                  group_for_significance = "supplier",
                                                  metadata = core_metadata,
                                                  target_comp = target_comp)
  
}

n_797_acc <- c()
n_23_acc<- c()
n_230_acc<- c()

for (item in 1:20) {
  n_797_acc <- c(n_797_acc, n_797_rf_res_list[[item]]$full_features_acc)
  n_23_acc <- c(n_23_acc, n_23_rf_res_list[[item]]$full_features_acc)
  n_230_acc <- c(n_230_acc, n_230_rf_res_list[[item]]$full_features_acc)
}

# Kruskal-Wallis Rank Sum Test - Is there different in Accuracy between these 3 datasets
stats::kruskal.test(list(n_797_acc, n_23_acc, n_230_acc)) #=> there is no differences at all
## => there is no differences at all: Kruskal-Wallis chi-squared = 2.1326, df = 2, p-value = 0.3443

## box plot of accuracy for these 5 feature selection methods:
df <- data.frame("A (n = 797)" = n_797_acc,
                 "B (n = 230)" = n_230_acc,
                 "C (n = 23)" = n_23_acc)

# Reshape the data from wide to long format
df_long <- melt(df, 
                measure.vars = colnames(df),
                variable.name = "Technique",
                value.name = "Accuracy")

# Generate the boxplot using ggplot2
# In ggplot2, the default whiskers also extend to 1.5*IQR from the quartiles.
ggplot(df_long, aes(x = Technique, y = Accuracy)) +
  geom_boxplot(fill = "lightgreen", color = "darkgreen") +
  # Add a thicker line at the mean using a crossbar
  stat_summary(fun = median, 
               geom = "crossbar", 
               width = 0.75,      # Width of the crossbar
               fatten = 2,        # Emphasizes the line by increasing its thickness
               linewidth = 1.5,   # Use linewidth instead of size
               color = "black") +   # A color to highlight the mean
  # Add whisker caps at the lower whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats  # stats: lower whisker, Q1, median, Q3, upper whisker
    data.frame(ymin = stats[1], ymax = stats[1])
  }, geom = "errorbar", width = 0.2, color = "darkgreen", linewidth = 1) +
  # Add whisker caps at the upper whisker endpoint
  stat_summary(fun.data = function(x) {
    stats <- boxplot.stats(x)$stats
    data.frame(ymin = stats[5], ymax = stats[5])
  }, geom = "errorbar", width = 0.2, color = "darkgreen", linewidth = 1) +
  labs(
    #title = "Boxplot of 5 Techniques",
       x = "Datasets",
       y = "Test accuracy") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_discrete(labels = c("A..n...797." = "A (n = 797)",
                              "B..n...230."= "B (n = 230)",
                              "C..n...23." = "C (n = 23)")) +
  theme_minimal(base_size = 20)
```



```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(MASS)  # For mvrnorm
library(stats)  # For ANOVA

# Function to generate confidence ellipses
confidence_ellipse <- function(mean, cov, conf.level = 0.95) {
  radius <- sqrt(qchisq(conf.level, df = 2))
  eigen_decomp <- eigen(cov)
  angle <- atan2(eigen_decomp$vectors[2, 1], eigen_decomp$vectors[1, 1])
  a <- radius * sqrt(eigen_decomp$values[1])
  b <- radius * sqrt(eigen_decomp$values[2])
  list(mean = mean, angle = angle, a = a, b = b)
}

# Function to determine overlap of two ellipses
ellipses_overlap <- function(ellipse1, ellipse2) {
  # Calculate the Euclidean distance between the centers of the ellipses
  distance <- sqrt(sum((ellipse1$mean - ellipse2$mean)^2))
  critical_distance <- ellipse1$a + ellipse2$a
  return(distance < critical_distance)
}

# Function to calculate cluster resolution
calculate_cluster_resolution <- function(data1, data2, initial_conf = 0.75) {
  conf.level <- initial_conf
  overlap <- TRUE
  
  while (overlap) {
    if (conf.level <= 0 || conf.level >= 1) {
      break
    }
    ellipse1 <- confidence_ellipse(colMeans(data1), cov(data1), conf.level)
    ellipse2 <- confidence_ellipse(colMeans(data2), cov(data2), conf.level)
    overlap <- ellipses_overlap(ellipse1, ellipse2)
    if (is.na(overlap)) {
      break
    }
    if (overlap) {
      conf.level <- conf.level - 0.01
    } else {
      conf.level <- conf.level + 0.01
    }
  }
  
  return(conf.level)
}

# Function to perform ANOVA and rank variables from lowest to highest F
rank_variables_anova <- function(data, group) {
  anova_results <- apply(data, 2, function(x) anova(lm(x ~ group))$`Pr(>F)`[1])
  ranked_vars <- order(anova_results)
  return(ranked_vars)
}

# With rank variables from highest to lowest of their calculated mean and std., variables here already occur in >1 sample
# rank_variables_high_mean_std <- function(data) {
#   temp1 <- data[, -c(1,2,3,5)] %>% column_to_rownames(., var="Sample_name")
#   temp2 <- t(temp1)
#   colnames(temp2) <- rownames(temp1)
#   rownames(temp2) <- colnames(temp1)
#   return(as.numeric((as.data.frame(temp2) %>% 
#                        rownames_to_column(., var = "Feature") %>%
#                        ungroup() %>%
#                        rowwise() %>%
#                        mutate(mean_area = mean(c_across(2:ncol(.)), na.rm = TRUE),
#                               sd_area = sd(c_across(2:ncol(.)), na.rm = TRUE)) %>%
#                        ungroup() %>% 
#                        relocate(mean_area, sd_area, .before = 1) %>%
#                        arrange(desc(mean_area), desc(sd_area)))$Feature))
#          
# }

# Function to perform PCA and calculate cluster resolution for multiple groups
# variable_selection <- function(data, group, max_vars = 10) {
#   ranked_vars <- rank_variables_anova(data, group)
#   # ranked_vars <-  rank_variables_high_mean_std(data)
#   selected_vars <- c()
#   max_resolution <- 0
#   resolutions <- c()
#   
#   for (i in 1:max_vars) { # Backward feature elimination
#     selected_vars <- c(selected_vars, ranked_vars[i])
#     if (length(selected_vars) >= 2) {  # Ensure at least two variables are selected
#       pca_model <- prcomp(data[, as.character(selected_vars)], center = TRUE, scale. = TRUE)
#       scores <- pca_model$x
#       
#       group_levels <- levels(group)
#       pair_resolutions <- c()
#       
#       # Calculate cluster resolution for each pair of groups
#       for (g1 in 1:(length(group_levels) - 1)) {
#         for (g2 in (g1 + 1):length(group_levels)) {
#           data1 <- scores[group == group_levels[g1], 1:2]
#           data2 <- scores[group == group_levels[g2], 1:2]
#           resolution <- calculate_cluster_resolution(data1, data2)
#           pair_resolutions <- c(pair_resolutions, resolution)
#         }
#       }
#       
#       avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
#       resolutions <- c(resolutions, avg_resolution)
#     }
#   }
#   
#   return(list(selected_vars = selected_vars, resolutions = resolutions))
# }

# ----------------------------------------------------------------------------------------------
# input1 <- percentage_normalized_all # %>% filter(supplier %notin% c("Miscellanous", "unknown"))
# input2 <- input1[, 6:ncol(input1)]
# group <- factor(input1$gas_station)
# 
# # Perform variable selection
# max_vars <- ncol(input2)
# result <- variable_selection(input2, group, max_vars = max_vars)
# print(result)

# ----------------------------------------------------------------------------------------------
# Plot resolutions against number of variables
# resolutions_df <- data.frame(
#   num_vars = seq_along(result$resolutions),
#   resolution = result$resolutions
# )
# 
# ggplot(resolutions_df, aes(x = num_vars, y = resolution)) +
#   geom_line() +
#   geom_point() +
#   labs(title = "Impact of Each Variable on Cluster Resolution",
#        x = "Number of Variables Added",
#        y = "Cluster Resolution") +
#   theme_classic(base_size = 18) +
#   scale_x_continuous(limits = c(1, max_vars), breaks = seq(from = 0, to = max_vars, by = 25)) +
#   scale_y_continuous(limits = c(0, 1), breaks = seq(from = 0, 
#                                                     to = 1, 
#                                                     by = 0.1)) +
#   theme(plot.title = element_text(hjust = 0.5),
#         # axis.text.x = element_text(angle = 90),
#         panel.grid.minor.x = element_blank(),
#         panel.grid.minor.y = element_blank(),
#         panel.grid.major.x = element_blank(),
#         panel.grid.major.y = element_blank()
#   )

# df_0.001_percentage_normalized_resolution <- 0.1864444
# df_0.001_log_normalized_resolution <- 0.1162222
# min_df_percentage_normalized_resolution <- 0.1911111
# min_df_log_normalized_resolution <- 0.1306667
# df_lod_percentage_normalized <- 0.1808889
# df_lod_log_normalized <- 0.124
# mice_cart_df3_percentage_normalized <- 0.1493333
# mice_cart_df3_log_normalized <- 0.09666667
```


## Venn Diagram:
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(reshape2)
library(RColorBrewer)

setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Arson manuscript/Arson project/Rproject/Arson/comparison of feature grouping methods and data compression via pair-wise test/13th Dec")

sigcomp_list <- list.files(pattern = '*.xlsx', full.names = TRUE)

read_and_add_filename <- function(file) {
  if (!file.exists(file)) {
    warning(paste("File not found:", file))
    return(NULL)  # Skip missing files
  }
  df <- read_xlsx(file) %>%
    mutate(File = basename(file)) %>% 
    mutate(Feature = as.character(Feature))
  return(df)
}

# Apply the function to all files and combine them into a single dataframe
sigcomp <- lapply(sigcomp_list, read_and_add_filename) %>%
  bind_rows() %>%
  mutate(Feature_name = paste0(Chemical_group, "_", round(RT1, 2), "_", round(RT2, 2), "_", round(Ion1), "_", round(Ion2))) %>%
  mutate(File = gsub("\\.xlsx$", "", File))

data_matrix <- dcast(sigcomp, Feature_name ~ File, fun.aggregate = function(x) length(x) > 0, value.var = "File")

# Set row names (features) in the matrix
rownames(data_matrix) <- data_matrix[, 1]
data_matrix <- data_matrix[, -1]  # Remove the first column that is now the row names

# Convert the matrix to binary (1 for presence, 0 for absence)
data_matrix[data_matrix > 0] <- 1

data_matrix_df <- as.data.frame(data_matrix)
data_matrix_df$Feature_name <- rownames(data_matrix_df)  # Add the features as a colum
# Melt the data frame
data_melted <- melt(data_matrix_df, id.vars = "Feature_name")
# Ensure column names are correctly assigned
colnames(data_melted) <- c("Feature_name", "Filename", "Presence")

# Count how many files each feature appears in (sum of Presence column)
feature_frequency <- data_melted %>%
  group_by(Feature_name) %>%
  summarise(Frequency = sum(Presence)) %>%
  arrange(desc(Frequency))  # Sort by frequency (most to least)

# Reorder Feature levels based on Frequency
data_melted$Feature_name <- factor(data_melted$Feature_name, levels = feature_frequency$Feature_name)

# Identify features that appear in all files
features_all_files <- feature_frequency$Feature_name[feature_frequency$Frequency == ncol(data_matrix)]

# Plot heatmap with reordered y-axis
ggplot(data_melted, aes(x = Filename, y = Feature_name, fill = factor(Presence))) +
  geom_tile() +
  scale_fill_manual(values = c("white", "blue")) +  # Color scale for 0 and 1
  theme_minimal() +
  labs(title = "Feature Overlap Across Files", x = "Files", y = "Features", fill = "Presence") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),   # Rotate x-axis labels
    axis.text.y = element_text(size = 10),               # Adjust y-axis text size
    axis.title.y = element_text(size = 12),              # Adjust y-axis title size
    plot.margin = margin(1, 1, 1, 3)                     # Increase left margin to avoid overlap
  ) +
  coord_cartesian(clip = "off")

# Save the plot with a longer height to avoid overlap
ggsave("heatmap_plot.png", height = 20, width = 16, dpi = 600)
```

## NMDS 
**References:** 
https://eddatascienceees.github.io/tutorial-rayrr13/
https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/

```{r echo=FALSE, warning=FALSE, message=FALSE}
data <- percentage_normalized 

# Calculate the dissimilarity matrix using Bray-Curtis distance
dissimilarity_matrix <- vegdist(data %>% select(6:ncol(data)), method = "bray", na.rm = TRUE)

# Perform NMDS
nmds_result <- metaMDS(dissimilarity_matrix, k = 2, trymax = 100)

# Extract NMDS scores
nmds_scores <- as.data.frame(scores(nmds_result))

# Add metadata (Product_Name, Source, Day) to the NMDS scores
nmds_scores <- cbind(nmds_scores, data %>% select(supplier, sampling_season, Octane_rating, gas_station, Sample_name))

# Define a vector of shapes
shapes <- c(0:16) #, 21:24, 25:32, 33:40, 41:48, 49:56, 57:63)

# Generate a discrete color palette with 61 colors
# palette <- colorspace::qualitative_hcl(61, palette = "Dark 3")

# Plot the NMDS results using ggplot2
ggplot(nmds_scores, aes(x = NMDS1, y = NMDS2)) +
  geom_point(aes(
    color = sampling_season,
    shape = sampling_season), size = 3) +
  labs(title = "NMDS",
       x = "NMDS1", y = "NMDS2") +
  theme_minimal() +
  theme(legend.position = "right") +
  # scale_color_manual(name = "Source", values = palette) +
  scale_shape_manual(name = "Source", values = shapes)

# Report Stress values
stressplot(nmds_result)

# PERMANOVA results
## Reference: 
## https://nathanielwoodward.com/posts/a-cool-thing-called-permanova/
## https://onlinelibrary.wiley.com/doi/full/10.1002/9781118445112.stat07841

```

## HCA

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- normalized_df[, -c(1:5)]

## Dissimilarity Indices Calculated by vegan::vegdist()
hca_samp <- stats::hclust(vegan::vegdist(hc_df,
                                         method = "robust.aitchison")) # Since our data is continous -> canberra // manhattan // robust.aitchison

# png(paste0("HCA_plot_without_unknown_and _misc_supplier","_only_non-uniquefillNA0.001_TSN_sig_comp_nonunique", format(Sys.time(), "%d-%m-%Y"),".png"), width = 13, height = 8, units = "in", res = 600)

plot(hca_samp,
     labels = normalized_df$gas_station,
     hang = -1,
     main = "", cex = 1.25)

dev.off()
```    

## t-SNE

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# REFERENCES VISUALIZATION: 
# https://plotly.com/r/t-sne-and-umap-projections/
# https://distill.pub/2016/misread-tsne/

library(tsne)
library(plotly)

features <- subset(percentage_normalized, select = 5:ncol(percentage_normalized))

tsne <- tsne(features,
             initial_dims = 3, 
             k = 3, 
             perplexity = 30, # Hyperparameter: perplexity (optimal number of neighbors) < number of samples
             max_iter = 5000
             )

pdb <- cbind(data.frame(tsne), percentage_normalized$Octane_rating)
options(warn = -1)
tsne_plot <- plot_ly(data = pdb ,x =  ~X1, y = ~X2, z = ~X3, 
               color = ~percentage_normalized$Octane_rating) %>% 
  add_markers(size = 8) %>%
  layout( 
    xaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'), 
    yaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'),
    scene =list(bgcolor = "#e5ecf6"))

tsne_plot
```

## UMAP clustering 

```{r, echo=FALSE, warning = FALSE, message=FALSE}
input <- df_0.001_percentage_normalized %>% filter(supplier %notin% c("unknown", "Miscellanous"))

features <- input[, -c(1:5)]

umap <- umap(features, n_components = 3,
             alpha = 0.00001, gamma = 0.00001,
             metric = "pearson2") # euclidean, manhattan, cosine, pearson, pearson2

layout <- cbind(data.frame(umap[["layout"]]), input$gas_station)
umap_plot <- plot_ly(layout, x = ~X1, y = ~X2, z = ~X3, 
                color = ~input$gas_station) %>% 
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x-axis'),
                      yaxis = list(title = 'y-axis'),
                      zaxis = list(title = 'z-axis')))
umap_plot
```


# SIMCA
```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(chemometrics)
library(pls)
library(caret)

test <- percentage_normalized %>% filter(supplier %notin% "unknown")
X <- test[, -c(1:5)]  # feature matrix
Y <- factor(test$supplier, levels = unique(test$supplier))  # class labels

set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(Y, p = 0.8, list = FALSE)
X_train <- X[trainIndex, ]
Y_train <- Y[trainIndex]
X_test <- X[-trainIndex, ]
Y_test <- Y[-trainIndex]

# Function to fit PCA model for a given class
fit_pca_model <- function(class_data, ncomp) {
  pca_model <- prcomp(class_data, center = FALSE, scale. = FALSE)
  pca_model$x <- pca_model$x[, 1:ncomp]  # keep only ncomp components
  return(pca_model)
}

# Fit PCA models for each class
class_levels <- levels(Y_train)
pca_models <- list()

for (class in class_levels) {
  class_data <- X_train[Y_train == class, ]
  pca_models[[class]] <- fit_pca_model(class_data, ncomp = 2)  # example with 2 components
}

# Function to calculate residuals and scores for a sample
calculate_residuals_scores <- function(sample, pca_model) {
  scores <- as.matrix(sample) %*% pca_model$rotation
  residuals <- sample - (scores %*% t(pca_model$rotation))
  return(list(scores = scores, residuals = residuals))
}

# Apply the function to test data
results <- list()

for (i in 1:nrow(X_test)) {
  sample <- X_test[i, ]
  sample_results <- list()
  for (class in class_levels) {
    res <- calculate_residuals_scores(sample, pca_models[[class]])
    sample_results[[class]] <- res
  }
  results[[i]] <- sample_results
}

# Function to calculate Q-residuals and T² for a sample
calculate_criteria <- function(residuals, scores, pca_model) {
  Q_residuals <- sum(residuals^2)
  T2 <- sum((scores^2) / apply(pca_model$x, 2, var))
  return(list(Q_residuals = Q_residuals, T2 = T2))
}

# Calculate criteria for each sample and each class
criteria <- list()

for (i in 1:length(results)) {
  sample_criteria <- list()
  for (class in class_levels) {
    res <- results[[i]][[class]]
    crit <- calculate_criteria(res$residuals, res$scores, pca_models[[class]])
    sample_criteria[[class]] <- crit
  }
  criteria[[i]] <- sample_criteria
}

# Simple classification rule based on threshold values (adjust thresholds as needed)
classify_sample <- function(criteria, Q_threshold, T2_threshold) {
  for (class in names(criteria)) {
    if (criteria[[class]]$Q_residuals < Q_threshold && criteria[[class]]$T2 < T2_threshold) {
      return(class)
    }
  }
  return("unknown")
}

# Apply classification to each sample
predictions <- sapply(criteria, classify_sample, Q_threshold = 10, T2_threshold = 5)

# Compare predictions with true labels
confusionMatrix(factor(predictions, levels = class_levels), Y_test)

```


# Log-likelihood Ratio
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Load necessary libraries
library(MASS)
library(stats)
library(ks)

# Function to calculate peak area ratios
calculate_ratios <- function(data) {
  ratios <- data
  # Calculate all peak area ratios
  for (i in 1:ncol(data)) {
    for (j in 1:ncol(data)) {
      if (i != j) {
        ratios <- cbind(ratios, data[, i] / data[, j])
      }
    }
  }
  return(ratios)
}

# Function to compute F'
compute_F_prime <- function(ratios) {
  n <- nrow(ratios)
  F_prime <- rep(0, ncol(ratios))
  
  for (k in 1:ncol(ratios)) {
    R <- ratios[, k]
    R_mean <- apply(ratios, 1, mean)
    sdR <- sd(R)
    
    F_prime[k] <- (1 / n) * sqrt(sum((R / R_mean - 1)^2)) * (1 / sdR)
  }
  return(F_prime)
}

# Function to calculate distances d1
calculate_d1 <- function(ratios_stationA, ratios_stationB) {
  d1 <- sqrt(rowSums(log(ratios_stationA / ratios_stationB)^2))
  return(d1)
}

# Example usage
data <- matrix(rnorm(1000), nrow=100, ncol=10)  # Replace with your data
evaporation_levels <- c(0, 25, 50, 75)
ratios <- calculate_ratios(data)
F_prime <- compute_F_prime(ratios, evaporation_levels)
stable_ratios <- order(F_prime[F_prime < 0.05])[1:13]  # Keeping F prime < 0.05 -> Select 13 most stable ratios

# Assuming ratios_evaporated and ratios_unevaporated are available
d1 <- calculate_d1(ratios_evaporated, ratios_unevaporated)

# Kernel density estimation
density_d1 <- kde(d1)
```


### Score-Based Likelihood Ratio:
Using Linear Discriminant Analysis (LDA) for score calculation.
```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(e1071)

# Perform LDA
lda_model <- qda(supplier ~ ., data = p_test[, c(6:ncol(p_test))] %>% filter(supplier %notin% "unknown"))

# Predict and obtain scores
lda_predict <- predict(lda_model, p_test[, c(5, 6:ncol(p_test))] %>% filter(supplier %notin% "unknown"))
scores <- lda_predict$x

# Calculate likelihood ratios (LR)
calculate_lr <- function(scores, true_labels) {
  lr_values <- numeric(length(true_labels))
  
  for (i in 1:length(true_labels)) {
    same_class <- scores[true_labels == true_labels[i], ]
    different_class <- scores[true_labels != true_labels[i], ]
    
    mean_same <- colMeans(same_class)
    mean_diff <- colMeans(different_class)
    
    cov_same <- cov(same_class)
    cov_diff <- cov(different_class)
    
    lr <- dmvnorm(scores[i, ], mean_same, cov_same) / dmvnorm(scores[i, ], mean_diff, cov_diff)
    lr_values[i] <- lr
  }
  return(lr_values)
}

# Assuming the true labels are stored in a vector called 'labels'
lr_values <- calculate_lr(scores, labels)
```














# **Using only target compounds**

```{r, echo = FALSE, message = FALSE, warning = FALSE}
dflist <- list()
i <- 1
for (j in 1:nrow(target_comp)) {
  # Catch all peaks in dataframe that falling into the window with target compounds as center of the window
   idx1 <- which((df_step2$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion1 - 0.1) & 
                   df_step2$Ion2 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion2 - 0.1)) |
                  
                  (df_step2$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & df_step2$Ion1 >= (target_comp[j,]$Ion2 - 0.1) &
                     df_step2$Ion2 <= (target_comp[j,]$Ion1 + 0.1) & df_step2$Ion2 >= (target_comp[j,]$Ion1 - 0.1)))
  
  temp <- df_step2[idx1,]
  idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + 0.4) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - 0.4) &
                  temp$RT2 <= (target_comp[j,]$RT2 + 0.16) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - 0.16))
  
  dflist[[i]] <- temp[idx2,] %>% mutate(Feature = paste0("Compound_", i, "."))
  i <- i + 1
}
    
test <- bind_rows(dflist)
filter_alignment <- comp_filter(test)

shared_comp_alignment <- test[filter_alignment[[1]],]
```


# Overlay Scatter plot as Visual confirmation of appropriate RT windows by checking overlapping of RT1 and RT2 of aligned compounds and target compounds
```{r, echo = FALSE, message = FALSE, warning = FALSE}
plotlist <- list()
i <- 1
for (j in 1:nrow(target_comp)) {
  idx1 <- which((alignment$Ion1 <= (target_comp[j,]$Ion1 + 0.1) & alignment$Ion1 >= (target_comp[j,]$Ion1 - 0.1)) | 
                  (alignment$Ion1 <= (target_comp[j,]$Ion2 + 0.1) & alignment$Ion1 >= (target_comp[j,]$Ion2 - 0.1)))
  
  temp <- alignment[idx1,]
  idx2 <- which(temp$RT1 <= (target_comp[j,]$RT1 + 0.4) & 
                  temp$RT1 >= (target_comp[j,]$RT1 - 0.4) &
                  temp$RT2 <= (target_comp[j,]$RT2 + 0.16) & 
                  temp$RT2 >= (target_comp[j,]$RT2 - 0.16))
  plotlist[[i]] <- temp[idx2,]
  i <- i + 1
}

plotdf <- bind_rows(plotlist)

plotly::ggplotly(ggplot() +
                   geom_point(data = plotdf, aes(x = RT1, y = RT2, colour = Feature), pch = 1, size = 0.5) +
                   geom_point(data = target_comp, pch = 4, size = 2, colour = "red",
                              aes(x=RT1, y= RT2, label = Compound)) + 
                   xlim(5, 40))
```

### Catch by compound name
```{r, echo = FALSE, message = FALSE, warning = FALSE}
step3 <- df_step1.2 %>%
  filter(Area > 300000) %>%
  filter(!is.na(Compound))

unique_comp <- c()
# all_unique_compounds_idx <- c()

for (comp_grp in unique(step3$Compound)) {
  idx <- which(grepl(comp_grp, step3$Compound, fixed = TRUE))
  
  if (length(unique(step3[idx,]$Sample_name)) < 2) {
    print(comp_grp)
    unique_comp <- c(unique_comp, comp_grp)
    next
  }
}

shared_comp_alignment <- step3 %>% filter(Compound %notin% unique_comp)
```

























## ANOVA 
(Update 15 April 2024:) 
*** With Zero imputation 
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 2.370e-31 1.184e-31   0.603   0.55
Residuals       68 1.335e-29 1.963e-31   

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 1.800e-32 6.160e-33    0.03  0.993
Residuals     67 1.356e-29 2.025e-31 

*** With global minimum imputation
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 6.710e-27 3.354e-27   1.222  0.301
Residuals       68 1.866e-25 2.744e-27  

2. There is no difference between Octane rating
              Df   Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 2.28e-27 7.603e-28   0.267  0.849
Residuals     67 1.91e-25 2.851e-27

==> WHAT's NEXT?: Try to remove compound that appear in only 1 sample and run ANOVA//PCA again

!!! After removing unique cols
*** With Zero imputation
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value Pr(>F)
sampling_season  2 8.280e-31 4.141e-31   1.354  0.265
Residuals       68 2.079e-29 3.058e-31 

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 4.420e-31 1.473e-31   0.466  0.707
Residuals     67 2.118e-29 3.161e-31 

*** With global minimum imputation 
1. There is no difference between sampling seasons
                Df    Sum Sq   Mean Sq F value  Pr(>F)   
sampling_season  2 2.539e-26 1.269e-26    6.71 0.00219 **
Residuals       68 1.286e-25 1.892e-27                   
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

2. There is no difference between Octane rating
              Df    Sum Sq   Mean Sq F value Pr(>F)
Octane_rating  3 3.250e-27 1.085e-27   0.482  0.696
Residuals     67 1.508e-25 2.250e-27

(Update 22nd April 2024:) There is no difference between gas stations
            Df    Sum Sq   Mean Sq F value Pr(>F)
gas_station  9 4.311e-30 4.790e-31   0.926  0.509
Residuals   61 3.155e-29 5.171e-31 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(aov(as.formula(paste(paste(setdiff(names(comp_normalized_rt10.1 %>%
                                             select(-c("Octane_rating", "sampling_season"))), "gas_station"), collapse = "+"), " ~ gas_station")), data = comp_normalized_rt10.1 %>%
                                             select(-c("Octane_rating", "sampling_season"))))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
## SELECT TOP 100 COMPOUNDS THAT HAVE HIGHEST CUMULATIVE PEAK AREA ACROSS ALL SAMPLES ---------------
# Compute the sum of values of each column
column_sums <- colMeans(df_step3[,4:ncol(df_step3)])

# Order the columns based on their sum values
# Select the top 100 columns with the highest sum value
top_100_cum_sum <- names(column_sums)[order(column_sums, decreasing = TRUE)[1:100]]

# Create a new dataframe with only the top 100 columns
df_step3 <- df_step3[, c("Octane_rating", "samp_event", "gas_station", top_100_cum_sum)]

## SELECT TOP 150 COMPOUNDS THAT HAVE HIGHEST Variance of ACROSS ALL SAMPLES ------------
# Compute variance of values for each column
variances <- apply(df_step3[,3:ncol(df_step3)], 2, var)

# Sort variances in descending order and get the indices of the top 100 columns
top_100_var <- order(variances, decreasing = TRUE)[1:100]

# Select the top 100 columns with highest variance values
df_step3 <- df_step3[, c(1, 2, top_100_var)]

```


### 2. Via Investigate data distribution of compounds with high mean and high/low std

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Investigate data distribution for removal of compounds with high mean and low variance
# hist((shared_comp_alignment %>%
#   filter(Area < 10000000))$Area)

# summary_table <- new_df %>% # metadata_na_remove %>% 
#   ungroup() %>%
#   rowwise() %>%
#   mutate(mean_area = mean(c_across(7:ncol(.)), na.rm = TRUE),
#          sd_area = sd(c_across(7:ncol(.)), na.rm = TRUE)) %>%
#   ungroup() %>% 
#   relocate(mean_area, sd_area, .before = 1)

# View(summary_table %>% arrange(desc(mean_area), desc(sd_area)))

# highmean_highstd <- list()
# i <- 1
# for (num_feature in c(25,50,100,200)) {
#   # Adding back the labelling to metadata
#   highmean_highstd[[i]] <- 
    # as.data.frame(t((metadata %>%
    #                  # %>% arrange(desc(mean_area), desc(sd_area)))[1:num_feature,] %>%  #
    #                             # filter(Feature %in% unique(highmean_highstd$Feature)) %>% # Select compounds with high peak area's mean and std.
    #                             dplyr::select(-c("Chemical_group", "RT1", 
    #                                       "RT2", "Ion1", "Ion2")) %>% #, "mean_area", "sd_area")) %>% 
    #                             column_to_rownames(., var = "Feature")))) %>% 
    # rownames_to_column(., var="Sample_name") %>% 
    # mutate(Octane_rating = ifelse(str_detect(Sample_name, "A"), "Gas 87", 
    #                               ifelse(str_detect(Sample_name, "B"), "Gas 89",
    #                                      ifelse(str_detect(Sample_name, "C"), "Gas 91", "Gas 94")))) %>%
    # mutate(sampling_season = ifelse(str_detect(Sample_name, "b"), "blue",
    #                                 ifelse(str_detect(Sample_name, "p"), "purple", "orange"))) %>%
    # mutate(gas_station = ifelse(str_detect(Sample_name, "F001"), "Station 1",
    #                             ifelse(str_detect(Sample_name, "F002"), "Station 2",
    #                                    ifelse(str_detect(Sample_name, "F003"), "Station 3",
    #                                           ifelse(str_detect(Sample_name, "F004"), "Station 4",
    #                                                  ifelse(str_detect(Sample_name, "F005"), "Station 5",
    #                                                         ifelse(str_detect(Sample_name, "F006"), "Station 6",
    #                                                                ifelse(str_detect(Sample_name, "F007"), "Station 7",
    #                                                                       ifelse(str_detect(Sample_name, "F008"), "Station 8",
    #                                                                              ifelse(str_detect(Sample_name, "F009"), "Station 9", "Station 10")))))))))) %>% 
    # # Add info on Suppliers
    # mutate(supplier = ifelse(str_detect(Sample_name, "F001"), "Miscellanous",
    #                          ifelse(str_detect(Sample_name, "F002"), "Miscellanous",
    #                                 ifelse(str_detect(Sample_name, "F003"), "Shell",
    #                                        ifelse(str_detect(Sample_name, "F004"), "Cenovus",
    #                                               ifelse(str_detect(Sample_name, "F005"), "Miscellanous",
    #                                                      ifelse(str_detect(Sample_name, "F006"), "Suncor",
    #                                                             ifelse(str_detect(Sample_name, "F007"), "Imperial",
    #                                                                    ifelse(str_detect(Sample_name, "F008"), "Burnaby",
    #                                                                           ifelse(str_detect(Sample_name, "F009"), "Imperial", "unknown")))))))))) %>% 
    # relocate(Octane_rating, sampling_season, gas_station, supplier, .after = 1)
#   i <- i + 1
# }

# ggplot(data = plotdf %>% mutate(Feature = factor(Feature, levels = unique(Feature))), aes(x=Feature, y = mean_area)) +
#   geom_bar(stat = 'identity', position = position_dodge(width = 0.9)) +
#   geom_errorbar(aes(ymin = mean_area - sd_area,
#                     ymax = mean_area + sd_area, colour = "red")) +
#   ggtitle("Rank 1:100") +
#   theme_classic(base_size = 15) +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

## Reserved code for assessing imputation and normalization
```{r , echo=FALSE, warning = FALSE, message=FALSE}
#########################
##   1. Imputation     ##
#########################

## 1.1 Random Forest Imputation (similar to Python's IterativeImputer with ExtraTrees)
random_forest_imputation <- function(df) {
  # Run missForest
  rf_out <- missForest(df, verbose = FALSE)
  imputed <- rf_out$ximp
  
  # Safety check: Are there any leftover missing or infinite values?
  if (anyNA(imputed)) {
    warning("random_forest_imputation: After missForest, data still has NA.")
  }
  if (any(!is.finite(as.matrix(imputed)))) {
    warning("random_forest_imputation: Imputed data has Inf or NaN.")
  }
  
  return(imputed)
}

## 1.2 Bayesian PCA Imputation
bayesian_pca_imputation <- function(df){
  # 1) Fill NA with 0 for the initial run (similar to Python's df.fillna(0))
  df_zero <- df
  df_zero[is.na(df_zero)] <- 0
  
  # 2) Run Bayesian PCA on the zero-filled data
  bpca_res <- pcaMethods::pca(
    df_zero,
    method = "bpca",
    nPcs   = min(ncol(df_zero) - 1, nrow(df_zero) - 1)
  )
  
  # 3) Retrieve the reconstructed (imputed) data
  df_imputed <- completeObs(bpca_res)
  
  return(df_imputed)
}


## 1.3 Mean Imputation
mean_imputation <- function(df){
  df_numeric <- df
  for(i in seq_along(df_numeric)){
    col_data <- df_numeric[[i]]
    # Replace NA with mean
    col_data[is.na(col_data)] <- mean(col_data, na.rm = TRUE)
    df_numeric[[i]] <- col_data
  }
  return(df_numeric)
}

## 1.4 kNN Imputation
knn_imputation <- function(df, k = 5){
  # VIM::kNN modifies the data in place; to avoid that, use a copy
  df_copy <- df
  # kNN returns a data.frame with the imputed columns appended with ".imp"
  # The original columns remain but with no missing values replaced in place
  # So we can do something like:
  imputed_data <- kNN(df_copy, k = k, imp_var = FALSE)  # no new columns
  return(imputed_data)
}

## 1.5 SVD Imputation (SoftImpute)
svd_imputation <- function(df, rank.max = 5, lambda = 1, thresh = 1e-5, maxit = 100){
  # Convert to matrix
  mat <- as.matrix(df)
  
  # Use softImpute
  fit <- softImpute(mat, rank.max = rank.max, lambda = lambda, maxit = maxit, thresh = thresh)
  
  # Reconstruct the completed matrix
  mat_imputed <- complete(mat, fit)
  
  # Return as data.frame with original names
  df_imputed <- as.data.frame(mat_imputed)
  colnames(df_imputed) <- colnames(df)
  rownames(df_imputed) <- rownames(df)
  
  return(df_imputed)
}

## 1.6 Fill with zeros
zero_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    col_data[is.na(col_data)] <- 0
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.7 Fill with half-min of non-missing values
half_min_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    non_zero <- col_data[col_data > 0]
    if(length(non_zero) > 0){
      half_min_val <- min(non_zero, na.rm = TRUE) / 2
      col_data[is.na(col_data)] <- half_min_val
    } else {
      # if entire column is zero or NA, just set to 0 or small value
      col_data[is.na(col_data)] <- 0
    }
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.8 Fill with median
median_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    col_data[is.na(col_data)] <- median(col_data, na.rm = TRUE)
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.9 No imputation (pass-through)
no_imputation <- function(x) {
  return(x)
}


###############################
##   2. Normalization        ##
###############################

## 2.1 TSN normalization (divide each row by sum of row)
TSN_normalize_features <- function(df){
  # for each row, divide each element by the row sum
  row_sums <- rowSums(df, na.rm = TRUE)
  df_norm <- sweep(df, 1, row_sums, FUN = "/")
  return(df_norm)
}

## 2.2 Min-Max scaler 
scale_minmax <- function(df){
  # We can do it manually or use caret::preProcess
  # Here is a manual approach for numeric columns
  df_mat <- as.matrix(df)
  mins <- apply(df_mat, 2, min, na.rm = TRUE)
  maxs <- apply(df_mat, 2, max, na.rm = TRUE)
  
  scaled_mat <- sweep(df_mat, 2, mins, FUN = "-")
  ranges <- maxs - mins
  scaled_mat <- sweep(scaled_mat, 2, ranges, FUN = "/")
  
  # convert back to data frame
  df_scaled <- as.data.frame(scaled_mat)
  colnames(df_scaled) <- colnames(df)
  rownames(df_scaled) <- rownames(df)
  
  return(df_scaled)
}

## 2.3 Z-score normalization
z_score_normalization <- function(df){
  # scale() in R does mean-center and unit variance by column
  df_scaled <- scale(df, center = TRUE, scale = TRUE)
  return(as.data.frame(df_scaled))
}

## 2.4 Log10 normalization
log_normalize_rows <- function(df, offset = 1){
  if(offset <= 0){
    stop("Offset must be > 0 to avoid log(0).")
  }
  df_mat <- as.matrix(df)
  df_log <- log10(df_mat + offset)
  
  df_out <- as.data.frame(df_log)
  colnames(df_out) <- colnames(df)
  rownames(df_out) <- rownames(df)
  return(df_out)
}

## 2.5 No normalization (pass-through)
no_normalization <- function(x){
  return(x)
}


###############################
##  3. Model Stability (RF)  ##
###############################

evaluate_model_stability <- function(X, y){
  # We want to replicate StratifiedKFold with n_splits = min number of samples in any class
  # In R caret, we can define number of folds = min(table(y)).
  
  folds <- min(table(y))
  # If folds < 2, cross-validation can't be done; return NA
  if(folds < 2){
    return(list(cv_mean = NA, cv_std = NA))
  }
  
  # Prepare data
  data_in <- data.frame(X, Class = factor(y))
  
  # Define trainControl
  set.seed(sample(1:999, 1))  # or pick a random seed
  ctrl <- trainControl(method = "cv", number = folds, classProbs = TRUE)
  
  # Train a random forest model using caret
  rf_fit <- caret::train(
    Class ~ .,
    data = data_in,
    method = "rf",
    trControl = ctrl,
    metric = "Accuracy"
  )
  
  # Caret stores resample accuracies in rf_fit$resample$Accuracy
  accuracy_vals <- rf_fit$resample$Accuracy
  cv_mean <- mean(accuracy_vals, na.rm = TRUE)
  cv_std  <- sd(accuracy_vals, na.rm = TRUE)
  
  return(list(cv_mean = cv_mean, cv_std = cv_std))
}

###############################
##   4. Cluster Resolution   ##
###############################

# 4.1 Compute cluster resolution for two clusters
calculate_cluster_resolution <- function(data1, data2) {
  # data1 and data2: matrices/data.frames of the first two PCA scores for each group
  
  # Calculate centroids
  centroid1 <- colMeans(data1)
  centroid2 <- colMeans(data2)
  
  # Euclidean distance between the centroids
  centroid_distance <- sqrt(sum((centroid1 - centroid2)^2))
  
  # Calculate average Euclidean distance of points to their respective centroid
  avg_distance1 <- mean(sqrt(rowSums((as.matrix(data1) - matrix(centroid1, nrow = nrow(data1), ncol = ncol(data1), byrow = TRUE))^2)))
  avg_distance2 <- mean(sqrt(rowSums((as.matrix(data2) - matrix(centroid2, nrow = nrow(data2), ncol = ncol(data2), byrow = TRUE))^2)))
  
  # Resolution: distance between centroids divided by within-cluster dispersions
  resolution <- centroid_distance / (avg_distance1 + avg_distance2)
  return(resolution)
}

# 4.2 Compute average cluster resolution over all pairs of groups
calculate_average_cluster_resolution <- function(X, group_vector) {
  # X: a numeric data.frame or matrix after imputation and normalization.
  # group_vector: a vector or factor of class labels corresponding to each row in X.
  
  # Perform PCA on the numeric features. We use only the first two principal components.
  pca_model <- prcomp(X, center = TRUE, scale. = FALSE)
  scores <- as.data.frame(pca_model$x[, 1:2])
  
  # Ensure the group labels are factors
  group <- as.factor(group_vector)
  group_levels <- levels(group)
  
  pair_resolutions <- c()
  
  # Loop over each pair of groups to calculate their resolution
  for (i in 1:(length(group_levels) - 1)) {
    for (j in (i + 1):length(group_levels)) {
      # Subset the PCA scores for the two groups
      data1 <- scores[group == group_levels[i], , drop = FALSE]
      data2 <- scores[group == group_levels[j], , drop = FALSE]
      
      # Only compute if there is more than one data point in each group
      if(nrow(data1) > 1 && nrow(data2) > 1) {
        res <- calculate_cluster_resolution(data1, data2)
        pair_resolutions <- c(pair_resolutions, res)
      }
    }
  }
  
  # Calculate the average resolution over all group pairs
  avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
  return(avg_resolution)
}

###################################################
##   5. Pairwise Significance Testing            ##
###################################################

pairwise_significance_tests <- function(input_df, 
                                        group_col = "Subcategory",
                                        start_col_index = 5){
  # For each feature from start_col_index onward,
  #   for each pair of group categories,
  #     check if both groups have >=3 data points
  #     do Shapiro test for normality
  #     if not normal => Mann-Whitney (wilcox.test)
  #     else => t.test
  # Collect p-values and do multiple corrections
  
  df_results <- data.frame(Feature=character(), comparison_pair=character(), pval=integer())
  
  # get unique group combos
  group_pairs <- utils::combn(unique(as.character(input_df[[group_col]])), 2)

  for(feature_col in start_col_index:ncol(input_df)){
    for (col in 1:ncol(group_pairs)){
      p_1 <- group_pairs[1,col]
      p_2 <- group_pairs[2,col]

      vec1 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_1), feature_col]))
      vec2 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_2), feature_col]))

      # ensure at least 3 data points
      if(length(vec1) >= 3 && length(vec2) >= 3){
        # skip if all values are the same
        if(all(vec1 == vec1[1]) || all(vec2 == vec2[1])){
          next
        }
        
        # normality tests
        p_shapiro1 <- tryCatch(shapiro.test(vec1)$p.value, error = function(e) NA)
        p_shapiro2 <- tryCatch(shapiro.test(vec2)$p.value, error = function(e) NA)
        
        # decide parametric vs non-parametric
        if(is.na(p_shapiro1) || is.na(p_shapiro2)){
          # if for some reason test failed, skip
          next
        }
        
        if(p_shapiro1 < 0.05 || p_shapiro2 < 0.05){
          # mann-whitney
          pval_test <- tryCatch(wilcox.test(vec1, vec2)$p.value, error = function(e) NA)
        } else {
          # t-test
          pval_test <- tryCatch(t.test(vec1, vec2, var.equal = FALSE)$p.value, error = function(e) NA)
        }
        
        if(!is.na(pval_test)){
          # append to df
          df_results[nrow(df_results) + 1,] <- c(colnames(input_df)[feature_col], 
                                                 paste0(p_1, " & ", p_2), 
                                                 pval)
        }
      }
    }
  }
  
  # multiple corrections
  pvalue_correction_methods <- c("bonferroni", "holm", "hochberg", "hommel", "BH", "BY")
  
  # For convenience, store all results in a list
  results_list <- list()
  j <- 1
  for(method in pvalue_correction_methods){
    temp <- df_results
    temp$adjusted_pvalue <- p.adjust(temp$pval, method = method)
    significant <- temp %>% dplyr::filter(adjusted_pvalue < 0.05) %>% arrange(adjusted_pvalue)
    results_list[[j]] <- significant
    
    if(nrow(significant) == 0){
      cat(paste0(method, " did not result in any significant compound\n"))
    } else {
      # cat(paste0("Significant results for method: ", method, "\n"))
      # print(significant)
      # cat(paste0(method, " has ", length(unique(significant$Feature)), " significant features\n"))
      j <- j + 1
    }
  }
  
  return(list(
    uncorrected = df_results,
    corrected   = results_list
  ))
}

# Define imputation and normalization methods as lists of functions
imputation_methods <- list(
  random_forest_imputation   = random_forest_imputation,
  bayesian_pca_imputation    = bayesian_pca_imputation,
  mean_imputation            = mean_imputation,
  knn_imputation             = knn_imputation,
  svd_imputation             = svd_imputation,
  zero_imputation            = zero_imputation,
  half_min_imputation        = half_min_imputation,
  median_imputation          = median_imputation
  # No_imputation              = no_imputation
)

normalization_methods <- list(
  TSN_Normalization    = TSN_normalize_features,
  Min_Max_Scaler       = scale_minmax,
  Z_Score_Normalization= z_score_normalization,
  Log10_Normalization  = log_normalize_rows,
  No_normalization     = no_normalization
)

find_best_impute_normalize <- function(df,
                                       class_col="Subcategory",
                                       group_for_significance="Subcategory",
                                       remove_cols=c("Polymer","Source", "technique"),
                                       start_col_index=6) {
  # 1) Extract numeric features X and target y
  X       <- df %>% dplyr::select(-all_of(c(remove_cols, class_col))) %>% as.data.frame()
  y       <- df[[class_col]]
  
  # Make sure X is numeric
  X[] <- lapply(X, function(x) as.numeric(x))
  rownames(X) <- rownames(df)
  
  # We'll store the results
  results_list <- list()
  
  # 2) Loop over imputation + normalization
  for(imp_name in names(imputation_methods)){
    impute_fun <- imputation_methods[[imp_name]]
    
    for(norm_name in names(normalization_methods)){
      norm_fun <- normalization_methods[[norm_name]]
      
      # Impute
      if(imp_name == "No_imputation"){
        X_imputed <- X
      } else {
        X_imputed <- tryCatch(
          impute_fun(X),
          error = function(e) {
            message("Error in ", imp_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_imputed)) next
      }
      
      # Normalize
      if(norm_name == "No_normalization"){
        X_norm <- X_imputed
      } else {
        X_norm <- tryCatch(
          norm_fun(X_imputed),
          error = function(e){
            message("Error in ", norm_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_norm)) next
      }
      
      if(any(is.na(X_norm))){
        stop("Normalization method ", norm_name, " produced NAs!")
      }
      if(any(!is.finite(as.matrix(X_norm)))){
        stop("Normalization method ", norm_name, " produced Inf/NaN!")
      }

      # Evaluate cluster resolution
      cluster_res <- calculate_average_cluster_resolution(X_norm, y)
      
      # Evaluate CV accuracy
      rf_stability <- evaluate_model_stability(X_norm, y)
      cv_mean <- rf_stability$cv_mean
      cv_std  <- rf_stability$cv_std
      
      # Evaluate significance (# of significant features)
      # We need the entire df to do the pairwise test, so let's combine X_norm with original grouping col:
      df_temp <- df
      
      # Overwrite numeric columns with X_norm
      numeric_cols <- colnames(X)
      df_temp[numeric_cols] <- X_norm

      sig_res  <- pairwise_significance_tests(df_temp,
                                              group_col=group_for_significance,
                                              start_col_index=start_col_index)
      # "max_signif" is the maximum number of sig features across correction methods
      sig_count <- max(
        sapply(sig_res$corrected, function(df) length(unique(df$Feature)))
      )
      
      results_list[[length(results_list)+1]] <- data.frame(
        Imputation   = imp_name,
        Normalization= norm_name,
        ClusterRes   = cluster_res,
        CVmean       = cv_mean,
        CVstd        = cv_std,
        SigCount     = sig_count,
        stringsAsFactors=FALSE
      )
    }
  }
  
  df_results <- do.call(rbind, results_list)
  
  # 3) Choose “best” combo. For example, rank by each criterion, then sum of ranks
  df_results$rank_clusterRes <- rank(-df_results$ClusterRes, ties.method="min")
  df_results$rank_cvmean     <- rank(-df_results$CVmean,     ties.method="min")
  # df_results$rank_sigcount   <- rank(-df_results$SigCount,   ties.method="min")
  df_results$combined_rank   <- df_results$rank_clusterRes + df_results$rank_cvmean # + df_results$rank_sigcount
  
  # best row = min combined rank
  best_idx <- which.min(df_results$combined_rank)
  best_row <- df_results[best_idx, ]
  
  # Return the full table + the best combo row
  list(
    results_table = df_results,
    best_combo    = best_row
  )
}
```

# STEP 4: Missing value imputation
(Update 21 April 2024:) After remove compounds appear in only one sample and compounds that have >90% Missing values -> plot histogram of distribution -> most of compounds are < 2e+06 => try filter all peaks that is >= 2e+06

## Option 1: 0.001
```{r, echo = FALSE, message = FALSE, warning = FALSE}
df_0.001 <- copy(input_rf) 

for (r in 1:nrow(df_0.001)) {
  df_0.001[r, which(base::is.na(df_0.001[r,]))] <- 0.001
}

```

## Option 2: LOD 
LOD = go through each row of the df and replacing missing values = the lowest values of each sample * 3

```{r , echo=FALSE, warning = FALSE, message=FALSE}
df_lod <- copy(df_filtered_step3) 
for (r in 1:nrow(df_lod)) { 
  df_lod[r, 6:ncol(df_lod)][,which(base::is.na(df_lod[r, 6:ncol(df_lod)]))] <- 3*min(df_lod[r, 6:ncol(df_lod)][,which(!base::is.na(df_lod[r, 6:ncol(df_lod)]))])
} 

```

## Option 3: Randomized values drawn from uniform distribution of the two global minimum values
```{r , echo=FALSE, warning = FALSE, message=FALSE}
min_df <- copy(df_filtered_step3) 

for (r in 1:nrow(min_df)) { 
  min_df[r, which(base::is.na(min_df[r,]))] <- as.list(runif(length(which(is.na(min_df[r,]))),
                                                             min = 0,
                                                             max = sort(df_all$Area)[1]))
}
```

## Option 4: with MissForest

'cart', 'rf', 'sample', "2l.lmer" is quite robust when the matrix has linearly dependent columns or is near-singular due to multicollinearity or insufficient variation in the data. 
'rf' still resulted in missing values after complete(imp) -> consider not using it

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(missForest)
missrf_df <- copy(df_filtered_step3)
subset <- missrf_df[,-c(1,2,3,4,5)]
colnames(subset) <- paste0("x", 1:ncol(subset))
missforest_df <- missForest(subset, parallelize = 'variables')$ximp
colnames(missforest_df) <- colnames(missrf_df[,-c(1,2,3,4,5)])
missforest <- cbind(missforest_df, missrf_df[,c(1,2,3,4,5)]) %>% relocate(Sample_name, Octane_rating, sampling_season, gas_station, supplier, .before =1)
```


# STEP 5: Data Normalization

### Percentage-based normalization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
input <- df_0.001
percentage_normalized <- as.data.frame(t(apply(input[, 6:ncol(input)],
                                               MARGIN = 1, 
                                               function(row) {row/sum(row, na.rm = TRUE)}))) 

df_percentage <- percentage_normalized %>% 
  mutate(Octane_rating = input$Octane_rating) %>%
  mutate(sampling_season = input$sampling_season) %>%
  mutate(gas_station = input$gas_station) %>%
  mutate(Sample_name = input$Sample_name) %>%
  mutate(supplier = input$supplier) %>%
  relocate(Octane_rating, sampling_season, gas_station, Sample_name, supplier, .before = 1)

```

### Log normalization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
input <- missforest
log_normalized <- as.data.frame(t(apply(input[, 6:ncol(input)], 
                                        MARGIN = 1, function(x) log(x))))

log_normalized_df <- log_normalized %>% 
  mutate(Octane_rating = input$Octane_rating) %>%
  mutate(sampling_season = input$sampling_season) %>%
  mutate(gas_station = input$gas_station) %>%
  mutate(Sample_name = input$Sample_name) %>%
  mutate(supplier = input$supplier) %>%
  relocate(supplier, Octane_rating, sampling_season, gas_station, Sample_name, .before = 1)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
pairwise_significance_tests <- function(input_df, 
                                          group_col = "gas_station",
                                          start_col_index = 6) {
  df_results <- data.frame(Feature = character(), comparison_pair = character(), pval = numeric(), stringsAsFactors = FALSE)
  
  # Get all unique group pairs
  group_pairs <- utils::combn(unique(as.character(input_df[[group_col]])), 2)
  
  for (feature_col in start_col_index:ncol(input_df)) {
    for (col in 1:ncol(group_pairs)) {
      p_1 <- group_pairs[1, col]
      p_2 <- group_pairs[2, col]
      
      vec1 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_1), feature_col]))
      vec2 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_2), feature_col]))
      
      if (length(vec1) >= 3 && length(vec2) >= 3) {
        # Skip if all values are constant in one group
        if (all(vec1 == vec1[1]) || all(vec2 == vec2[1])) {
          next
        }
        
        p_shapiro1 <- tryCatch(shapiro.test(vec1)$p.value, error = function(e) NA)
        p_shapiro2 <- tryCatch(shapiro.test(vec2)$p.value, error = function(e) NA)
        
        if (is.na(p_shapiro1) || is.na(p_shapiro2)) next
        
        if (p_shapiro1 < 0.05 || p_shapiro2 < 0.05) {
          pval_test <- tryCatch(wilcox.test(vec1, vec2)$p.value, error = function(e) NA)
        } else {
          pval_test <- tryCatch(t.test(vec1, vec2, var.equal = FALSE)$p.value, error = function(e) NA)
        }
        
        if (!is.na(pval_test)) {
          df_results[nrow(df_results) + 1, ] <- c(colnames(input_df)[feature_col], 
                                                   paste0(p_1, " & ", p_2), 
                                                   pval_test)
        }
      }
    }
  }
  
  pvalue_correction_methods <- c("bonferroni", "holm", "hochberg", "hommel", "BH", "BY")
  results_list <- list()
  j <- 1
  for (method in pvalue_correction_methods) {
    temp <- df_results
    temp$adjusted_pvalue <- p.adjust(as.numeric(temp$pval), method = method)
    significant <- temp %>% filter(adjusted_pvalue < 0.05) %>% arrange(adjusted_pvalue)
    results_list[[j]] <- significant
    if (nrow(significant) == 0) {
      cat(paste0(method, " did not result in any significant compound\n"))
    }
    j <- j + 1
  }
  
  return(list(
    uncorrected = df_results,
    corrected   = results_list
  ))
}
```



# Pair-wise test with p-value correction for multiple testing

```{r, echo = TRUE, message = FALSE, warning = FALSE}
# Histogram
hist(as.numeric(unlist(input[which(input$supplier == "Shell"), 13])), col='steelblue', main='')
hist(as.numeric(unlist(input[which(input$supplier == "Cenovus"), 13])), col='steelblue', main='')

# Q-Q plots aka. Normal Probability plots
stats::qqnorm(as.numeric(unlist(input[which(input$supplier == "Shell"), 13])), main='')
stats::qqline(as.numeric(unlist(input[which(input$supplier == "Shell"), 13])))

stats::qqnorm(as.numeric(unlist(input[which(input$supplier == "Cenovus"), 13])), main='')
stats::qqline(as.numeric(unlist(input[which(input$supplier == "Cenovus"), 13])))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(car)
input <- df_0.001_percentage_normalized 
# Here is an example case of a significant compound taken from  Welch's t-test(for unequal variances) of suppliers
p_1 <- utils::combn(unique(input$gas_station), 2)[1,1]
p_2 <- utils::combn(unique(input$gas_station), 2)[2,2]

# Check normality for both groups
# Q-Q plots
qqnorm(group1, main = "Q-Q Plot for Group 1")
qqline(group1, col = "red")

qqnorm(group2, main = "Q-Q Plot for Group 2")
qqline(group2, col = "red")

# Shapiro-Wilk test - If the p-value is less than 0.05, we conclude that the sample is not normal distributed
group1 <- as.numeric(unlist(input[which(input$gas_station == p_1), 6]))
group2 <- as.numeric(unlist(input[which(input$gas_station == p_2), 6]))
shapiro.test(group1)$p.value
shapiro.test(group2)$p.value

# Check variance homogeneity using Levene's test
library(car)
leveneTest(values ~ group, data = data) # A p-value greater than 0.05 indicates that the variances are equal (homoscedasticity). If < 0.05 -> use Welch's t-test

# Identify outliers
outlierTest(lm(differences ~ 1)) 
# (rstudent): Residuals significantly different from 0 (typically greater than 2 or less than -2) suggest potential outliers.
# (unadjusted p-value): A small p-value (typically < 0.05) indicates that the observation is a potential outlier.
# (Bonferroni p) : A small Bonferroni p-value (typically < 0.05) indicates a strong evidence that the observation is an outlier
```

(Update 9th July 2024): Welch t-test is more appropriate than Wilcoxon test. 

In the simplest possible terms, you control FWER (Bonferroni method) when you care about the result of the specific hypotheses tested whereas you control FDR (BH method) when you care about the number of significant results.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
input <- df_percentage # df_percentage, log_normalized_df
 
df <- data.frame(Feature=character(), comparison_pair=character(), pval=integer())

for (feature_col in 6:ncol(input)) {      
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(input$gas_station), 2))) {  # "sampling_season", "Octane_rating", "gas_station"
    
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(input$gas_station), 2)[1,col]
    p_2 <- utils::combn(unique(input$gas_station), 2)[2,col]
    
    # print(paste0(p_1, "_", p_2))
    # calculating the p-value between each gas station pair 
    vec1 <- as.numeric(unlist(input[which(input$gas_station == p_1), feature_col]))
    vec2 <- as.numeric(unlist(input[which(input$gas_station == p_2), feature_col]))
    
    if (sum(!is.na(vec1)) >= 3 & sum(!is.na(vec2)) >= 3) {
      # If all number in either of the vector are exactly identical
      if (all(vec1 == vec1[1]) || all(vec2 == vec2[1])) {
        next
      } else {
        if (shapiro.test(vec1)$p.value < 0.05 || shapiro.test(vec2)$p.value < 0.05) { # if both not normally distributed
          pval <- try(stats::wilcox.test(vec1, vec2)$p.value)
        } else if (shapiro.test(vec1)$p.value < 0.05 & shapiro.test(vec2)$p.value < 0.05) { # if both normally distributed
          pval <- try(stats::t.test(vec1, vec2)$p.value)
        } else {
          pval <- try(stats::wilcox.test(vec1, vec2)$p.value)
        }
      }
    } else {
      next
    }
    
    # For cases when Error: not enough 'x' observations
    if (inherits(pval, "try-error")) {
      next  # Skip to the next iteration if an error occurs
    }
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(input)[feature_col], 
                           paste0(p_1, " & ", p_2), 
                           pval)
  }
}

sig_comp <- list()
uncorrected_raw_pair_test_results <- list()
j <- 1
target_comp <- read_xlsx(path = paste0(getwd(), "/python/Shortened ILR Compound List PF001A 07-06-2024_Huy_modified_08Aug2024.xlsx"))

pvaluecorrect <- c("bonferroni", "holm", "hochberg", "hommel", "BH", "BY")
for (m in pvaluecorrect) {
  temp <- copy(df)
  temp$adjusted_pvalue <- stats::p.adjust(temp$pval, method = m)
  sig_comp[[j]] <- temp %>%
    filter(., adjusted_pvalue < 0.05) %>%
    arrange(adjusted_pvalue)
  
  uncorrected_raw_pair_test_results[[j]] <- temp
  
  if (dim(sig_comp[[j]])[1] == 0) {
    print(paste0(m," does not resulted in any significant compound"))
    j <- j + 1
    next
  } else {
    print(m)
    # get RT and ions information of chemical features
    rt1 <- c()
    rt2 <- c()
    major <- c()
    minor <- c()
    compound_name <- c()
    for (row in 1:nrow(sig_comp[[j]])) {
      rt1 <- c(rt1, metadata[which(metadata$Feature == as.numeric(sig_comp[[j]][row,]$Feature)),]$RT1)
      rt2 <- c(rt2, metadata[which(metadata$Feature == as.numeric(sig_comp[[j]][row,]$Feature)),]$RT2)
      major <- c(major, metadata[which(metadata$Feature == as.numeric(sig_comp[[j]][row,]$Feature)),]$Ion1)
      minor <- c(minor, metadata[which(metadata$Feature == as.numeric(sig_comp[[j]][row,]$Feature)),]$Ion2)
    }

    sig_comp[[j]]$RT1 <- rt1
    sig_comp[[j]]$RT2 <- rt2
    sig_comp[[j]]$Ion1 <- major
    sig_comp[[j]]$Ion2 <- minor

    ## Get compound name
    compound_name <- c()
    chem_grp <- c()
    for (row in 1:nrow(sig_comp[[j]])) {
      idx <- which(abs(target_comp$RT1 - sig_comp[[j]][row,]$RT1) <= 0.1 &
                     abs(target_comp$RT2 - sig_comp[[j]][row,]$RT2) <= 0.1 &
                     abs(target_comp$Ion1 - sig_comp[[j]][row,]$Ion1) <= 0.5  &
                     abs(target_comp$Ion2 - sig_comp[[j]][row,]$Ion2) <= 0.5
      )
      # print(paste0(row, "_", idx))
      if (identical(idx, integer(0))) {
        chem_grp <- c(chem_grp, "unknown")
        compound_name <- c(compound_name, "unknown")
      }
      chem_grp <- c(chem_grp, target_comp[idx,]$Group)
      compound_name <- c(compound_name, target_comp[idx,]$Compound)
    }

    sig_comp[[j]]$compound_name <- compound_name
    sig_comp[[j]]$Chemical_group <- chem_grp
    print(sig_comp[[j]] %>%
            arrange(adjusted_pvalue))
    print(paste0(m, " has ", length(unique((sig_comp[[j]] %>%
            arrange(adjusted_pvalue))$Feature)), " significant features"))
    j <- j + 1
  }
}

# write_xlsx(sig_comp[[5]][, c(1,5,6,7,8,10)] %>% distinct(), path = "groupingbyrt1rt2ion1_withdatacompression_0.001_TSN.xlsx")
```

### Bubble plots of significant compounds with labelling of Chemical groups
```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(cowplot)
library(ggpubr)

## Unique + Non-uniques -----------------------------
# Create subdf for 4 stations with majority of significant instances (3, 4, 6, 7), with 3 columns: RT1, RT2, Normalized Area
df_station3467 <- p_df %>%
  # Select only the significant compounds form Welch's t-test
  dplyr::select(-setdiff(colnames(p_df[,6:ncol(p_df)]), unique(combined_df$Feature))) %>% 
  # Select gas station
  dplyr::filter(gas_station %in% c("Station 3", "Station 4", "Station 6", "Station 7"))

master_list_3467 <- list()
i <- 1
for (names in c("Station 3", "Station 4", "Station 6", "Station 7")) {
  master_list_3467[[i]] <- data.frame(t(colMeans(df_station3467 %>%
                                                   filter(gas_station %in% names) %>%
                                                   dplyr::select(-c(Octane_rating, sampling_season, gas_station, Sample_name, supplier)))))
  
  colnames(master_list_3467[[i]]) <- colnames(df_station3467[, 6:ncol(df_station3467)]) 
  master_list_3467[[i]]$gas_station <- names
  master_list_3467[[i]] <- master_list_3467[[i]] %>% pivot_longer(cols = 1:(ncol(.)-1), names_to = "Feature", values_to = "TSN Normalized Area")
  i <- i + 1
}

# Adding compounds info (RTs, Ions, Chemical group) by full_join
collapsed_df_station3467 <- full_join(bind_rows(master_list_3467), 
                                       combined_df %>% 
                                         dplyr::select(-comparison_pair) %>% 
                                         distinct(), by = "Feature")
 
  
testbubble <- ggplot(data = collapsed_df_station3467, aes(x = RT1, y = RT2, size = `TSN Normalized Area`, color = `Chemical_group`)) +
  geom_point(pch = 21) + 
  facet_wrap(~gas_station) +
  # scale_x_continuous(breaks = seq(from = 0, to = 40, by = 5)) +
  scale_size(limits = c(min(collapsed_df_station3467$`TSN Normalized Area`),
                        max(collapsed_df_station3467$`TSN Normalized Area`))) +
  # scale_alpha(limits = c(min(min(df_gas$area), min(df_diesel$area)), 
  #                        max(max(df_gas$area), max(df_diesel$area)))) +
  guides(colour = guide_legend(override.aes = list(size=5))) +
  theme_classic(base_size = 15) +
  labs(x = "Retention time 1", y = "Retention time 2") +
  theme(plot.title = element_text(hjust = 0.5)) +
   
  # Add a red rectangular annotation
  # BTEX
  annotate("rect", xmin = 14, xmax = 17, ymin = 3.8, ymax = 4, 
           color = "red", fill = NA, size = 1) +
  # Castle Group
  annotate("rect", xmin = 18.5, xmax = 21.5, ymin = 3.8, ymax = 3.95, 
           color = "red", fill = NA, size = 1) +
  # Unknown
  annotate("rect", xmin = 6.5, xmax = 23.5, ymin = 3.1, ymax = 3.35, 
           color = "red", fill = NA, size = 1)

ggsave(filename = paste0("Bubble_plot","_both-unique-and-non-unique_filNA0.001_TSN_sig_comp_unique_and_nonunique_unionize", format(Sys.time(), "%d-%m-%Y"),".tiff"), 
       plot = testbubble, 
       device = "tiff",
       dpi = 600,        # Set DPI to 600
       width = 10,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 

## Only Non-uniques -----------------------------
df_station3467 <- p_nonunique_df %>%
  # Select only the significant compounds form Welch's t-test
  dplyr::select(-setdiff(colnames(p_nonunique_df[,6:ncol(p_nonunique_df)]), unique(combined_df$Feature))) %>% 
  # Select gas station
  dplyr::filter(gas_station %in% c("Station 3", "Station 4", "Station 6", "Station 7"))

master_list_3467 <- list()
i <- 1
for (names in c("Station 3", "Station 4", "Station 6", "Station 7")) {
  master_list_3467[[i]] <- data.frame(t(colMeans(df_station3467 %>%
                                                   filter(gas_station %in% names) %>%
                                                   dplyr::select(-c(Octane_rating, sampling_season, gas_station, Sample_name, supplier)))))
  
  colnames(master_list_3467[[i]]) <- colnames(df_station3467[, 6:ncol(df_station3467)]) 
  master_list_3467[[i]]$gas_station <- names
  master_list_3467[[i]] <- master_list_3467[[i]] %>% pivot_longer(cols = 1:(ncol(.)-1), names_to = "Feature", values_to = "TSN Normalized Area")
  i <- i + 1
}

# Adding compounds info (RTs, Ions, Chemical group) by full_join
collapsed_df_station3467 <- full_join(bind_rows(master_list_3467), 
                                       combined_df %>% 
                                         dplyr::select(-comparison_pair) %>% 
                                         distinct(), by = "Feature")
 
  
ggplot(data = collapsed_df_station3467, aes(x = RT1, y = RT2, size = `TSN Normalized Area`, color = `compound_name`)) +
  geom_point(pch = 19) + 
  facet_wrap(~gas_station) +
  # scale_x_continuous(breaks = seq(from = 0, to = 40, by = 5)) +
  scale_size(limits = c(min(collapsed_df_station3467$`TSN Normalized Area`),
                        max(collapsed_df_station3467$`TSN Normalized Area`))) +
  # scale_alpha(limits = c(min(min(df_gas$area), min(df_diesel$area)), 
  #                        max(max(df_gas$area), max(df_diesel$area)))) +
  guides(colour = guide_legend(override.aes = list(size=5))) +
  theme_classic(base_size = 15) +
  labs(x = "Retention time 1 (minutes)", y = "Retention time 2 (seconds)",  
       color = "Compound Name", size = " Magnitude of Normalized Peak Area") +
  theme(plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 20, face = "bold"),   # Make facet label text bold
        strip.background = element_blank()) + # Remove the black bounding box
   
  # Add a annotation of chemical groups
  # Unknown
  annotate("rect", xmin = 6.5, xmax = 23.5, ymin = 3.1, ymax = 3.35, 
           color = "black", fill = NA, size = 0.5) + 
  annotate("text", x = 7, y = 3.32, label = "1", 
           hjust = 0, vjust = 0.5, size = 4.5, color = "black", fontface = "bold") +
  # BTEX
  annotate("rect", xmin = 14, xmax = 17, ymin = 3.8, ymax = 4, 
           color = "black", fill = NA, size = 0.5)  +
  annotate("text", x = 14.5, y = 3.97, label = "2", 
           hjust = 0, vjust = 0.5, size = 4.5, color = "black", fontface = "bold") +
  # Castle Group
  annotate("rect", xmin = 18.5, xmax = 21.5, ymin = 3.8, ymax = 3.95, 
           color = "black", fill = NA, size = 0.5) + 
  annotate("text", x = 19, y = 3.92, label = "3", 
           hjust = 0, vjust = 0.5, size = 4.5, color = "black", fontface = "bold")
  

ggsave(filename = paste0("Bubble_plot","_only_non-uniquefillNA0.001_TSN_sig_comp_nonunique_unionize_", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = testbubble, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 13,       # Width in inches (adjust as needed)
       height = 8,       # Height in inches (adjust as needed)
       units = "in") 



# Figure out what are these unknown
```

# Feature filtering: Modified 80% rule
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Assuming your dataframe is called `df`, and the class column is named `Class`
# Define a function to filter features based on non-missing values within each class
filter_features_by_class <- function(data, class_col, threshold = 0.8) {
  # Split the data by classes
  class_split <- split(data, data[[class_col]])
  
  # Initialize a list to store columns that meet the threshold in each class
  columns_meeting_threshold <- list()
  
  for (class_name in names(class_split)) {
    # Calculate non-missing proportion for each feature within the class
    class_data <- class_split[[class_name]]
    feature_non_missing <- colMeans(!is.na(class_data))
    
    # Select columns that meet the threshold
    columns_meeting_threshold[[class_name]] <- names(feature_non_missing[feature_non_missing >= threshold])
  }
  
  # Find common columns that meet the threshold across all classes
  common_columns <- Reduce(intersect, columns_meeting_threshold)
  
  # Combine the common columns with the class column into one vector for selection
  selected_columns <- c(common_columns, class_col)
  
  # Subset the original data to keep only the selected columns
  filtered_data <- data %>%
    dplyr::select(all_of(selected_columns))
  
  return(filtered_data)
}

# Apply the function to your dataframe
filtered_df <- filter_features_by_class(coredf, "gas_station", threshold = 0.8)
```

# Replace missing values with zero based on threshold
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
### Define the threshold for the proportion of non-missing values
threshold <- 0.8

# Apply data replacement based on the condition described
df <- coredf %>%
  group_by(supplier) %>%
  mutate(across(where(is.numeric), ~ {
    # Calculate the proportion of non-missing values within each class
    non_missing_prop <- sum(!is.na(.)) / n()
    
    if (non_missing_prop < threshold) {
      # Replace all values with zero if the proportion of non-missing values is less than threshold
      0
    } else {
      # Replace only missing values with zero if the proportion of non-missing values is at least threshold
      replace(., is.na(.), 0)
    }
  })) %>%
  ungroup()

View(df)

```

